<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>My Customized Papers</title><link>https://github.com/your_username/your_repo</link><description>Aggregated research papers</description><language>en-US</language><lastBuildDate>Tue, 13 Jan 2026 06:41:23 GMT</lastBuildDate><generator>rfeed v1.1.1</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>[arXiv-CV] HyperTopo-Adapters: Geometry- and Topology-Aware Segmentation of Leaf Lesions on Frozen Encoders</title><link>https://arxiv.org/abs/2601.06067</link><description>arXiv:2601.06067v1 Announce Type: new 
Abstract: Leaf-lesion segmentation is topology-sensitive: small merges, splits, or false holes can be biologically meaningful descriptors of biochemical pathways, yet they are weakly penalized by standard pixel-wise losses in Euclidean latents. I explore HyperTopo-Adapters, a lightweight, parameter-efficient head trained on top of a frozen vision encoder, which embeds features on a product manifold -- hyperbolic + Euclidean + spherical (H + E + S) -- to encourage hierarchical separation (H), local linear detail (E), and global closure (S). A topology prior complements Dice/BCE in two forms: (i) persistent-homology (PH) distance for evaluation and selection, and (ii) a differentiable surrogate that combines a soft Euler-characteristic match with total variation regularization for stable training. I introduce warm-ups for both the hyperbolic contrastive term and the topology prior, per-sample evaluation of structure-aware metrics (Boundary-F1, Betti errors, PD distance), and a min-PD within top-K Dice rule for checkpoint selection. On a Kaggle leaf-lesion dataset (N=2,940), early results show consistent gains in boundary and topology metrics (reducing Delta beta_1 hole error by 9%) while Dice/IoU remain competitive. The study is diagnostic by design: I report controlled ablations (curvature learning, latent dimensions, contrastive temperature, surrogate settings), and ongoing tests varying encoder strength (ResNet-50, DeepLabV3, DINOv2/v3), input resolution, PH weight, and partial unfreezing of late blocks. The contribution is an open, reproducible train/eval suite (available at https://github.com/ChimdiWalter/HyperTopo-Adapters) that isolates geometric/topological priors and surfaces failure modes to guide stronger, topology-preserving architectures.</description><author>cs.CV updates on arXiv.org</author><pubDate>Tue, 13 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2601.06067v1</guid></item><item><title>[arXiv-CV] A Unified Attention U-Net Framework for Cross-Modality Tumor Segmentation in MRI and CT</title><link>https://arxiv.org/abs/2601.06187</link><description>arXiv:2601.06187v1 Announce Type: new 
Abstract: This study presents a unified Attention U-Net architecture trained jointly on MRI (BraTS 2021) and CT (LIDC-IDRI) datasets to investigate the generalizability of a single model across diverse imaging modalities and anatomical sites. Our proposed pipeline incorporates modality-harmonized preprocessing, attention-gated skip connections, and a modality-aware Focal Tversky loss function. To the best of our knowledge, this study is among the first to evaluate a single Attention U-Net trained simultaneously on separate MRI (BraTS) and CT (LIDC-IDRI) tumor datasets, without relying on modality-specific encoders or domain adaptation. The unified model demonstrates competitive performance in terms of Dice coefficient, IoU, and AUC on both domains, thereby establishing a robust and reproducible baseline for future research in cross-modality tumor segmentation.</description><author>cs.CV updates on arXiv.org</author><pubDate>Tue, 13 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2601.06187v1</guid></item><item><title>[arXiv-CV] When Imbalance Comes Twice: Active Learning under Simulated Class Imbalance and Label Shift in Binary Semantic Segmentation</title><link>https://arxiv.org/abs/2601.06209</link><description>arXiv:2601.06209v1 Announce Type: new 
Abstract: The aim of Active Learning is to select the most informative samples from an unlabelled set of data. This is useful in cases where the amount of data is large and labelling is expensive, such as in machine vision or medical imaging. Two particularities of machine vision are first, that most of the images produced are free of defects, and second, that the amount of images produced is so big that we cannot store all acquired images. This results, on the one hand, in a strong class imbalance in defect distribution and, on the other hand, in a potential label shift caused by limited storage. To understand how these two forms of imbalance affect active learning algorithms, we propose a simulation study based on two open-source datasets. We artificially create datasets for which we control the levels of class imbalance and label shift. Three standard active learning selection strategies are compared: random sampling, entropy-based selection, and core-set selection. We demonstrate that active learning strategies, and in particular the entropy-based and core-set selections, remain interesting and efficient even for highly imbalanced datasets. We also illustrate and measure the loss of efficiency that occurs in the situation a strong label shift.</description><author>cs.CV updates on arXiv.org</author><pubDate>Tue, 13 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2601.06209v1</guid></item><item><title>[arXiv-CV] SIRR-LMM: Single-image Reflection Removal via Large Multimodal Model</title><link>https://arxiv.org/abs/2601.07209</link><description>arXiv:2601.07209v1 Announce Type: new 
Abstract: Glass surfaces create complex interactions of reflected and transmitted light, making single-image reflection removal (SIRR) challenging. Existing datasets suffer from limited physical realism in synthetic data or insufficient scale in real captures. We introduce a synthetic dataset generation framework that path-traces 3D glass models over real background imagery to create physically accurate reflection scenarios with varied glass properties, camera settings, and post-processing effects. To leverage the capabilities of Large Multimodal Model (LMM), we concatenate the image layers into a single composite input, apply joint captioning, and fine-tune the model using task-specific LoRA rather than full-parameter training. This enables our approach to achieve improved reflection removal and separation performance compared to state-of-the-art methods.</description><author>cs.CV updates on arXiv.org</author><pubDate>Tue, 13 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2601.07209v1</guid></item><item><title>[arXiv-CV] Explainable Deep Radiogenomic Molecular Imaging for MGMT Methylation Prediction in Glioblastoma</title><link>https://arxiv.org/abs/2601.07035</link><description>arXiv:2601.07035v1 Announce Type: cross 
Abstract: Glioblastoma (GBM) is a highly aggressive primary brain tumor with limited therapeutic options and poor prognosis. The methylation status of the O6-methylguanine-DNA methyltransferase (MGMT) gene promoter is a critical molecular biomarker that influences patient response to temozolomide chemotherapy. Traditional methods for determining MGMT status rely on invasive biopsies and are limited by intratumoral heterogeneity and procedural risks. This study presents a radiogenomic molecular imaging analysis framework for the non-invasive prediction of MGMT promoter methylation using multi-parametric magnetic resonance imaging (mpMRI).
  Our approach integrates radiomics, deep learning, and explainable artificial intelligence (XAI) to analyze MRI-derived imaging phenotypes and correlate them with molecular labels. Radiomic features are extracted from FLAIR, T1-weighted, T1-contrast-enhanced, and T2-weighted MRI sequences, while a 3D convolutional neural network learns deep representations from the same modalities. These complementary features are fused using both early fusion and attention-based strategies and classified to predict MGMT methylation status.
  To enhance clinical interpretability, we apply XAI methods such as Grad-CAM and SHAP to visualize and explain model decisions. The proposed framework is trained on the RSNA-MICCAI Radiogenomic Classification dataset and externally validated on the BraTS 2021 dataset. This work advances the field of molecular imaging by demonstrating the potential of AI-driven radiogenomics for precision oncology, supporting non-invasive, accurate, and interpretable prediction of clinically actionable molecular biomarkers in GBM.</description><author>cs.CV updates on arXiv.org</author><pubDate>Tue, 13 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2601.07035v1</guid></item><item><title>[arXiv-CV] A Multimodal Dataset of Student Oral Presentations with Sensors and Evaluation Data</title><link>https://arxiv.org/abs/2601.07576</link><description>arXiv:2601.07576v1 Announce Type: cross 
Abstract: Oral presentation skills are a critical component of higher education, yet comprehensive datasets capturing real-world student performance across multiple modalities remain scarce. To address this gap, we present SOPHIAS (Student Oral Presentation monitoring for Holistic Insights &amp; Analytics using Sensors), a 12-hour multimodal dataset containing recordings of 50 oral presentations (10-15-minute presentation followed by 5-15-minute Q&amp;amp;A) delivered by 65 undergraduate and master's students at the Universidad Autonoma de Madrid. SOPHIAS integrates eight synchronized sensor streams from high-definition webcams, ambient and webcam audio, eye-tracking glasses, smartwatch physiological sensors, and clicker, keyboard, and mouse interactions. In addition, the dataset includes slides and rubric-based evaluations from teachers, peers, and self-assessments, along with timestamped contextual annotations. The dataset captures presentations conducted in real classroom settings, preserving authentic student behaviors, interactions, and physiological responses. SOPHIAS enables the exploration of relationships between multimodal behavioral and physiological signals and presentation performance, supports the study of peer assessment, and provides a benchmark for developing automated feedback and Multimodal Learning Analytics tools. The dataset is publicly available for research through GitHub and Science Data Bank.</description><author>cs.CV updates on arXiv.org</author><pubDate>Tue, 13 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2601.07576v1</guid></item><item><title>[arXiv-CV] Does DINOv3 Set a New Medical Vision Standard?</title><link>https://arxiv.org/abs/2509.06467</link><description>arXiv:2509.06467v2 Announce Type: replace 
Abstract: The advent of large-scale vision foundation models, pre-trained on diverse natural images, has marked a paradigm shift in computer vision. However, how the frontier vision foundation models' efficacies transfer to specialized domains remains such as medical imaging remains an open question. This report investigates whether DINOv3, a state-of-the-art self-supervised vision transformer (ViT) that features strong capability in dense prediction tasks, can directly serve as a powerful, unified encoder for medical vision tasks without domain-specific pre-training. To answer this, we benchmark DINOv3 across common medical vision tasks, including 2D/3D classification and segmentation on a wide range of medical imaging modalities. We systematically analyze its scalability by varying model sizes and input image resolutions. Our findings reveal that DINOv3 shows impressive performance and establishes a formidable new baseline. Remarkably, it can even outperform medical-specific foundation models like BiomedCLIP and CT-Net on several tasks, despite being trained solely on natural images. However, we identify clear limitations: The model's features degrade in scenarios requiring deep domain specialization, such as in Whole-Slide Pathological Images (WSIs), Electron Microscopy (EM), and Positron Emission Tomography (PET). Furthermore, we observe that DINOv3 does not consistently obey scaling law in the medical domain; performance does not reliably increase with larger models or finer feature resolutions, showing diverse scaling behaviors across tasks. Ultimately, our work establishes DINOv3 as a strong baseline, whose powerful visual features can serve as a robust prior for multiple complex medical tasks. This opens promising future directions, such as leveraging its features to enforce multiview consistency in 3D reconstruction.</description><author>cs.CV updates on arXiv.org</author><pubDate>Tue, 13 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2509.06467v2</guid></item><item><title>[arXiv-CV] Parameter-efficient fine-tuning (PEFT) of Vision Foundation Models for Atypical Mitotic Figure Classification</title><link>https://arxiv.org/abs/2509.16935</link><description>arXiv:2509.16935v2 Announce Type: replace 
Abstract: Atypical mitotic figures (AMFs) are rare abnormal cell divisions associated with tumor aggressiveness and poor prognosis. Their detection remains a significant challenge due to subtle morphological cues, class imbalance, and inter-observer variability among pathologists. The MIDOG 2025 challenge introduced a dedicated track for atypical mitosis classification, enabling systematic evaluation of deep learning methods. In this study, we investigated the use of large vision foundation models, including Virchow, Virchow2, and UNI, with Low-Rank Adaptation (LoRA) for parameter-efficient fine-tuning. We conducted extensive experiments with different LoRA ranks, as well as random and group-based data splits, to analyze robustness under varied conditions. Our best approach, Virchow with LoRA rank 8 and ensemble of three-fold cross-validation, achieved a balanced accuracy of 88.37% on the preliminary test set, ranking joint 9th in the challenge leaderboard. These results highlight the promise of foundation models with efficient adaptation strategies for the classification of atypical mitosis, while underscoring the need for improvements in specificity and domain generalization.</description><author>cs.CV updates on arXiv.org</author><pubDate>Tue, 13 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2509.16935v2</guid></item><item><title>[arXiv-CV] Encoding Structural Constraints into Segment Anything Models via Probabilistic Graphical Models</title><link>https://arxiv.org/abs/2509.21750</link><description>arXiv:2509.21750v2 Announce Type: replace 
Abstract: While the Segment Anything Model (SAM) has achieved remarkable success in image segmentation, its direct application to medical imaging remains hindered by fundamental challenges, including ambiguous boundaries, insufficient modeling of anatomical relationships, and the absence of uncertainty quantification. To address these limitations, we introduce KG-SAM, a knowledge-guided framework that synergistically integrates anatomical priors with boundary refinement and uncertainty estimation. Specifically, KG-SAM incorporates (i) a medical knowledge graph to encode fine-grained anatomical relationships, (ii) an energy-based Conditional Random Field (CRF) to enforce anatomically consistent predictions, and (iii) an uncertainty-aware fusion module to enhance reliability in high-stakes clinical scenarios. Extensive experiments across multi-center medical datasets demonstrate the effectiveness of our approach: KG-SAM achieves an average Dice score of 82.69% on prostate segmentation and delivers substantial gains in abdominal segmentation, reaching 78.05% on MRI and 79.68% on CT. These results establish KG-SAM as a robust and generalizable framework for advancing medical image segmentation.</description><author>cs.CV updates on arXiv.org</author><pubDate>Tue, 13 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2509.21750v2</guid></item><item><title>[arXiv-CV] ReBrain: Brain MRI Reconstruction from Sparse CT Slice via Retrieval-Augmented Diffusion</title><link>https://arxiv.org/abs/2511.17068</link><description>arXiv:2511.17068v3 Announce Type: replace 
Abstract: Magnetic Resonance Imaging (MRI) plays a crucial role in brain disease diagnosis, but it is not always feasible for certain patients due to physical or clinical constraints. Recent studies attempt to synthesize MRI from Computed Tomography (CT) scans; however, low-dose protocols often result in highly sparse CT volumes with poor through-plane resolution, making accurate reconstruction of the full brain MRI volume particularly challenging. To address this, we propose ReBrain, a retrieval-augmented diffusion framework for brain MRI reconstruction. Given any 3D CT scan with limited slices, we first employ a Brownian Bridge Diffusion Model (BBDM) to synthesize MRI slices along the 2D dimension. Simultaneously, we retrieve structurally and pathologically similar CT slices from a comprehensive prior database via a fine-tuned retrieval model. These retrieved slices are used as references, incorporated through a ControlNet branch to guide the generation of intermediate MRI slices and ensure structural continuity. We further account for rare retrieval failures when the database lacks suitable references and apply spherical linear interpolation to provide supplementary guidance. Extensive experiments on SynthRAD2023 and BraTS demonstrate that ReBrain achieves state-of-the-art performance in cross-modal reconstruction under sparse conditions.</description><author>cs.CV updates on arXiv.org</author><pubDate>Tue, 13 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2511.17068v3</guid></item><item><title>[arXiv-CV] Diagnostic Performance of Universal-Learning Ultrasound AI Across Multiple Organs and Tasks: the UUSIC25 Challenge</title><link>https://arxiv.org/abs/2512.17279</link><description>arXiv:2512.17279v2 Announce Type: replace 
Abstract: IMPORTANCE: Modern ultrasound systems are universal diagnostic tools capable of imaging the entire body. However, current AI solutions remain fragmented into single-task tools. This critical gap between hardware versatility and software specificity limits workflow integration and clinical utility.
  OBJECTIVE: To evaluate the diagnostic accuracy, versatility, and efficiency of single general-purpose deep learning models for multi-organ classification and segmentation.
  DESIGN: The Universal UltraSound Image Challenge 2025 (UUSIC25) involved developing algorithms on 11,644 images aggregated from 12 sources (9 public, 3 private). Evaluation used an independent, multi-center private test set of 2,479 images, including data from a center completely unseen during training to assess generalization.
  OUTCOMES: Diagnostic performance (Dice Similarity Coefficient [DSC]; Area Under the Receiver Operating Characteristic Curve [AUC]) and computational efficiency (inference time, GPU memory).
  RESULTS: Of 15 valid algorithms, the top model (SMART) achieved a macro-averaged DSC of 0.854 across 5 segmentation tasks and AUC of 0.766 for binary classification. Models demonstrated high capability in anatomical segmentation (e.g., fetal head DSC: 0.942) but variability in complex diagnostic tasks subject to domain shift. Specifically, in breast cancer molecular subtyping, the top model's performance dropped from an AUC of 0.571 (internal) to 0.508 (unseen external center), highlighting the challenge of generalization.
  CONCLUSIONS: General-purpose AI models can achieve high accuracy and efficiency across multiple tasks using a single architecture. However, significant performance degradation on unseen data suggests domain generalization is critical for future clinical deployment.</description><author>cs.CV updates on arXiv.org</author><pubDate>Tue, 13 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2512.17279v2</guid></item><item><title>[arXiv-ML] A Foundation Model Approach for Fetal Stress Prediction During Labor From cardiotocography (CTG) recordings</title><link>https://arxiv.org/abs/2601.06149</link><description>arXiv:2601.06149v1 Announce Type: new 
Abstract: Intrapartum cardiotocography (CTG) is widely used for fetal monitoring during labor, yet its interpretation suffers from high inter-observer variability and limited predictive accuracy. Deep learning approaches have been constrained by the scarcity of CTG recordings with clinical outcome labels. We present the first application of self-supervised pre-training to intrapartum CTG analysis, leveraging 2,444 hours of unlabeled recordings for masked pre-training followed by fine-tuning on the 552-recording CTU-UHB benchmark. Using a PatchTST transformer architecture with a channel-asymmetric masking scheme designed for fetal heart rate reconstruction, we achieve an area under the receiver operating characteristic curve of 0.83 on the full test set and 0.853 on uncomplicated vaginal deliveries, exceeding previously reported results on this benchmark (0.68-0.75). Error analysis reveals that false-positive alerts typically correspond to CTG patterns judged concerning on retrospective clinical review, suggesting clinically meaningful predictions even when umbilical pH is normal. We release standardized dataset splits and model weights to enable reproducible benchmarking. Our results demonstrate that self-supervised pre-training can address data scarcity in fetal monitoring, offering a path toward reliable decision support in the labor room.</description><author>cs.LG updates on arXiv.org</author><pubDate>Tue, 13 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2601.06149v1</guid></item><item><title>[arXiv-ML] Cyber Threat Detection and Vulnerability Assessment System using Generative AI and Large Language Model</title><link>https://arxiv.org/abs/2601.06213</link><description>arXiv:2601.06213v1 Announce Type: cross 
Abstract: Background: Cyber-attacks have evolved rapidly in recent years, many individuals and business owners have been affected by cyber-attacks in various ways. Cyber-attacks include various threats such as ransomware, malware, phishing, and Denial of Service (DoS)-related attacks. Challenges: Traditional models such as Generative Artificial Intelligence (AI) and Security Bidirectional Encoder Representations from Transformers (BERT) were implemented to detect cyber threats. However, the existing Security BERT model has a limited contextual understanding of text data, which has less impact on detecting cyber-attacks. Proposed Methodology: To overcome the above-mentioned challenges, Robustly Optimized Bidirectional Encoder Representations from Transformers Pretraining Approach (RoBERTa) model is proposed which consists of diverse words of vocabulary understanding. Initially, data are extracted from a Packet Capture (PCAP) file and encrypted using Fully Harmonic Encryption (FHE). Subsequently, a Byte-level and Byte Pair Encoding (BBPE) tokenizer was used to generate tokens and help maintain the vocabulary for the encrypted values. Then, these values are applied to the RoBERTa model of the transformer with extensive training. Finally, Softmax is used for the detection and classification of attacks. The proposed RoBERTa model achieved better results than the existing BERT model in terms of accuracy (0.99), recall (0.91), and precision (0.89) respectively.</description><author>cs.LG updates on arXiv.org</author><pubDate>Tue, 13 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2601.06213v1</guid></item><item><title>[arXiv-ML] A Backpropagation-Free Feedback-Hebbian Network for Continual Learning Dynamics</title><link>https://arxiv.org/abs/2601.06758</link><description>arXiv:2601.06758v1 Announce Type: cross 
Abstract: Feedback-rich neural architectures can regenerate earlier representations and inject temporal context, making them a natural setting for strictly local synaptic plasticity. We ask whether a minimal, backpropagation-free feedback--Hebbian system can already express interpretable continual-learning--relevant behaviors under controlled training schedules. We introduce a compact prediction--reconstruction architecture with two feedforward layers for supervised association learning and two dedicated feedback layers trained to reconstruct earlier activity and re-inject it as additive temporal context. All synapses are updated by a unified local rule combining centered Hebbian covariance, Oja-style stabilization, and a local supervised drive where targets are available, requiring no weight transport or global error backpropagation. On a small two-pair association task, we characterize learning through layer-wise activity snapshots, connectivity trajectories (row/column means of learned weights), and a normalized retention index across phases. Under sequential A-&gt;B training, forward output connectivity exhibits a long-term depression (LTD)-like suppression of the earlier association while feedback connectivity preserves an A-related trace during acquisition of B. Under deterministic interleaving A,B,A,B,..., both associations are concurrently maintained rather than sequentially suppressed. Architectural controls and rule-term ablations isolate the role of dedicated feedback in regeneration and co-maintenance, and the role of the local supervised term in output selectivity and unlearning. Together, the results show that a compact feedback pathway trained with local plasticity can support regeneration and continual-learning--relevant dynamics in a minimal, mechanistically transparent setting.</description><author>cs.LG updates on arXiv.org</author><pubDate>Tue, 13 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2601.06758v1</guid></item><item><title>[arXiv-ML] Unity Forests: Improving Interaction Modelling and Interpretability in Random Forests</title><link>https://arxiv.org/abs/2601.07003</link><description>arXiv:2601.07003v1 Announce Type: cross 
Abstract: Random forests (RFs) are widely used for prediction and variable importance analysis and are often believed to capture any types of interactions via recursive splitting. However, since the splits are chosen locally, interactions are only reliably captured when at least one involved covariate has a marginal effect. We introduce unity forests (UFOs), an RF variant designed to better exploit interactions involving covariates without marginal effects. In UFOs, the first few splits of each tree are optimized jointly across a random covariate subset to form a "tree root" capturing such interactions; the remainder is grown conventionally. We further propose the unity variable importance measure (VIM), which is based on out-of-bag split criterion values from the tree roots. Here, only a small fraction of tree root splits with the highest in-bag criterion values are considered per covariate, reflecting that covariates with purely interaction-based effects are discriminative only if a split in an interacting covariate occurred earlier in the tree. Finally, we introduce covariate-representative tree roots (CRTRs), which select representative tree roots per covariate and provide interpretable insight into the conditions - marginal or interactive - under which each covariate has its strongest effects. In a simulation study, the unity VIM reliably identified interacting covariates without marginal effects, unlike conventional RF-based VIMs. In a large-scale real-data comparison, UFOs achieved higher discrimination and predictive accuracy than standard RFs, with comparable calibration. The CRTRs reproduced the covariates' true effect types reliably in simulated data and provided interesting insights in a real data analysis.</description><author>cs.LG updates on arXiv.org</author><pubDate>Tue, 13 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2601.07003v1</guid></item><item><title>[arXiv-ML] Covariance-Driven Regression Trees: Reducing Overfitting in CART</title><link>https://arxiv.org/abs/2601.07281</link><description>arXiv:2601.07281v1 Announce Type: cross 
Abstract: Decision trees are powerful machine learning algorithms, widely used in fields such as economics and medicine for their simplicity and interpretability. However, decision trees such as CART are prone to overfitting, especially when grown deep or the sample size is small. Conventional methods to reduce overfitting include pre-pruning and post-pruning, which constrain the growth of uninformative branches. In this paper, we propose a complementary approach by introducing a covariance-driven splitting criterion for regression trees (CovRT). This method is more robust to overfitting than the empirical risk minimization criterion used in CART, as it produces more balanced and stable splits and more effectively identifies covariates with true signals. We establish an oracle inequality of CovRT and prove that its predictive accuracy is comparable to that of CART in high-dimensional settings. We find that CovRT achieves superior prediction accuracy compared to CART in both simulations and real-world tasks.</description><author>cs.LG updates on arXiv.org</author><pubDate>Tue, 13 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2601.07281v1</guid></item><item><title>[arXiv-ML] Learning About Learning: A Physics Path from Spin Glasses to Artificial Intelligence</title><link>https://arxiv.org/abs/2601.07635</link><description>arXiv:2601.07635v1 Announce Type: cross 
Abstract: The Hopfield model, originally inspired by spin-glass physics, occupies a central place at the intersection of statistical mechanics, neural networks, and modern artificial intelligence. Despite its conceptual simplicity and broad applicability -- from associative memory to near-optimal solutions of combinatorial optimization problems -- it is rarely integrated into standard undergraduate physics curricula. In this paper, we present the Hopfield model as a pedagogically rich framework that naturally unifies core topics from undergraduate statistical physics, dynamical systems, linear algebra, and computational methods. We provide a concise and illustrated theoretical introduction grounded in familiar physics concepts, analyze the model's energy function, dynamics, and pattern stability, and discuss practical aspects of simulation, including a freely available simulation code. To support instruction, we conclude with classroom-ready example problems designed to mirror research practice. By explicitly connecting fundamental physics to contemporary AI applications, this work aims to help prepare physics students to understand, apply, and critically engage with the computational tools increasingly central to research, industry, and society.</description><author>cs.LG updates on arXiv.org</author><pubDate>Tue, 13 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2601.07635v1</guid></item><item><title>[arXiv-ML] Learning to accelerate Krasnosel'skii-Mann fixed-point iterations with guarantees</title><link>https://arxiv.org/abs/2601.07665</link><description>arXiv:2601.07665v1 Announce Type: cross 
Abstract: We introduce a principled learning to optimize (L2O) framework for solving fixed-point problems involving general nonexpansive mappings. Our idea is to deliberately inject summable perturbations into a standard Krasnosel'skii-Mann iteration to improve its average-case performance over a specific distribution of problems while retaining its convergence guarantees. Under a metric sub-regularity assumption, we prove that the proposed parametrization includes only iterations that locally achieve linear convergence-up to a vanishing bias term-and that it encompasses all iterations that do so at a sufficiently fast rate. We then demonstrate how our framework can be used to augment several widely-used operator splitting methods to accelerate the solution of structured monotone inclusion problems, and validate our approach on a best approximation problem using an L2O-augmented Douglas-Rachford splitting algorithm.</description><author>cs.LG updates on arXiv.org</author><pubDate>Tue, 13 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2601.07665v1</guid></item><item><title>[arXiv-ML] Failure-Aware RL: Reliable Offline-to-Online Reinforcement Learning with Self-Recovery for Real-World Manipulation</title><link>https://arxiv.org/abs/2601.07821</link><description>arXiv:2601.07821v1 Announce Type: cross 
Abstract: Post-training algorithms based on deep reinforcement learning can push the limits of robotic models for specific objectives, such as generalizability, accuracy, and robustness. However, Intervention-requiring Failures (IR Failures) (e.g., a robot spilling water or breaking fragile glass) during real-world exploration happen inevitably, hindering the practical deployment of such a paradigm. To tackle this, we introduce Failure-Aware Offline-to-Online Reinforcement Learning (FARL), a new paradigm minimizing failures during real-world reinforcement learning. We create FailureBench, a benchmark that incorporates common failure scenarios requiring human intervention, and propose an algorithm that integrates a world-model-based safety critic and a recovery policy trained offline to prevent failures during online exploration. Extensive simulation and real-world experiments demonstrate the effectiveness of FARL in significantly reducing IR Failures while improving performance and generalization during online reinforcement learning post-training. FARL reduces IR Failures by 73.1% while elevating performance by 11.3% on average during real-world RL post-training. Videos and code are available at https://failure-aware-rl.github.io.</description><author>cs.LG updates on arXiv.org</author><pubDate>Tue, 13 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2601.07821v1</guid></item><item><title>[arXiv-ML] Prophet as a Reproducible Forecasting Framework: A Methodological Guide for Business and Financial Analytics</title><link>https://arxiv.org/abs/2601.05929</link><description>arXiv:2601.05929v2 Announce Type: replace 
Abstract: Reproducibility remains a persistent challenge in forecasting research and practice, particularly in business and financial analytics, where forecasts inform high-stakes decisions. Traditional forecasting methods, while theoretically interpretable, often require extensive manual tuning and are difficult to replicate in proprietary environments. Machine learning approaches offer predictive flexibility but introduce challenges related to interpretability, stochastic training procedures, and cross-environment reproducibility. This paper examines Prophet, an open-source forecasting framework developed by Meta, as a reproducibility-enabling solution that balances interpretability, standardized workflows, and accessibility. Rather than proposing a new algorithm, this study evaluates how Prophet's additive structure, open-source implementation, and standardized workflow contribute to transparent and replicable forecasting practice. Using publicly available financial and retail datasets, we compare the performance and interpretability of Prophet with multiple ARIMA specifications (auto-selected, manually specified, and seasonal variants) and Random Forest, under a controlled and fully documented experimental design. This multi-model comparison provides a robust assessment of Prophet's relative performance and reproducibility advantages. Through concrete Python examples, we demonstrate how Prophet facilitates efficient forecasting workflows and integration with analytical pipelines. The study positions Prophet within the broader context of reproducible research. It highlights Prophet's role as a methodological building block that supports verification, auditability, and methodological rigor. This work provides researchers and practitioners with a practical reference framework for reproducible forecasting in Python-based research workflows.</description><author>cs.LG updates on arXiv.org</author><pubDate>Tue, 13 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2601.05929v2</guid></item><item><title>[BSPC] Depth-conditioned adversarial learning with dual discriminators for polyp segmentation</title><link>https://www.sciencedirect.com/science/article/pii/S1746809426001412?dgcid=rss_sd_all</link><description>&lt;p&gt;Publication date: 1 May 2026&lt;/p&gt;&lt;p&gt;&lt;b&gt;Source:&lt;/b&gt; Biomedical Signal Processing and Control, Volume 116&lt;/p&gt;&lt;p&gt;Author(s): Haiyu Wang, Jianan Zhang, Xueyu Liu, Yongfei Wu, Di Huang&lt;/p&gt;</description><author>ScienceDirect Publication: Biomedical Signal Processing and Control</author><pubDate>Mon, 12 Jan 2026 18:39:30 GMT</pubDate><guid isPermaLink="true">https://www.sciencedirect.com/science/article/pii/S1746809426001412</guid></item><item><title>[arXiv-CV] Prompt-Free SAM-Based Multi-Task Framework for Breast Ultrasound Lesion Segmentation and Classification</title><link>https://arxiv.org/abs/2601.05498</link><description>arXiv:2601.05498v1 Announce Type: new 
Abstract: Accurate tumor segmentation and classification in breast ultrasound (BUS) imaging remain challenging due to low contrast, speckle noise, and diverse lesion morphology. This study presents a multi-task deep learning framework that jointly performs lesion segmentation and diagnostic classification using embeddings from the Segment Anything Model (SAM) vision encoder. Unlike prompt-based SAM variants, our approach employs a prompt-free, fully supervised adaptation where high-dimensional SAM features are decoded through either a lightweight convolutional head or a UNet-inspired decoder for pixel-wise segmentation. The classification branch is enhanced via mask-guided attention, allowing the model to focus on lesion-relevant features while suppressing background artifacts. Experiments on the PRECISE 2025 breast ultrasound dataset, split per class into 80 percent training and 20 percent testing, show that the proposed method achieves a Dice Similarity Coefficient (DSC) of 0.887 and an accuracy of 92.3 percent, ranking among the top entries on the PRECISE challenge leaderboard. These results demonstrate that SAM-based representations, when coupled with segmentation-guided learning, significantly improve both lesion delineation and diagnostic prediction in breast ultrasound imaging.</description><author>cs.CV updates on arXiv.org</author><pubDate>Mon, 12 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2601.05498v1</guid></item><item><title>[arXiv-CV] Bidirectional Channel-selective Semantic Interaction for Semi-Supervised Medical Segmentation</title><link>https://arxiv.org/abs/2601.05855</link><description>arXiv:2601.05855v1 Announce Type: new 
Abstract: Semi-supervised medical image segmentation is an effective method for addressing scenarios with limited labeled data. Existing methods mainly rely on frameworks such as mean teacher and dual-stream consistency learning. These approaches often face issues like error accumulation and model structural complexity, while also neglecting the interaction between labeled and unlabeled data streams. To overcome these challenges, we propose a Bidirectional Channel-selective Semantic Interaction~(BCSI) framework for semi-supervised medical image segmentation. First, we propose a Semantic-Spatial Perturbation~(SSP) mechanism, which disturbs the data using two strong augmentation operations and leverages unsupervised learning with pseudo-labels from weak augmentations. Additionally, we employ consistency on the predictions from the two strong augmentations to further improve model stability and robustness. Second, to reduce noise during the interaction between labeled and unlabeled data, we propose a Channel-selective Router~(CR) component, which dynamically selects the most relevant channels for information exchange. This mechanism ensures that only highly relevant features are activated, minimizing unnecessary interference. Finally, the Bidirectional Channel-wise Interaction~(BCI) strategy is employed to supplement additional semantic information and enhance the representation of important channels. Experimental results on multiple benchmarking 3D medical datasets demonstrate that the proposed method outperforms existing semi-supervised approaches.</description><author>cs.CV updates on arXiv.org</author><pubDate>Mon, 12 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2601.05855v1</guid></item><item><title>[arXiv-ML] Tensor-DTI: Enhancing Biomolecular Interaction Prediction with Contrastive Embedding Learning</title><link>https://arxiv.org/abs/2601.05792</link><description>arXiv:2601.05792v1 Announce Type: new 
Abstract: Accurate drug-target interaction (DTI) prediction is essential for computational drug discovery, yet existing models often rely on single-modality predefined molecular descriptors or sequence-based embeddings with limited representativeness. We propose Tensor-DTI, a contrastive learning framework that integrates multimodal embeddings from molecular graphs, protein language models, and binding-site predictions to improve interaction modeling. Tensor-DTI employs a siamese dual-encoder architecture, enabling it to capture both chemical and structural interaction features while distinguishing interacting from non-interacting pairs. Evaluations on multiple DTI benchmarks demonstrate that Tensor-DTI outperforms existing sequence-based and graph-based models. We also conduct large-scale inference experiments on CDK2 across billion-scale chemical libraries, where Tensor-DTI produces chemically plausible hit distributions even when CDK2 is withheld from training. In enrichment studies against Glide docking and Boltz-2 co-folder, Tensor-DTI remains competitive on CDK2 and improves the screening budget required to recover moderate fractions of high-affinity ligands on out-of-family targets under strict family-holdout splits. Additionally, we explore its applicability to protein-RNA and peptide-protein interactions. Our findings highlight the benefits of integrating multimodal information with contrastive objectives to enhance interaction-prediction accuracy and to provide more interpretable and reliability-aware models for virtual screening.</description><author>cs.LG updates on arXiv.org</author><pubDate>Mon, 12 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2601.05792v1</guid></item><item><title>[arXiv-ML] Fusion Matters: Length-Aware Analysis of Positional-Encoding Fusion in Transformers</title><link>https://arxiv.org/abs/2601.05807</link><description>arXiv:2601.05807v1 Announce Type: new 
Abstract: Transformers require positional encodings to represent sequence order, yet most prior work focuses on designing new positional encodings rather than examining how positional information is fused with token embeddings. In this paper, we study whether the fusion mechanism itself affects performance, particularly in long-sequence settings. We conduct a controlled empirical study comparing three canonical fusion strategies--element-wise addition, concatenation with projection, and scalar gated fusion--under identical Transformer architectures, data splits, and random seeds. Experiments on three text classification datasets spanning short (AG News), medium (IMDB), and long (ArXiv) sequences show that fusion choice has negligible impact on short texts but produces consistent gains on long documents. To verify that these gains are structural rather than stochastic, we perform paired-seed analysis and cross-dataset comparison across sequence-length regimes. Additional experiments on the ArXiv dataset indicate that the benefit of learnable fusion generalizes across multiple positional encoding families. Finally, we explore a lightweight convolutional gating mechanism that introduces local inductive bias at the fusion level, evaluated on long documents only. Our results indicate that positional-encoding fusion is a non-trivial design choice for long-sequence Transformers and should be treated as an explicit modeling decision rather than a fixed default.</description><author>cs.LG updates on arXiv.org</author><pubDate>Mon, 12 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2601.05807v1</guid></item><item><title>[arXiv-ML] Prophet as a Repro ducible Forecasting Framework: A Methodological Guide for Business and Financial Analytics</title><link>https://arxiv.org/abs/2601.05929</link><description>arXiv:2601.05929v1 Announce Type: new 
Abstract: Reproducibility remains a persistent challenge in forecasting research and practice, particularly in business and financial analytics where forecasts inform high-stakes decisions. Traditional forecasting methods, while theoretically interpretable, often require extensive manual tuning and are difficult to replicate in proprietary environments. Machine learning approaches offer predictive flexibility but introduce challenges related to interpretability, stochastic training procedures, and cross-environment reproducibility. This paper examines Prophet, an open-source forecasting framework developed by Meta, as a reproducibility-enabling solution that balances interpretability, standardized workflows, and accessibility. Rather than proposing a new algorithm, this study evaluates how Prophet's additive structure, open-source implementation, and standardized workflow contribute to transparent and replicable forecasting practice. Using publicly available financial and retail datasets, we compare Prophet's performance and interpretability with multiple ARIMA specifications (auto-selected, manually specified, and seasonal variants) and Random Forest under a controlled and fully documented experimental design. This multi-model comparison provides a robust assessment of Prophet's relative performance and reproducibility advantages. Through concrete Python examples, we demonstrate how Prophet facilitates efficient forecasting workflows and integration with analytical pipelines. The study positions Prophet within the broader context of reproducible research. It highlights Prophet's role as a methodological building block that supports verification, auditability, and methodological rigor. This work provides researchers and practitioners with a practical reference framework for reproducible forecasting in Python-based research workflows.</description><author>cs.LG updates on arXiv.org</author><pubDate>Mon, 12 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2601.05929v1</guid></item><item><title>[arXiv-ML] SCOPE: Sequential Causal Optimization of Process Interventions</title><link>https://arxiv.org/abs/2512.17629</link><description>arXiv:2512.17629v3 Announce Type: replace 
Abstract: Prescriptive Process Monitoring (PresPM) recommends interventions during business processes to optimize key performance indicators (KPIs). In realistic settings, interventions are rarely isolated: organizations need to align sequences of interventions to jointly steer the outcome of a case. Existing PresPM approaches fall short in this respect. Many focus on a single intervention decision, while others treat multiple interventions independently, ignoring how they interact over time. Methods that do address these dependencies depend either on simulation or data augmentation to approximate the process to train a Reinforcement Learning (RL) agent, which can create a reality gap and introduce bias. We introduce SCOPE, a PresPM approach that learns aligned sequential intervention recommendations. SCOPE employs backward induction to estimate the effect of each candidate intervention action, propagating its impact from the final decision point back to the first. By leveraging causal learners, our method can utilize observational data directly, unlike methods that require constructing process approximations for reinforcement learning. Experiments on both an existing synthetic dataset and a new semi-synthetic dataset show that SCOPE consistently outperforms state-of-the-art PresPM techniques in optimizing the KPI. The novel semi-synthetic setup, based on a real-life event log, is provided as a reusable benchmark for future work on sequential PresPM.</description><author>cs.LG updates on arXiv.org</author><pubDate>Mon, 12 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2512.17629v3</guid></item><item><title>[BSPC] MACNet:Multiscale attention cross-sharing networks for colorectal polyp segmentation</title><link>https://www.sciencedirect.com/science/article/pii/S1746809426000042?dgcid=rss_sd_all</link><description>&lt;p&gt;Publication date: 1 May 2026&lt;/p&gt;&lt;p&gt;&lt;b&gt;Source:&lt;/b&gt; Biomedical Signal Processing and Control, Volume 116&lt;/p&gt;&lt;p&gt;Author(s): Xinhui Jiang, Chunmiao Wei, Xiaolin Li, Zhicheng Dai&lt;/p&gt;</description><author>ScienceDirect Publication: Biomedical Signal Processing and Control</author><pubDate>Sun, 11 Jan 2026 18:35:03 GMT</pubDate><guid isPermaLink="true">https://www.sciencedirect.com/science/article/pii/S1746809426000042</guid></item><item><title>[BSPC] CSA-Net: Class self-attention based feature extraction network for polyp segmentation</title><link>https://www.sciencedirect.com/science/article/pii/S174680942600042X?dgcid=rss_sd_all</link><description>&lt;p&gt;Publication date: 1 May 2026&lt;/p&gt;&lt;p&gt;&lt;b&gt;Source:&lt;/b&gt; Biomedical Signal Processing and Control, Volume 116&lt;/p&gt;&lt;p&gt;Author(s): Xuehu Wang, Xiangqian Liu, Jin Lu, Yuhao Wang, Chao Xue, Han Yu, Yongchang Zheng, Chen Geng, Chengwei Guo, Xiaoping Yin&lt;/p&gt;</description><author>ScienceDirect Publication: Biomedical Signal Processing and Control</author><pubDate>Sun, 11 Jan 2026 18:35:03 GMT</pubDate><guid isPermaLink="true">https://www.sciencedirect.com/science/article/pii/S174680942600042X</guid></item><item><title>[BSPC] EGNet: A boundary-region closed-loop network for medical image segmentation with fuzzy lesions</title><link>https://www.sciencedirect.com/science/article/pii/S1746809426001564?dgcid=rss_sd_all</link><description>&lt;p&gt;Publication date: 1 May 2026&lt;/p&gt;&lt;p&gt;&lt;b&gt;Source:&lt;/b&gt; Biomedical Signal Processing and Control, Volume 116&lt;/p&gt;&lt;p&gt;Author(s): Shuwen Wu, Qilong Li, Ruixue Xia, Chongsheng Zhang&lt;/p&gt;</description><author>ScienceDirect Publication: Biomedical Signal Processing and Control</author><pubDate>Sun, 11 Jan 2026 18:35:03 GMT</pubDate><guid isPermaLink="true">https://www.sciencedirect.com/science/article/pii/S1746809426001564</guid></item><item><title>[BSPC] Hawk-Net: Medical image segmentation and classification using multi-scale convolutional self-attention-based image processor with DK-CNN-Mamba-xAttention Fusion Network</title><link>https://www.sciencedirect.com/science/article/pii/S1746809425019123?dgcid=rss_sd_all</link><description>&lt;p&gt;Publication date: 15 April 2026&lt;/p&gt;&lt;p&gt;&lt;b&gt;Source:&lt;/b&gt; Biomedical Signal Processing and Control, Volume 115&lt;/p&gt;&lt;p&gt;Author(s): Mahmudul Hasan, Md. Kamrul Hasan&lt;/p&gt;</description><author>ScienceDirect Publication: Biomedical Signal Processing and Control</author><pubDate>Sun, 11 Jan 2026 13:09:00 GMT</pubDate><guid isPermaLink="true">https://www.sciencedirect.com/science/article/pii/S1746809425019123</guid></item><item><title>[BSPC] EFFM: Rethinking feature fusion mechanisms for medical image segmentation</title><link>https://www.sciencedirect.com/science/article/pii/S1746809425018452?dgcid=rss_sd_all</link><description>&lt;p&gt;Publication date: 15 April 2026&lt;/p&gt;&lt;p&gt;&lt;b&gt;Source:&lt;/b&gt; Biomedical Signal Processing and Control, Volume 115&lt;/p&gt;&lt;p&gt;Author(s): Xiaoyan Zhang, Zheng Zhao, Yongqin Zhang, Chunlin Yu, Xiangfu Meng, Shuai Li&lt;/p&gt;</description><author>ScienceDirect Publication: Biomedical Signal Processing and Control</author><pubDate>Sun, 11 Jan 2026 13:09:00 GMT</pubDate><guid isPermaLink="true">https://www.sciencedirect.com/science/article/pii/S1746809425018452</guid></item><item><title>[BSPC] Applications of a novel deep neural network to the classification of liver steatosis and breast lesions in ultrasound images</title><link>https://www.sciencedirect.com/science/article/pii/S1746809425019068?dgcid=rss_sd_all</link><description>&lt;p&gt;Publication date: 15 April 2026&lt;/p&gt;&lt;p&gt;&lt;b&gt;Source:&lt;/b&gt; Biomedical Signal Processing and Control, Volume 115&lt;/p&gt;&lt;p&gt;Author(s): zlem Polat, Zmray Dokur, Tamer lmez&lt;/p&gt;</description><author>ScienceDirect Publication: Biomedical Signal Processing and Control</author><pubDate>Sun, 11 Jan 2026 13:09:00 GMT</pubDate><guid isPermaLink="true">https://www.sciencedirect.com/science/article/pii/S1746809425019068</guid></item><item><title>[BSPC] Frequency matters: Dual Domain Consistency for Semi-supervised Skin Lesion Segmentation</title><link>https://www.sciencedirect.com/science/article/pii/S1746809425019421?dgcid=rss_sd_all</link><description>&lt;p&gt;Publication date: 15 April 2026&lt;/p&gt;&lt;p&gt;&lt;b&gt;Source:&lt;/b&gt; Biomedical Signal Processing and Control, Volume 115&lt;/p&gt;&lt;p&gt;Author(s): Haoran Xi, Shan Ling, Min He, Xiaolin Li&lt;/p&gt;</description><author>ScienceDirect Publication: Biomedical Signal Processing and Control</author><pubDate>Sun, 11 Jan 2026 13:09:00 GMT</pubDate><guid isPermaLink="true">https://www.sciencedirect.com/science/article/pii/S1746809425019421</guid></item><item><title>[BSPC] Flexible dilated convolution and bidirectional pyramid approach to polyp segmentation</title><link>https://www.sciencedirect.com/science/article/pii/S1746809425019202?dgcid=rss_sd_all</link><description>&lt;p&gt;Publication date: 15 April 2026&lt;/p&gt;&lt;p&gt;&lt;b&gt;Source:&lt;/b&gt; Biomedical Signal Processing and Control, Volume 115&lt;/p&gt;&lt;p&gt;Author(s): Yunfei Yin, Zumnan Timbi, Zheng Yuan, Sijing Xiong, MD Tanvir Islam, Argho Dey, Swachha Ray&lt;/p&gt;</description><author>ScienceDirect Publication: Biomedical Signal Processing and Control</author><pubDate>Sun, 11 Jan 2026 13:09:00 GMT</pubDate><guid isPermaLink="true">https://www.sciencedirect.com/science/article/pii/S1746809425019202</guid></item><item><title>[BSPC] Enhanced diagnosis of skin lesions through torsional wave propagation and probabilistic inverse problem algorithms: An experimental study</title><link>https://www.sciencedirect.com/science/article/pii/S174680942501907X?dgcid=rss_sd_all</link><description>&lt;p&gt;Publication date: 15 April 2026&lt;/p&gt;&lt;p&gt;&lt;b&gt;Source:&lt;/b&gt; Biomedical Signal Processing and Control, Volume 115&lt;/p&gt;&lt;p&gt;Author(s): Yousef Almashakbeh, Hirad Shamimi, Inas H Faris, J.L. Martn-Rodrguez, Antonio Callejas, Guillermo Rus&lt;/p&gt;</description><author>ScienceDirect Publication: Biomedical Signal Processing and Control</author><pubDate>Sun, 11 Jan 2026 13:09:00 GMT</pubDate><guid isPermaLink="true">https://www.sciencedirect.com/science/article/pii/S174680942501907X</guid></item><item><title>[BSPC] DiffMoE-UNet: A differential transformer with Mixture-of-Experts for accurate medical image segmentation</title><link>https://www.sciencedirect.com/science/article/pii/S174680942501924X?dgcid=rss_sd_all</link><description>&lt;p&gt;Publication date: 15 April 2026&lt;/p&gt;&lt;p&gt;&lt;b&gt;Source:&lt;/b&gt; Biomedical Signal Processing and Control, Volume 115&lt;/p&gt;&lt;p&gt;Author(s): Jaouad Tagnamas, Hiba Ramadan, Ali Yahyaouy, Hamid Tairi&lt;/p&gt;</description><author>ScienceDirect Publication: Biomedical Signal Processing and Control</author><pubDate>Sun, 11 Jan 2026 13:09:00 GMT</pubDate><guid isPermaLink="true">https://www.sciencedirect.com/science/article/pii/S174680942501924X</guid></item><item><title>[BSPC] Uncertainty-weighted feature alignment and information interaction network for multi-scale medical image segmentation</title><link>https://www.sciencedirect.com/science/article/pii/S174680942501910X?dgcid=rss_sd_all</link><description>&lt;p&gt;Publication date: 15 April 2026&lt;/p&gt;&lt;p&gt;&lt;b&gt;Source:&lt;/b&gt; Biomedical Signal Processing and Control, Volume 115&lt;/p&gt;&lt;p&gt;Author(s): Min Zhang, Junxia Wang, Junkai Wang, Yuanjie Zheng&lt;/p&gt;</description><author>ScienceDirect Publication: Biomedical Signal Processing and Control</author><pubDate>Sun, 11 Jan 2026 13:09:00 GMT</pubDate><guid isPermaLink="true">https://www.sciencedirect.com/science/article/pii/S174680942501910X</guid></item><item><title>[BSPC] DisFreAda: Distribution-driven multi-frequency adaptive network for generalizable medical image segmentation</title><link>https://www.sciencedirect.com/science/article/pii/S1746809426000236?dgcid=rss_sd_all</link><description>&lt;p&gt;Publication date: 1 May 2026&lt;/p&gt;&lt;p&gt;&lt;b&gt;Source:&lt;/b&gt; Biomedical Signal Processing and Control, Volume 116&lt;/p&gt;&lt;p&gt;Author(s): Minjun Wang, Houjin Chen, Yanfeng Li, Jia Sun, Luyifu Chen, Peng Liang&lt;/p&gt;</description><author>ScienceDirect Publication: Biomedical Signal Processing and Control</author><pubDate>Sun, 11 Jan 2026 13:09:00 GMT</pubDate><guid isPermaLink="true">https://www.sciencedirect.com/science/article/pii/S1746809426000236</guid></item><item><title>[BSPC] Morphology-enhanced CAM-guided SAM for weakly supervised breast lesion segmentation</title><link>https://www.sciencedirect.com/science/article/pii/S1746809426000637?dgcid=rss_sd_all</link><description>&lt;p&gt;Publication date: 1 May 2026&lt;/p&gt;&lt;p&gt;&lt;b&gt;Source:&lt;/b&gt; Biomedical Signal Processing and Control, Volume 116&lt;/p&gt;&lt;p&gt;Author(s): Xin Yue, Qing Zhao, Xiaoling Liu, Jianqiang Li, Jing Bai, Changwei Song, Suqin Liu, Rodrigo Moreno, Zhikai Yang, Stefano E. Romero, Gabriel Jimenez, Guanghui Fu&lt;/p&gt;</description><author>ScienceDirect Publication: Biomedical Signal Processing and Control</author><pubDate>Sun, 11 Jan 2026 13:09:00 GMT</pubDate><guid isPermaLink="true">https://www.sciencedirect.com/science/article/pii/S1746809426000637</guid></item><item><title>[BSPC] Medical image segmentation based on 3D PDC with Swin Transformer</title><link>https://www.sciencedirect.com/science/article/pii/S1746809426000704?dgcid=rss_sd_all</link><description>&lt;p&gt;Publication date: 1 May 2026&lt;/p&gt;&lt;p&gt;&lt;b&gt;Source:&lt;/b&gt; Biomedical Signal Processing and Control, Volume 116&lt;/p&gt;&lt;p&gt;Author(s): Lin Fan, Xiaojia Ding, Zhongmin Wang, Hai Wang, Rong Zhang&lt;/p&gt;</description><author>ScienceDirect Publication: Biomedical Signal Processing and Control</author><pubDate>Sun, 11 Jan 2026 13:09:00 GMT</pubDate><guid isPermaLink="true">https://www.sciencedirect.com/science/article/pii/S1746809426000704</guid></item><item><title>[EAAI] Cross-Granularity Fusion Vision Mamba UNet for medical image segmentation</title><link>https://www.sciencedirect.com/science/article/pii/S0952197626000679?dgcid=rss_sd_all</link><description>&lt;p&gt;Publication date: 1 March 2026&lt;/p&gt;&lt;p&gt;&lt;b&gt;Source:&lt;/b&gt; Engineering Applications of Artificial Intelligence, Volume 167, Part 1&lt;/p&gt;&lt;p&gt;Author(s): Tuersunjiang Baidi, Zitong Ren, Kurban Ubul, Alimjan Aysa, Boyuan Li, Shihao Wang&lt;/p&gt;</description><author>ScienceDirect Publication: Engineering Applications of Artificial Intelligence</author><pubDate>Sun, 11 Jan 2026 13:09:00 GMT</pubDate><guid isPermaLink="true">https://www.sciencedirect.com/science/article/pii/S0952197626000679</guid></item><item><title>[AIM] Siamese evolutionary masking: Enhancing the generalization of self-supervised medical image segmentation model</title><link>https://www.sciencedirect.com/science/article/pii/S0933365726000011?dgcid=rss_sd_all</link><description>&lt;p&gt;Publication date: Available online 7 January 2026&lt;/p&gt;&lt;p&gt;&lt;b&gt;Source:&lt;/b&gt; Artificial Intelligence in Medicine&lt;/p&gt;&lt;p&gt;Author(s): Yichen Zhi, Hongxia Bie, Jiali Wang, Zhao Jing&lt;/p&gt;</description><author>ScienceDirect Publication: Artificial Intelligence in Medicine</author><pubDate>Sun, 11 Jan 2026 13:09:00 GMT</pubDate><guid isPermaLink="true">https://www.sciencedirect.com/science/article/pii/S0933365726000011</guid></item><item><title>[NC] UP2D: Uncertainty-aware progressive pseudo-label denoising for source-free domain adaptive medical image segmentation</title><link>https://www.sciencedirect.com/science/article/pii/S0925231226000561?dgcid=rss_sd_all</link><description>&lt;p&gt;Publication date: Available online 9 January 2026&lt;/p&gt;&lt;p&gt;&lt;b&gt;Source:&lt;/b&gt; Neurocomputing&lt;/p&gt;&lt;p&gt;Author(s): Thanh-Huy Nguyen, Quang-Khai Bui-Tran, Manh D. Ho, Thinh B. Lam, Vi Vu, Hoang-Thien Nguyen, Phat Huynh, Ulas Bagci&lt;/p&gt;</description><author>ScienceDirect Publication: Neurocomputing</author><pubDate>Sun, 11 Jan 2026 13:08:59 GMT</pubDate><guid isPermaLink="true">https://www.sciencedirect.com/science/article/pii/S0925231226000561</guid></item><item><title>[KBS] BACFormer: A robust boundary-aware transformer for medical image segmentation</title><link>https://www.sciencedirect.com/science/article/pii/S0950705125022439?dgcid=rss_sd_all</link><description>&lt;p&gt;Publication date: 28 February 2026&lt;/p&gt;&lt;p&gt;&lt;b&gt;Source:&lt;/b&gt; Knowledge-Based Systems, Volume 335&lt;/p&gt;&lt;p&gt;Author(s): Zhiyong Huang, Mingyu Wang, Mingyang Hou, Zhi Yu, Shiwei Wang, Xiaoyu Li, Jiahong Wang, Yan Yan, Yushi Liu&lt;/p&gt;</description><author>ScienceDirect Publication: Knowledge-Based Systems</author><pubDate>Sun, 11 Jan 2026 13:08:59 GMT</pubDate><guid isPermaLink="true">https://www.sciencedirect.com/science/article/pii/S0950705125022439</guid></item><item><title>[KBS] Multimodality-based framework for enhanced skin lesion recognition over federated learning</title><link>https://www.sciencedirect.com/science/article/pii/S0950705125021859?dgcid=rss_sd_all</link><description>&lt;p&gt;Publication date: 28 February 2026&lt;/p&gt;&lt;p&gt;&lt;b&gt;Source:&lt;/b&gt; Knowledge-Based Systems, Volume 335&lt;/p&gt;&lt;p&gt;Author(s): Abdul Hai Karimi, Taimoor Khan, Chang Choi&lt;/p&gt;</description><author>ScienceDirect Publication: Knowledge-Based Systems</author><pubDate>Sun, 11 Jan 2026 13:08:59 GMT</pubDate><guid isPermaLink="true">https://www.sciencedirect.com/science/article/pii/S0950705125021859</guid></item><item><title>[ESWA] WBDM-ECRF: A bridge diffusion model with efficient conditional random field for skin lesion segmentation</title><link>https://www.sciencedirect.com/science/article/pii/S0957417425046159?dgcid=rss_sd_all</link><description>&lt;p&gt;Publication date: 25 April 2026&lt;/p&gt;&lt;p&gt;&lt;b&gt;Source:&lt;/b&gt; Expert Systems with Applications, Volume 307&lt;/p&gt;&lt;p&gt;Author(s): Hefeng Ji, Jing Xiao, Jimin Liu, Haoyong Yu, Chao Chen&lt;/p&gt;</description><author>ScienceDirect Publication: Expert Systems with Applications</author><pubDate>Sun, 11 Jan 2026 13:08:59 GMT</pubDate><guid isPermaLink="true">https://www.sciencedirect.com/science/article/pii/S0957417425046159</guid></item><item><title>[ESWA] MED-Net: Leveraging multi-visual encoding and Manhattan-distance pooling for robust medical image segmentation</title><link>https://www.sciencedirect.com/science/article/pii/S0957417426000175?dgcid=rss_sd_all</link><description>&lt;p&gt;Publication date: 1 May 2026&lt;/p&gt;&lt;p&gt;&lt;b&gt;Source:&lt;/b&gt; Expert Systems with Applications, Volume 308&lt;/p&gt;&lt;p&gt;Author(s): Zhaozhao Su, Yuxuan Luo, Shiji Song, Zhiqiang Zhu, Jiajie Wang, Zhihua Fang, Bowen Wang, Liejun Wang&lt;/p&gt;</description><author>ScienceDirect Publication: Expert Systems with Applications</author><pubDate>Sun, 11 Jan 2026 13:08:59 GMT</pubDate><guid isPermaLink="true">https://www.sciencedirect.com/science/article/pii/S0957417426000175</guid></item><item><title>[ESWA] A colon polyp segmentation network via collaborative decision-making of mixture of experts</title><link>https://www.sciencedirect.com/science/article/pii/S095741742600031X?dgcid=rss_sd_all</link><description>&lt;p&gt;Publication date: 1 May 2026&lt;/p&gt;&lt;p&gt;&lt;b&gt;Source:&lt;/b&gt; Expert Systems with Applications, Volume 308&lt;/p&gt;&lt;p&gt;Author(s): Wenchao Zhang, Wenhui Ye, Zhenhua Yu, Jianguo Ju&lt;/p&gt;</description><author>ScienceDirect Publication: Expert Systems with Applications</author><pubDate>Sun, 11 Jan 2026 13:08:59 GMT</pubDate><guid isPermaLink="true">https://www.sciencedirect.com/science/article/pii/S095741742600031X</guid></item><item><title>[PR] MFSF&lt;sup&gt;2&lt;/sup&gt;-NET: towards improved skin lesion diagnosis via multi-scale and multi-frequency-domain fusion</title><link>https://www.sciencedirect.com/science/article/pii/S003132032501581X?dgcid=rss_sd_all</link><description>&lt;p&gt;Publication date: June 2026&lt;/p&gt;&lt;p&gt;&lt;b&gt;Source:&lt;/b&gt; Pattern Recognition, Volume 174&lt;/p&gt;&lt;p&gt;Author(s): Hui Liu, Yibo Dou, Meng Cao, Kai Wang, Yunmin Zou, Guoxiang Ma, Chaohui Li&lt;/p&gt;</description><author>ScienceDirect Publication: Pattern Recognition</author><pubDate>Sun, 11 Jan 2026 13:08:58 GMT</pubDate><guid isPermaLink="true">https://www.sciencedirect.com/science/article/pii/S003132032501581X</guid></item><item><title>[PR] Efficient breast cancer segmentation via Brownian Bridge diffusion with semantic fusion strategy</title><link>https://www.sciencedirect.com/science/article/pii/S0031320325016565?dgcid=rss_sd_all</link><description>&lt;p&gt;Publication date: June 2026&lt;/p&gt;&lt;p&gt;&lt;b&gt;Source:&lt;/b&gt; Pattern Recognition, Volume 174&lt;/p&gt;&lt;p&gt;Author(s): Feiyan Feng, Tianyu Liu, Fulin Zheng, Yanshen Sun, Hong Wang&lt;/p&gt;</description><author>ScienceDirect Publication: Pattern Recognition</author><pubDate>Sun, 11 Jan 2026 13:08:58 GMT</pubDate><guid isPermaLink="true">https://www.sciencedirect.com/science/article/pii/S0031320325016565</guid></item><item><title>[PR] Empowering 2D neural network for 3D medical image segmentation via neighborhood information fusion</title><link>https://www.sciencedirect.com/science/article/pii/S003132032501698X?dgcid=rss_sd_all</link><description>&lt;p&gt;Publication date: July 2026&lt;/p&gt;&lt;p&gt;&lt;b&gt;Source:&lt;/b&gt; Pattern Recognition, Volume 175&lt;/p&gt;&lt;p&gt;Author(s): Qiankun Li, Xiaolong Huang, Yani Zhang, Bo Fang, Duo Hong, Junxin Chen&lt;/p&gt;</description><author>ScienceDirect Publication: Pattern Recognition</author><pubDate>Sun, 11 Jan 2026 13:08:58 GMT</pubDate><guid isPermaLink="true">https://www.sciencedirect.com/science/article/pii/S003132032501698X</guid></item><item><title>[MedIA] SicTTA: Single image continual test time adaptation for medical image segmentation</title><link>https://www.sciencedirect.com/science/article/pii/S1361841525004050?dgcid=rss_sd_all</link><description>&lt;p&gt;Publication date: February 2026&lt;/p&gt;&lt;p&gt;&lt;b&gt;Source:&lt;/b&gt; Medical Image Analysis, Volume 108&lt;/p&gt;&lt;p&gt;Author(s): Jianghao Wu, Xinya Liu, Guotai Wang, Shaoting Zhang&lt;/p&gt;</description><author>ScienceDirect Publication: Medical Image Analysis</author><pubDate>Sun, 11 Jan 2026 13:08:58 GMT</pubDate><guid isPermaLink="true">https://www.sciencedirect.com/science/article/pii/S1361841525004050</guid></item><item><title>[MedIA] Adaptive mix for semi-supervised medical image segmentation</title><link>https://www.sciencedirect.com/science/article/pii/S1361841525004037?dgcid=rss_sd_all</link><description>&lt;p&gt;Publication date: February 2026&lt;/p&gt;&lt;p&gt;&lt;b&gt;Source:&lt;/b&gt; Medical Image Analysis, Volume 108&lt;/p&gt;&lt;p&gt;Author(s): Zhiqiang Shen, Peng Cao, Junming Su, Jinzhu Yang, Osmar R. Zaiane&lt;/p&gt;</description><author>ScienceDirect Publication: Medical Image Analysis</author><pubDate>Sun, 11 Jan 2026 13:08:58 GMT</pubDate><guid isPermaLink="true">https://www.sciencedirect.com/science/article/pii/S1361841525004037</guid></item><item><title>[MedIA] Test-time generative augmentation for medical image segmentation</title><link>https://www.sciencedirect.com/science/article/pii/S1361841525004487?dgcid=rss_sd_all</link><description>&lt;p&gt;Publication date: March 2026&lt;/p&gt;&lt;p&gt;&lt;b&gt;Source:&lt;/b&gt; Medical Image Analysis, Volume 109&lt;/p&gt;&lt;p&gt;Author(s): Xiao Ma, Yuhui Tao, Zetian Zhang, Yuhan Zhang, Xi Wang, Sheng Zhang, Zexuan Ji, Yizhe Zhang, Qiang Chen, Guang Yang&lt;/p&gt;</description><author>ScienceDirect Publication: Medical Image Analysis</author><pubDate>Sun, 11 Jan 2026 13:08:58 GMT</pubDate><guid isPermaLink="true">https://www.sciencedirect.com/science/article/pii/S1361841525004487</guid></item><item><title>[MedIA] CHAP: Channel-spatial hierarchical adversarial perturbation for semi-supervised medical image segmentation</title><link>https://www.sciencedirect.com/science/article/pii/S1361841525004645?dgcid=rss_sd_all</link><description>&lt;p&gt;Publication date: March 2026&lt;/p&gt;&lt;p&gt;&lt;b&gt;Source:&lt;/b&gt; Medical Image Analysis, Volume 109&lt;/p&gt;&lt;p&gt;Author(s): Si-Ping Zhou, Zhi-Fang Gong, Kai-Ni Wang, Ping Zhou, Yang Chen, Guang-Quan Zhou&lt;/p&gt;</description><author>ScienceDirect Publication: Medical Image Analysis</author><pubDate>Sun, 11 Jan 2026 13:08:58 GMT</pubDate><guid isPermaLink="true">https://www.sciencedirect.com/science/article/pii/S1361841525004645</guid></item><item><title>[MP] Uncertaintyguided testtime optimization for personalizing segmentation models in longitudinal medical imaging</title><link>https://aapm.onlinelibrary.wiley.com/doi/10.1002/mp.70206?af=R</link><description>Medical Physics, Volume 53, Issue 1, January 2026.</description><author>Wiley: Medical Physics: Table of Contents</author><pubDate>Mon, 22 Dec 2025 04:06:14 GMT</pubDate><guid isPermaLink="true">10.1002/mp.70206</guid></item><item><title>[MP] Accelerating vision foundation model for efficient medical image segmentation</title><link>https://aapm.onlinelibrary.wiley.com/doi/10.1002/mp.70193?af=R</link><description>Medical Physics, Volume 53, Issue 1, January 2026.</description><author>Wiley: Medical Physics: Table of Contents</author><pubDate>Sat, 20 Dec 2025 03:50:16 GMT</pubDate><guid isPermaLink="true">10.1002/mp.70193</guid></item><item><title>[TIP] Uncertainty-Guided Adaptive Correction for Semi-Supervised Medical Image Segmentation</title><link>http://ieeexplore.ieee.org/document/11273087</link><description>Consistent perturbation strategies have emerged as a dominant paradigm in semi-supervised medical image segmentation. Nevertheless, prevailing approaches inadequately address two critical challenges: 1) prediction errors induced by data uncertainty from distribution shifts, and 2) loss instability caused by model uncertainty in parameter generalization. To overcome these limitations, we propose an Uncertainty-Guided Adaptive Correction (UGAC) framework with three key innovations. First, we develop a dual-path uncertainty rectification mechanism that employs normalized entropy measures to detect error-prone regions in unlabeled predictions, followed by bilateral correction through confidence-weighted fusion. Second, we introduce adversarial consistency constraints that leverage labeled data to discriminate authentic segmentation patterns, effectively regularizing uncertainty propagation in unlabeled predictions through spectral normalization. Third, we architect a frequency-aware segmentation backbone through our novel Freqfusion module, which performs adaptive spectral decomposition during feature decoding to explicitly disentangle high-frequency (boundary-aware) and low-frequency (structural) components, thereby enhancing anatomical boundary sensitivity. Comprehensive evaluations on MM-WHS, BUSI, M&amp;amp;Ms and PROMISE12 datasets demonstrate UGACs superior performance. The proposed framework exhibits robust generalizability across CT, MRI, and ultrasound modalities, while achieving significantly lower computational complexity than baseline UNet implementations. The code will be available at https://github.com/SIGMACX/UGAC.</description><author>IEEE Transactions on Image Processing - new TOC</author><pubDate>Wed, 03 Dec 2025 13:18:43 GMT</pubDate><guid isPermaLink="true">http://ieeexplore.ieee.org/document/11273087</guid></item><item><title>[TPAMI] H2OT: Hierarchical Hourglass Tokenizer for Efficient Video Pose Transformers</title><link>http://ieeexplore.ieee.org/document/11155208</link><description>Transformers have been successfully applied in the field of video-based 3D human pose estimation. However, the high computational costs of these video pose transformers (VPTs) make them impractical on resource-constrained devices. In this paper, we present a hierarchical plug-and-play pruning-and-recovering framework, called Hierarchical Hourglass Tokenizer (H2OT), for efficient transformer-based 3D human pose estimation from videos. H2OT begins with progressively pruning pose tokens of redundant frames and ends with recovering full-length sequences, resulting in a few pose tokens in the intermediate transformer blocks and thus improving the model efficiency. It works with two key modules, namely, a Token Pruning Module (TPM) and a Token Recovering Module (TRM). TPM dynamically selects a few representative tokens to eliminate the redundancy of video frames, while TRM restores the detailed spatio-temporal information based on the selected tokens, thereby expanding the network output to the original full-length temporal resolution for fast inference. Our method is general-purpose: it can be easily incorporated into common VPT models on both seq2seq and seq2frame pipelines while effectively accommodating different token pruning and recovery strategies. In addition, our H2OT reveals that maintaining the full pose sequence is unnecessary, and a few pose tokens of representative frames can achieve both high efficiency and estimation accuracy. Extensive experiments on multiple benchmark datasets demonstrate both the effectiveness and efficiency of the proposed method.</description><author>IEEE Transactions on Pattern Analysis and Machine Intelligence - new TOC</author><pubDate>Wed, 10 Sep 2025 13:15:34 GMT</pubDate><guid isPermaLink="true">http://ieeexplore.ieee.org/document/11155208</guid></item><item><title>[JBHI] SegTom: A 3D Volumetric Medical Image Segmentation Framework for Thoracoabdominal Multi-Organ Anatomical Structures</title><link>http://ieeexplore.ieee.org/document/11151753</link><description>Accurate segmentation of thoracoabdominal anatomical structures in three-dimensional medical imaging modalities is fundamental for informed clinical decision-making across a wide array of medical disciplines. Current approaches often struggle to efficiently and comprehensively process this regions intricate and heterogeneous anatomical information, leading to suboptimal outcomes in diagnosis, treatment planning, and disease management. To address this challenge, we introduce SegTom, a novel volumetric segmentation framework equipped with a cutting-edge SegTom Block specifically engineered to effectively capture the complex anatomical representations inherent to the thoracoabdominal region. This SegTom Block incorporates a hierarchical anatomical-representation decomposition to facilitate efficient information exchange by decomposing the computationally intensive self-attention mechanism and cost-effectively aggregating the extracted representations. Rigorous validation of SegTom across nine diverse datasets, encompassing both computed tomography (CT) and magnetic resonance imaging (MRI) modalities, consistently demonstrates high performance across a broad spectrum of anatomical structures. Specifically, SegTom achieves a mean Dice similarity coefficient (DSC) of 87.29% for cardiac segmentation on the MM-WHS MRI dataset, 83.48% for multi-organ segmentation on the BTCV abdominal CT dataset, and 92.01% for airway segmentation on a dedicated CT dataset.</description><author>IEEE Journal of Biomedical and Health Informatics - new TOC</author><pubDate>Fri, 05 Sep 2025 13:16:46 GMT</pubDate><guid isPermaLink="true">http://ieeexplore.ieee.org/document/11151753</guid></item><item><title>[TMI] EPDiff: Erasure Perception Diffusion Model for Unsupervised Anomaly Detection in Preoperative Multimodal Images</title><link>http://ieeexplore.ieee.org/document/11121881</link><description>Unsupervised anomaly detection (UAD) methods typically detect anomalies by learning and reconstructing the normative distribution. However, since anomalies constantly invade and affect their surroundings, sub-healthy areas in the junction present structural deformations that could be easily misidentified as anomalies, posing difficulties for UAD methods that solely learn the normative distribution. The use of multimodal images can facilitate to address the above challenges, as they can provide complementary information of anomalies. Therefore, this paper propose a novel method for UAD in preoperative multimodal images, called Erasure Perception Diffusion model (EPDiff). First, the Local Erasure Progressive Training (LEPT) framework is designed to better rebuild sub-healthy structures around anomalies through the diffusion model with a two-phase process. Initially, healthy images are used to capture deviation features labeled as potential anomalies. Then, these anomalies are locally erased in multimodal images to progressively learn sub-healthy structures, obtaining a more detailed reconstruction around anomalies. Second, the Global Structural Perception (GSP) module is developed in the diffusion model to realize global structural representation and correlation within images and between modalities through interactions of high-level semantic information. In addition, a training-free module, named Multimodal Attention Fusion (MAF) module, is presented for weighted fusion of anomaly maps between different modalities and obtaining binary anomaly outputs. Experimental results show that EPDiff improves the AUPRC and mDice scores by 2% and 3.9% on BraTS2021, and by 5.2% and 4.5% on Shifts over the state-of-the-art methods, which proves the applicability of EPDiff in diverse anomaly diagnosis. The code is available at https://github.com/wjiazheng/EPDiff</description><author>IEEE Transactions on Medical Imaging - new TOC</author><pubDate>Mon, 11 Aug 2025 13:16:40 GMT</pubDate><guid isPermaLink="true">http://ieeexplore.ieee.org/document/11121881</guid></item><item><title>[TMI] GM-ABS: Promptable Generalist Model Drives Active Barely Supervised Training in Specialist Model for 3D Medical Image Segmentation</title><link>http://ieeexplore.ieee.org/document/11119675</link><description>Semi-supervised learning (SSL) has greatly advanced 3D medical image segmentation by alleviating the need for intensive labeling by radiologists. While previous efforts focused on model-centric advancements, the emergence of foundational generalist models like the Segment Anything Model (SAM) is expected to reshape the SSL landscape. Although these generalists usually show performance gaps relative to previous specialists in medical imaging, they possess impressive zero-shot segmentation abilities with manual prompts. Thus, this capability could serve as free lunch for training specialists, offering future SSL a promising data-centric perspective, especially revolutionizing both pseudo and expert labeling strategies to enhance the data pool. In this regard, we propose the Generalist Model-driven Active Barely Supervised (GM-ABS) learning paradigm, for developing specialized 3D segmentation models under extremely limited (barely) annotation budgets, e.g., merely cross-labeling three slices per selected scan. In specific, building upon a basic mean-teacher SSL framework, GM-ABS modernizes the SSL paradigm with two key data-centric designs: (i) Specialist-generalist collaboration, where the in-training specialist leverages class-specific positional prompts derived from class prototypes to interact with the frozen class-agnostic generalist across multiple views to achieve noisy-yet-effective label augmentation. Then, the specialist robustly assimilates the augmented knowledge via noise-tolerant collaborative learning. (ii) Expert-model collaboration that promotes active cross-labeling with notably low labeling efforts. This design progressively furnishes the specialist with informative and efficient supervision via a human-in-the-loop manner, which in turn benefits the quality of class-specific prompts. Extensive experiments on three benchmark datasets highlight the promising performance of GM-ABS over recent SSL approaches under extremely constrained labeling resources.</description><author>IEEE Transactions on Medical Imaging - new TOC</author><pubDate>Thu, 07 Aug 2025 13:17:44 GMT</pubDate><guid isPermaLink="true">http://ieeexplore.ieee.org/document/11119675</guid></item><item><title>[TMI] Collaborative Learning of Augmentation and Disentanglement for Semi-Supervised Domain Generalized Medical Image Segmentation</title><link>http://ieeexplore.ieee.org/document/11115113</link><description>This paper explores a challenging yet realistic scenario: semi-supervised domain generalization (SSDG) that includes label scarcity and domain shift problems. We pinpoint that the limitations of previous SSDG methods lie in 1) neglecting the difference between domain shifts existing within a training dataset (intra-domain shift, IDS) and those occurring between training and testing datasets (cross-domain shift, CDS) and 2) overlooking the interplay between label scarcity and domain shifts, resulting in these methods merely stitching together semi-supervised learning (SSL) and domain generalization (DG) techniques. Considering these limitations, we propose a novel perspective to decompose SSDG into the combination of unsupervised domain adaptation (UDA) and DG problems. To this end, we design a causal augmentation and disentanglement framework (CausalAD) for semi-supervised domain generalized medical image segmentation. Concretely, CausalAD involves two collaborative processes: an augmentation process, which utilizes disentangled style factors to perform style augmentation for UDA, and a disentanglement process, which decouples domain-invariant (content) and domain-variant (noise and style) features for DG. Furthermore, we propose a proxy-based self-paced training strategy (ProSPT) to guide the training of CausalAD by gradually selecting unlabeled image pixels with high-quality pseudo labels in a self-paced training manner. Finally, we introduce a hierarchical structural causal model (HSCM) to explain the intuition and concept behind our method. Extensive experiments in the cross-sequence, cross-site, and cross-modality semi-supervised domain generalized medical image segmentation settings show the effectiveness of CausalAD and its superiority over the state-of-the-art. The code is available at https://github.com/Senyh/CausalAD</description><author>IEEE Transactions on Medical Imaging - new TOC</author><pubDate>Wed, 06 Aug 2025 13:17:46 GMT</pubDate><guid isPermaLink="true">http://ieeexplore.ieee.org/document/11115113</guid></item><item><title>[TMI] A Trust-Guided Approach to MR Image Reconstruction With Side Information</title><link>http://ieeexplore.ieee.org/document/11105520</link><description>Reducing MRI scan times can improve patient care and lower healthcare costs. Many acceleration methods are designed to reconstruct diagnostic-quality images from sparse k-space data, via an ill-posed or ill-conditioned linear inverse problem (LIP). To address the resulting ambiguities, it is crucial to incorporate prior knowledge into the optimization problem, e.g., in the form of regularization. Another form of prior knowledge less commonly used in medical imaging is the readily available auxiliary data (a.k.a. side information) obtained from sources other than the current acquisition. In this paper, we present the Trust-Guided Variational Network (TGVN), an end-to-end deep learning framework that effectively and reliably integrates side information into LIPs. We demonstrate its effectiveness in multi-coil, multi-contrast MRI reconstruction, where incomplete or low-SNR measurements from one contrast are used as side information to reconstruct high-quality images of another contrast from heavily under-sampled data. TGVN is robust across different contrasts, anatomies, and field strengths. Compared to baselines utilizing side information, TGVN achieves superior image quality while preserving subtle pathological features even at challenging acceleration levels, drastically speeding up acquisition while minimizing hallucinations. Source code and dataset splits are available on github.com/sodicksonlab/TGVN</description><author>IEEE Transactions on Medical Imaging - new TOC</author><pubDate>Thu, 31 Jul 2025 13:17:16 GMT</pubDate><guid isPermaLink="true">http://ieeexplore.ieee.org/document/11105520</guid></item><item><title>[TMI] Dual Cross-Image Semantic Consistency With Self-Aware Pseudo Labeling for Semi-Supervised Medical Image Segmentation</title><link>http://ieeexplore.ieee.org/document/11104231</link><description>Semi-supervised learning has proven highly effective in tackling the challenge of limited labeled training data in medical image segmentation. In general, current approaches, which rely on intra-image pixel-wise consistency training via pseudo-labeling, overlook the consistency at more comprehensive semantic levels (e.g., object region) and suffer from severe discrepancy of extracted features resulting from an imbalanced number of labeled and unlabeled data. To overcome these limitations, we present a new Dual Cross-image Semantic Consistency (DuCiSC) learning framework, for semi-supervised medical image segmentation. Concretely, beyond enforcing pixel-wise semantic consistency, DuCiSC proposes dual paradigms to encourage region-level semantic consistency across: 1) labeled and unlabeled images; and 2) labeled and fused images, by explicitly aligning their prototypes. Relying on the dual paradigms, DuCiSC can effectively establish consistent cross-image semantics via prototype representations, thereby addressing the feature discrepancy issue. Moreover, we devise a novel self-aware confidence estimation strategy to accurately select reliable pseudo labels, allowing for exploiting the training dynamics of unlabeled data. Our DuCiSC method is extensively validated on four datasets, including two popular binary benchmarks in segmenting the left atrium and pancreas, a multi-class Automatic Cardiac Diagnosis Challenge dataset, and a challenging scenario of segmenting the inferior alveolar nerve that features complicated anatomical structures, showing superior segmentation results over previous state-of-the-art approaches. Our code is publicly available at https://github.com/ShanghaiTech-IMPACT/DuCiSC</description><author>IEEE Transactions on Medical Imaging - new TOC</author><pubDate>Wed, 30 Jul 2025 13:17:43 GMT</pubDate><guid isPermaLink="true">http://ieeexplore.ieee.org/document/11104231</guid></item><item><title>[TMI] 3D Deep-Learning-Based Segmentation of Human Skin Sweat Glands and Their 3D Morphological Response to Temperature Variations</title><link>http://ieeexplore.ieee.org/document/11098795</link><description>Skin, the primary regulator of heat exchange, relies on sweat glands for thermoregulation. Alterations in sweat gland morphology play a crucial role in various pathological conditions and clinical diagnoses. Current methods for observing sweat gland morphology are limited by their two-dimensional, in vitro, and destructive nature, underscoring the urgent need for real-time, non-invasive, quantifiable technologies. We proposed a novel three-dimensional (3D) transformer-based segmentation framework, enabling quite precise 3D sweat gland segmentation from skin volume data captured by optical coherence tomography (OCT). We quantitatively reveal, for the first time, 3D sweat gland morphological changes with temperature: for instance, volume, surface area, and length increase by 42.0%, 26.4%, and 12.8% at 43C vs. 10C (all p &lt;0.001), while S/V ratio decreases (p =0.01). By establishing a benchmark for normal sweat gland morphology and offering a real-time, non-invasive tool for quantifying 3D structural parameters, our approach facilitates the study of individual variability and pathological changes in sweat gland morphology, contributing to advancements in dermatological research and clinical applications.</description><author>IEEE Transactions on Medical Imaging - new TOC</author><pubDate>Tue, 29 Jul 2025 13:17:02 GMT</pubDate><guid isPermaLink="true">http://ieeexplore.ieee.org/document/11098795</guid></item><item><title>[TMI] Visualization of Breast Cancer Using Contrast-Enhanced Optical Coherence Elastography Based on Tissue Heterogeneity</title><link>http://ieeexplore.ieee.org/document/11098988</link><description>By mapping the mechanical properties of tissue, elastography can improve identification of breast cancer. On the macro-scale, ultrasound elastography and magnetic resonance elastography have emerged as effective clinical methods for the diagnosis of tumors. On the micro-scale, optical coherence elastography (OCE) shows promise for intraoperative tumor margin assessment during breast-conserving surgery. Whilst several OCE studies have demonstrated strong potential, the mechanical models used require the assumption of uniaxial stress throughout the sample. However, breast tissue is heterogeneous and contains compressible features (e.g., ducts and blood vessels) and collagen-rich fibrotic features (e.g., stroma). This heterogeneity can invalidate the assumption of uniaxial stress and reduce the accuracy of OCE, often making it challenging to interpret images. Here, we demonstrate a new variant of OCE based on mapping the Euler angle, i.e., the angle between the principal compression and the loading axis induced by tissue heterogeneity, which removes the assumption of uniaxial deformation. This is enabled by a hybrid three-dimensional (3-D) displacement estimation method that combines phase-sensitive detection and complex cross-correlation, providing access to the 3-D displacement and 3-D strain tensor on the micro-scale. Through experiments on phantoms, we demonstrate that an accuracy of 98.6%, a sensitivity of 0.95 (i.e., 16.58 mrad), and a spatial resolution as high as  $36~\mu $ m can be achieved in Euler angle imaging. We demonstrate the potential of Euler angle imaging for visualization of breast cancer. Through close correspondence with histology, our results show that mapping the Euler angle provides additional contrast to both optical coherence tomography and a compression OCE technique in identifying cancer. Mapping the Euler angle in breast tissue may provide a new biomarker for intraoperative tumor margin assessment.</description><author>IEEE Transactions on Medical Imaging - new TOC</author><pubDate>Tue, 29 Jul 2025 13:17:02 GMT</pubDate><guid isPermaLink="true">http://ieeexplore.ieee.org/document/11098988</guid></item><item><title>[TMI] A Deep Learning Multimodal Fusion-Based Method for Cell and Nucleus Segmentation</title><link>http://ieeexplore.ieee.org/document/11096725</link><description>In recent years, deep learning has been widely utilized in the fields of biomedical image segmentation and cellular image analysis. Supervised deep neural networks trained on annotated data have demonstrated good performance in tasks related to cell and nucleus segmentation. However, the use of supervised models necessitates carefully constructed training data and a substantial amount of ground truth information. Unfortunately, high-quality annotated data for cellular images are scarce. To address the issue of limited datasets, we propose a cell and nucleus segmentation method based on deep learning multimodal fusion. The proposed method includes three modules: a segmentation fundamental module, a multimodal prompter module, and an object output module. This comprehensive approach enables cell and nucleus segmentation tasks to be performed without the need for retraining on new data. The segmentation fundamental module is the core of the framework, as it provides essential segmentation capabilities. By leveraging preexisting models trained on natural imagery, this module effectively performs cell segmentation by incorporating prior knowledge. The multimodal prompter module, a pretrained model, aids in combining image and textual information. It employs a data fusion technique for multiple modalities to deliver prompts that steer the networks output, thereby avoiding the constraints inherent to single-modality approaches. The object output module combines the inputs from the preceding modules to generate the final segmentation output. The experimental validation confirms the superiority of the proposed method, which outperforms comparative methods in cell and nucleus segmentation tasks and has promise for future applications in cell tracking.</description><author>IEEE Transactions on Medical Imaging - new TOC</author><pubDate>Fri, 25 Jul 2025 13:18:29 GMT</pubDate><guid isPermaLink="true">http://ieeexplore.ieee.org/document/11096725</guid></item><item><title>[TMI] SarAdapter: Prioritizing Attention on Semantic-Aware Representative Tokens for Enhanced Medical Image Segmentation</title><link>http://ieeexplore.ieee.org/document/11089976</link><description>Transformer-based segmentation methods exhibit considerable potential in medical image analysis. However, their improved performance often comes with increased computational complexity, limiting their application in resource-constrained medical settings. Prior methods follow two independent tracks: (i) accelerating existing networks via semantic-aware routing, and (ii) optimizing token adapter design to enhance network performance. Despite directness, they encounter unavoidable defects (e.g., inflexible acceleration techniques or non-discriminative processing) limiting further improvements of quality-complexity trade-off. To address these shortcomings, we integrate these schemes by proposing the semantic-aware adapter (SarAdapter), which employs a semantic-based routing strategy, leveraging neural operators (ViT and CNN) of varying complexities. Specifically, it merges semantically similar tokens volume into low-resolution regions while preserving semantically distinct tokens as high-resolution regions. Additionally, we introduce a Mixed-adapter unit, which adaptively selects convolutional operators of varying complexities to better model regions at different scales. We evaluate our method on four medical datasets from three modalities and show that it achieves a superior balance between accuracy, model size, and efficiency. Notably, our proposed method achieves state-of-the-art segmentation quality on the Synapse dataset while reducing the number of tokens by 65.6%, signifying a substantial improvement in the efficiency of ViTs for the segmentation task.</description><author>IEEE Transactions on Medical Imaging - new TOC</author><pubDate>Tue, 22 Jul 2025 13:16:21 GMT</pubDate><guid isPermaLink="true">http://ieeexplore.ieee.org/document/11089976</guid></item><item><title>[TMI] EICSeg: Universal Medical Image Segmentation via Explicit In-Context Learning</title><link>http://ieeexplore.ieee.org/document/11090002</link><description>Deep learning models for medical image segmentation often struggle with task-specific characteristics, limiting their generalization to unseen tasks with new anatomies, labels, or modalities. Retraining or fine-tuning these models requires substantial human effort and computational resources. To address this, in-context learning (ICL) has emerged as a promising paradigm, enabling query image segmentation by conditioning on example image-mask pairs provided as prompts. Unlike previous approaches that rely on implicit modeling or non-end-to-end pipelines, we redefine the core interaction mechanism in ICL as an explicit retrieval process, termed E-ICL, benefiting from the emergence of vision foundation models (VFMs). E-ICL captures dense correspondences between queries and prompts at minimal learning cost and leverages them to dynamically weight multi-class prompt masks. Built upon E-ICL, we propose EICSeg, the first end-to-end ICL framework that integrates complementary VFMs for universal medical image segmentation. Specifically, we introduce a lightweight SD-Adapter to bridge the distinct functionalities of the VFMs, enabling more accurate segmentation predictions. To fully exploit the potential of EICSeg, we further design a scalable self-prompt training strategy and an adaptive token-to-image prompt selection mechanism, facilitating both efficient training and inference. EICSeg is trained on 47 datasets covering diverse modalities and segmentation targets. Experiments on nine unseen datasets demonstrate its strong few-shot generalization ability, achieving an average Dice score of 74.0%, outperforming existing in-context and few-shot methods by 4.5%, and reducing the gap to task-specific models to 10.8%. Even with a single prompt, EICSeg achieves a competitive average Dice score of 60.1%. Notably, it performs automatic segmentation without manual prompt engineering, delivering results comparable to interactive models while requiring minimal labeled data. Source code will be available at https://github.com/zerone-fg/EICSeg</description><author>IEEE Transactions on Medical Imaging - new TOC</author><pubDate>Tue, 22 Jul 2025 13:16:21 GMT</pubDate><guid isPermaLink="true">http://ieeexplore.ieee.org/document/11090002</guid></item><item><title>[TMI] REDNet: Reliable Evidential Discounting Network for Multi-Modality Medical Image Segmentation</title><link>http://ieeexplore.ieee.org/document/11087642</link><description>In the field of computer-aided diagnosis, particularly for tumor diseases, segmentation is a prerequisite and primary step. Multi-modality images become essential for achieving accurate segmentation, which offer critical insights beyond the limitations of single-modality data. However, different modalities and images may suffer from different types of data imperfection, such as intensity non-uniformity, motion artifact, and low quality due to hardware limitations, which challenge image segmentation algorithms. To address this challenge, we propose a Reliable Evidential Discounting Network (REDNet), which is composed of three main modules: 1) the Intra-modality Consistency Evaluation Module (ICEM) measuring the data cohesion within the same modality; 2) the Cross-modality Difference Aggregation Module (CDAM) identifing data discrepancy across modalities; 3) the Discounting Fusion Module (DFM) processing the multi-modality evidence by applying discounting strategies to fuse the data. This approach maintains segmentation accuracy by effectively integrating multi-modality evidence, while discounting the influence of lower-quality data, ensuring reliable results despite the presence of image imperfections. We evaluated REDNet on two distinct datasets, BRATS2021 and an in-house pancreas dataset from Changhai Hospital. REDNet outperforms other methods, particularly in scenarios with imperfect image sources, and achieves reliable results in multi-modality tumor segmentation.</description><author>IEEE Transactions on Medical Imaging - new TOC</author><pubDate>Mon, 21 Jul 2025 13:19:10 GMT</pubDate><guid isPermaLink="true">http://ieeexplore.ieee.org/document/11087642</guid></item><item><title>[TMI] SegMamba-V2: Long-Range Sequential Modeling Mamba for General 3-D Medical Image Segmentation</title><link>http://ieeexplore.ieee.org/document/11084842</link><description>The Transformer architecture has demonstrated remarkable results in 3D medical image segmentation due to its capability of modeling global relationships. However, it poses a significant computational burden when processing high-dimensional medical images. Mamba, as a State Space Model (SSM), has recently emerged as a notable approach for modeling long-range dependencies in sequential data. Although a substantial amount of Mamba-based research has focused on natural language and 2D image processing, few studies explore the capability of Mamba on 3D medical images. In this paper, we propose SegMamba-V2, a novel 3D medical image segmentation model, to effectively capture long-range dependencies within whole-volume features at each scale. To achieve this goal, we first devise a hierarchical scale downsampling strategy to enhance the receptive field and mitigate information loss during downsampling. Furthermore, we design a novel tri-orientated spatial Mamba block that extends the global dependency modeling process from one plane to three orthogonal planes to improve feature representation capability. Moreover, we collect and annotate a large-scale dataset (named CRC-2000) with fine-grained categories to facilitate benchmarking evaluation in 3D colorectal cancer (CRC) segmentation. We evaluate the effectiveness of our SegMamba-V2 on CRC-2000 and three other large-scale 3D medical image segmentation datasets, covering various modalities, organs, and segmentation targets. Experimental results demonstrate that our Segmamba-V2 outperforms state-of-the-art methods by a significant margin, which indicates the universality and effectiveness of the proposed model on 3D medical image segmentation tasks. The code for SegMamba-V2 is publicly available at: https://github.com/ge-xing/SegMamba-V2</description><author>IEEE Transactions on Medical Imaging - new TOC</author><pubDate>Fri, 18 Jul 2025 13:16:20 GMT</pubDate><guid isPermaLink="true">http://ieeexplore.ieee.org/document/11084842</guid></item><item><title>[TBME] Assessment of Breast Composition With a Transmission-Based Microwave Imaging System</title><link>http://ieeexplore.ieee.org/document/11059861</link><description>Breast density is a key risk factor for breast cancer, but it is typically unknown before a first mammogram. Microwave imaging, proposed for cancer detection and monitoring, offers potential for measuring the composition of the breast. Objective: Assess the potential of microwave imaging as a method for estimating breast composition via correlation with mammogram metrics. Methods: Transmission based microwave imaging was applied to a cohort of 110 participants with prior mammograms. Several techniques were developed to estimate breast composition from microwave images, including average permittivity calculation, image thresholding and segmentation, and estimation of the fraction of glandular tissue in each pixel. These measures were compared to breast density category and percent density available from mammograms. Results: Average permittivity from microwave images correlated strongly with mammogram-based metrics. For the average permittivity, statistical analysis using one-way ANOVA revealed significant group differences across the various breast density categories. Thresholding and segmentation involved more detailed analysis of the images, and showed potential as alternative approaches to differentiating between breast composition categories. Conclusions: This study represents the largest cohort of healthy participants in which microwave breast images were compared with breast composition data available from clinical imaging. The cohort is well balanced across all categories. It highlights microwave imaging as a safe, portable, and affordable tool for non-invasive breast composition assessment and early cancer risk detection. Significance: The correlation between microwave imaging and mammogram-based breast density metrics highlights the potential for microwave imaging as a novel method for assessment of breast composition.</description><author>IEEE Transactions on Biomedical Engineering - new TOC</author><pubDate>Mon, 30 Jun 2025 13:19:40 GMT</pubDate><guid isPermaLink="true">http://ieeexplore.ieee.org/document/11059861</guid></item><item><title>[TBME] Prompt Learning With Bounding Box Constraints for Medical Image Segmentation</title><link>http://ieeexplore.ieee.org/document/11049003</link><description>Pixel-wise annotations are notoriously labourious and costly to obtain in the medical domain. To mitigate this burden, weakly supervised approaches based on bounding box annotationsmuch easier to acquireoffer a practical alternative. Vision foundation models have recently shown noteworthy segmentation performance when provided with prompts such as points or bounding boxes. Prompt learning exploits these models by adapting them to downstream tasks and automating segmentation, thereby reducing user intervention. However, existing prompt learning approaches depend on fully annotated segmentation masks. This paper proposes a novel framework that combines the representational power of foundation models with the annotation efficiency of weakly supervised segmentation. More specifically, our approach automates prompt generation for foundation models using only bounding box annotations. Our proposed optimization scheme integrates multiple constraints derived from box annotations with pseudo-labels generated by the prompted foundation model. Extensive experiments across multi-modal datasets reveal that, using the Segment Anything Model (SAM) as backbone, our weakly supervised method achieves an average Dice score of 84.90% in a limited data setting, outperforming existing fully-supervised and weakly-supervised approaches.</description><author>IEEE Transactions on Biomedical Engineering - new TOC</author><pubDate>Tue, 24 Jun 2025 13:17:26 GMT</pubDate><guid isPermaLink="true">http://ieeexplore.ieee.org/document/11049003</guid></item></channel></rss>