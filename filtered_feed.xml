<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>My Customized Papers</title><link>https://github.com/your_username/your_repo</link><description>Aggregated research papers</description><language>en-US</language><lastBuildDate>Sun, 18 Jan 2026 12:49:58 GMT</lastBuildDate><generator>rfeed v1.1.1</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>[EAAI] Artificial Intelligence-based back-calculation model for scrap compiling optimization</title><link>https://www.sciencedirect.com/science/article/pii/S0952197626000904?dgcid=rss_sd_all</link><description>&lt;p&gt;Publication date: 1 March 2026&lt;/p&gt;&lt;p&gt;&lt;b&gt;Source:&lt;/b&gt; Engineering Applications of Artificial Intelligence, Volume 167, Part 2&lt;/p&gt;&lt;p&gt;Author(s): Michael Schäfer, Ulrike Faltings, Björn Glaser&lt;/p&gt;</description><author>ScienceDirect Publication: Engineering Applications of Artificial Intelligence</author><pubDate>Fri, 16 Jan 2026 18:39:53 GMT</pubDate><guid isPermaLink="true">https://www.sciencedirect.com/science/article/pii/S0952197626000904</guid></item><item><title>[AIM] Rethinking U-Net architecture in medical imaging: Advancing the efficient and interpretable UKAN-CBAM framework for colorectal polyp segmentation</title><link>https://www.sciencedirect.com/science/article/pii/S0933365726000047?dgcid=rss_sd_all</link><description>&lt;p&gt;Publication date: Available online 15 January 2026&lt;/p&gt;&lt;p&gt;&lt;b&gt;Source:&lt;/b&gt; Artificial Intelligence in Medicine&lt;/p&gt;&lt;p&gt;Author(s): Md. Faysal Ahamed, Fariya Bintay Shafi, Md. Rabiul Islam, Md. Fahmidun Nabi, Julfikar Haider&lt;/p&gt;</description><author>ScienceDirect Publication: Artificial Intelligence in Medicine</author><pubDate>Fri, 16 Jan 2026 18:39:53 GMT</pubDate><guid isPermaLink="true">https://www.sciencedirect.com/science/article/pii/S0933365726000047</guid></item><item><title>[ESWA] AdLER: Adversarial Training with Label Error Rectification for One-Shot Medical Image Segmentation</title><link>https://www.sciencedirect.com/science/article/pii/S0957417426001387?dgcid=rss_sd_all</link><description>&lt;p&gt;Publication date: Available online 15 January 2026&lt;/p&gt;&lt;p&gt;&lt;b&gt;Source:&lt;/b&gt; Expert Systems with Applications&lt;/p&gt;&lt;p&gt;Author(s): Xiangyu Zhao, Sheng Wang, Zhiyun Song, Zhenrong Shen, Linlin Yao, Haolei Yuan, Qian Wang, Lichi Zhang&lt;/p&gt;</description><author>ScienceDirect Publication: Expert Systems with Applications</author><pubDate>Fri, 16 Jan 2026 18:39:51 GMT</pubDate><guid isPermaLink="true">https://www.sciencedirect.com/science/article/pii/S0957417426001387</guid></item><item><title>[ESWA] Reusability and Benchmarking Potential of Architectural Cultural Heritage Datasets for Generative AI: An Analytical Study</title><link>https://www.sciencedirect.com/science/article/pii/S0957417425045312?dgcid=rss_sd_all</link><description>&lt;p&gt;Publication date: Available online 16 January 2026&lt;/p&gt;&lt;p&gt;&lt;b&gt;Source:&lt;/b&gt; Expert Systems with Applications&lt;/p&gt;&lt;p&gt;Author(s): Manar Abu Talib, Iman Ibrahim, Manar Anwer Abusirdaneh&lt;/p&gt;</description><author>ScienceDirect Publication: Expert Systems with Applications</author><pubDate>Fri, 16 Jan 2026 18:39:51 GMT</pubDate><guid isPermaLink="true">https://www.sciencedirect.com/science/article/pii/S0957417425045312</guid></item><item><title>[arXiv-CV] VibrantSR: Sub-Meter Canopy Height Models from Sentinel-2 Using Generative Flow Matching</title><link>https://arxiv.org/abs/2601.09866</link><description>arXiv:2601.09866v1 Announce Type: new 
Abstract: We present VibrantSR (Vibrant Super-Resolution), a generative super-resolution framework for estimating 0.5 meter canopy height models (CHMs) from 10 meter Sentinel-2 imagery. Unlike approaches based on aerial imagery that are constrained by infrequent and irregular acquisition schedules, VibrantSR leverages globally available Sentinel-2 seasonal composites, enabling consistent monitoring at a seasonal-to-annual cadence. Evaluated across 22 EPA Level 3 eco-regions in the western United States using spatially disjoint validation splits, VibrantSR achieves a Mean Absolute Error of 4.39 meters for canopy heights &gt;= 2 m, outperforming Meta (4.83 m), LANDFIRE (5.96 m), and ETH (7.05 m) satellite-based benchmarks. While aerial-based VibrantVS (2.71 m MAE) retains an accuracy advantage, VibrantSR enables operational forest monitoring and carbon accounting at continental scales without reliance on costly and temporally infrequent aerial acquisitions.</description><author>cs.CV updates on arXiv.org</author><pubDate>Fri, 16 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2601.09866v1</guid></item><item><title>[arXiv-CV] MedVL-SAM2: A unified 3D medical vision-language model for multimodal reasoning and prompt-driven segmentation</title><link>https://arxiv.org/abs/2601.09879</link><description>arXiv:2601.09879v1 Announce Type: new 
Abstract: Recent progress in medical vision-language models (VLMs) has achieved strong performance on image-level text-centric tasks such as report generation and visual question answering (VQA). However, achieving fine-grained visual grounding and volumetric spatial reasoning in 3D medical VLMs remains challenging, particularly when aiming to unify these capabilities within a single, generalizable framework. To address this challenge, we proposed MedVL-SAM2, a unified 3D medical multimodal model that concurrently supports report generation, VQA, and multi-paradigm segmentation, including semantic, referring, and interactive segmentation. MedVL-SAM2 integrates image-level reasoning and pixel-level perception through a cohesive architecture tailored for 3D medical imaging, and incorporates a SAM2-based volumetric segmentation module to enable precise multi-granular spatial reasoning. The model is trained in a multi-stage pipeline: it is first pre-trained on a large-scale corpus of 3D CT image-text pairs to align volumetric visual features with radiology-language embeddings. It is then jointly optimized with both language-understanding and segmentation objectives using a comprehensive 3D CT segmentation dataset. This joint training enables flexible interaction via language, point, or box prompts, thereby unifying high-level visual reasoning with spatially precise localization. Our unified architecture delivers state-of-the-art performance across report generation, VQA, and multiple 3D segmentation tasks. Extensive analyses further show that the model provides reliable 3D visual grounding, controllable interactive segmentation, and robust cross-modal reasoning, demonstrating that high-level semantic reasoning and precise 3D localization can be jointly achieved within a unified 3D medical VLM.</description><author>cs.CV updates on arXiv.org</author><pubDate>Fri, 16 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2601.09879v1</guid></item><item><title>[arXiv-CV] VQ-Seg: Vector-Quantized Token Perturbation for Semi-Supervised Medical Image Segmentation</title><link>https://arxiv.org/abs/2601.10124</link><description>arXiv:2601.10124v1 Announce Type: new 
Abstract: Consistency learning with feature perturbation is a widely used strategy in semi-supervised medical image segmentation. However, many existing perturbation methods rely on dropout, and thus require a careful manual tuning of the dropout rate, which is a sensitive hyperparameter and often difficult to optimize and may lead to suboptimal regularization. To overcome this limitation, we propose VQ-Seg, the first approach to employ vector quantization (VQ) to discretize the feature space and introduce a novel and controllable Quantized Perturbation Module (QPM) that replaces dropout. Our QPM perturbs discrete representations by shuffling the spatial locations of codebook indices, enabling effective and controllable regularization. To mitigate potential information loss caused by quantization, we design a dual-branch architecture where the post-quantization feature space is shared by both image reconstruction and segmentation tasks. Moreover, we introduce a Post-VQ Feature Adapter (PFA) to incorporate guidance from a foundation model (FM), supplementing the high-level semantic information lost during quantization. Furthermore, we collect a large-scale Lung Cancer (LC) dataset comprising 828 CT scans annotated for central-type lung carcinoma. Extensive experiments on the LC dataset and other public benchmarks demonstrate the effectiveness of our method, which outperforms state-of-the-art approaches. Code available at: https://github.com/script-Yang/VQ-Seg.</description><author>cs.CV updates on arXiv.org</author><pubDate>Fri, 16 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2601.10124v1</guid></item><item><title>[arXiv-CV] BikeActions: An Open Platform and Benchmark for Cyclist-Centric VRU Action Recognition</title><link>https://arxiv.org/abs/2601.10521</link><description>arXiv:2601.10521v1 Announce Type: new 
Abstract: Anticipating the intentions of Vulnerable Road Users (VRUs) is a critical challenge for safe autonomous driving (AD) and mobile robotics. While current research predominantly focuses on pedestrian crossing behaviors from a vehicle's perspective, interactions within dense shared spaces remain underexplored. To bridge this gap, we introduce FUSE-Bike, the first fully open perception platform of its kind. Equipped with two LiDARs, a camera, and GNSS, it facilitates high-fidelity, close-range data capture directly from a cyclist's viewpoint. Leveraging this platform, we present BikeActions, a novel multi-modal dataset comprising 852 annotated samples across 5 distinct action classes, specifically tailored to improve VRU behavior modeling. We establish a rigorous benchmark by evaluating state-of-the-art graph convolution and transformer-based models on our publicly released data splits, establishing the first performance baselines for this challenging task. We release the full dataset together with data curation tools, the open hardware design, and the benchmark code to foster future research in VRU action understanding under https://iv.ee.hm.edu/bikeactions/.</description><author>cs.CV updates on arXiv.org</author><pubDate>Fri, 16 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2601.10521v1</guid></item><item><title>[arXiv-CV] Jordan-Segmentable Masks: A Topology-Aware definition for characterizing Binary Image Segmentation</title><link>https://arxiv.org/abs/2601.10577</link><description>arXiv:2601.10577v1 Announce Type: new 
Abstract: Image segmentation plays a central role in computer vision. However, widely used evaluation metrics, whether pixel-wise, region-based, or boundary-focused, often struggle to capture the structural and topological coherence of a segmentation. In many practical scenarios, such as medical imaging or object delineation, small inaccuracies in boundary, holes, or fragmented predictions can result in high metric scores, despite the fact that the resulting masks fail to preserve the object global shape or connectivity. This highlights a limitation of conventional metrics: they are unable to assess whether a predicted segmentation partitions the image into meaningful interior and exterior regions.
  In this work, we introduce a topology-aware notion of segmentation based on the Jordan Curve Theorem, and adapted for use in digital planes. We define the concept of a \emph{Jordan-segmentatable mask}, which is a binary segmentation whose structure ensures a topological separation of the image domain into two connected components. We analyze segmentation masks through the lens of digital topology and homology theory, extracting a $4$-curve candidate from the mask, verifying its topological validity using Betti numbers. A mask is considered Jordan-segmentatable when this candidate forms a digital 4-curve with $\beta_0 = \beta_1 = 1$, or equivalently when its complement splits into exactly two $8$-connected components.
  This framework provides a mathematically rigorous, unsupervised criterion with which to assess the structural coherence of segmentation masks. By combining digital Jordan theory and homological invariants, our approach provides a valuable alternative to standard evaluation metrics, especially in applications where topological correctness must be preserved.</description><author>cs.CV updates on arXiv.org</author><pubDate>Fri, 16 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2601.10577v1</guid></item><item><title>[arXiv-CV] MHub.ai: A Simple, Standardized, and Reproducible Platform for AI Models in Medical Imaging</title><link>https://arxiv.org/abs/2601.10154</link><description>arXiv:2601.10154v1 Announce Type: cross 
Abstract: Artificial intelligence (AI) has the potential to transform medical imaging by automating image analysis and accelerating clinical research. However, research and clinical use are limited by the wide variety of AI implementations and architectures, inconsistent documentation, and reproducibility issues. Here, we introduce MHub.ai, an open-source, container-based platform that standardizes access to AI models with minimal configuration, promoting accessibility and reproducibility in medical imaging. MHub.ai packages models from peer-reviewed publications into standardized containers that support direct processing of DICOM and other formats, provide a unified application interface, and embed structured metadata. Each model is accompanied by publicly available reference data that can be used to confirm model operation. MHub.ai includes an initial set of state-of-the-art segmentation, prediction, and feature extraction models for different modalities. The modular framework enables adaptation of any model and supports community contributions. We demonstrate the utility of the platform in a clinical use case through comparative evaluation of lung segmentation models. To further strengthen transparency and reproducibility, we publicly release the generated segmentations and evaluation metrics and provide interactive dashboards that allow readers to inspect individual cases and reproduce or extend our analysis. By simplifying model use, MHub.ai enables side-by-side benchmarking with identical execution commands and standardized outputs, and lowers the barrier to clinical translation.</description><author>cs.CV updates on arXiv.org</author><pubDate>Fri, 16 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2601.10154v1</guid></item><item><title>[arXiv-CV] Graph Algorithm Unrolling with Douglas-Rachford Iterations for Image Interpolation with Guaranteed Initialization</title><link>https://arxiv.org/abs/2509.11926</link><description>arXiv:2509.11926v3 Announce Type: replace 
Abstract: Conventional deep neural nets (DNNs) initialize network parameters at random and then optimize each one via stochastic gradient descent (SGD), resulting in substantial risk of poor-performing local minima.Focusing on the image interpolation problem and leveraging a recent theorem that maps a (pseudo-)linear interpolator {\Theta} to a directed graph filter that is a solution to a MAP problem regularized with a graph shift variation (GSV) prior, we first initialize a directed graph adjacency matrix A based on a known interpolator {\Theta}, establishing a baseline performance.Then, towards further gain, we learn perturbation matrices P and P(2) from data to augment A, whose restoration effects are implemented via Douglas-Rachford (DR) iterations, which we unroll into a lightweight interpretable neural net.Experimental results demonstrate state-of-the-art image interpolation results, while drastically reducing network parameters.</description><author>cs.CV updates on arXiv.org</author><pubDate>Fri, 16 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2509.11926v3</guid></item><item><title>[arXiv-CV] Mamba Goes HoME: Hierarchical Soft Mixture-of-Experts for 3D Medical Image Segmentation</title><link>https://arxiv.org/abs/2507.06363</link><description>arXiv:2507.06363v3 Announce Type: replace-cross 
Abstract: In recent years, artificial intelligence has significantly advanced medical image segmentation. Nonetheless, challenges remain, including efficient 3D medical image processing across diverse modalities and handling data variability. In this work, we introduce Hierarchical Soft Mixture-of-Experts (HoME), a two-level token-routing layer for efficient long-context modeling, specifically designed for 3D medical image segmentation. Built on the Mamba Selective State Space Model (SSM) backbone, HoME enhances sequential modeling through adaptive expert routing. In the first level, a Soft Mixture-of-Experts (SMoE) layer partitions input sequences into local groups, routing tokens to specialized per-group experts for localized feature extraction. The second level aggregates these outputs through a global SMoE layer, enabling cross-group information fusion and global context refinement. This hierarchical design, combining local expert routing with global expert refinement, enhances generalizability and segmentation performance, surpassing state-of-the-art results across datasets from the three most widely used 3D medical imaging modalities and varying data qualities. The code is publicly available at https://github.com/gmum/MambaHoME.</description><author>cs.CV updates on arXiv.org</author><pubDate>Fri, 16 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2507.06363v3</guid></item><item><title>[arXiv-ML] Time Aggregation Features for XGBoost Models</title><link>https://arxiv.org/abs/2601.10019</link><description>arXiv:2601.10019v1 Announce Type: new 
Abstract: This paper studies time aggregation features for XGBoost models in click-through rate prediction. The setting is the Avazu click-through rate prediction dataset with strict out-of-time splits and a no-lookahead feature constraint. Features for hour H use only impressions from hours strictly before H. This paper compares a strong time-aware target encoding baseline to models augmented with entity history time aggregation under several window designs. Across two rolling-tail folds on a deterministic ten percent sample, a trailing window specification improves ROC AUC by about 0.0066 to 0.0082 and PR AUC by about 0.0084 to 0.0094 relative to target encoding alone. Within the time aggregation design grid, event count windows provide the only consistent improvement over trailing windows, and the gain is small. Gap windows and bucketized windows underperform simple trailing windows in this dataset and protocol. These results support a practical default of trailing windows, with an optional event count window when marginal ROC AUC gains matter.</description><author>cs.LG updates on arXiv.org</author><pubDate>Fri, 16 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2601.10019v1</guid></item><item><title>[arXiv-ML] CC-OR-Net: A Unified Framework for LTV Prediction through Structural Decoupling</title><link>https://arxiv.org/abs/2601.10176</link><description>arXiv:2601.10176v1 Announce Type: new 
Abstract: Customer Lifetime Value (LTV) prediction, a central problem in modern marketing, is characterized by a unique zero-inflated and long-tail data distribution. This distribution presents two fundamental challenges: (1) the vast majority of low-to-medium value users numerically overwhelm the small but critically important segment of high-value "whale" users, and (2) significant value heterogeneity exists even within the low-to-medium value user base. Common approaches either rely on rigid statistical assumptions or attempt to decouple ranking and regression using ordered buckets; however, they often enforce ordinality through loss-based constraints rather than inherent architectural design, failing to balance global accuracy with high-value precision. To address this gap, we propose \textbf{C}onditional \textbf{C}ascaded \textbf{O}rdinal-\textbf{R}esidual Networks \textbf{(CC-OR-Net)}, a novel unified framework that achieves a more robust decoupling through \textbf{structural decomposition}, where ranking is architecturally guaranteed. CC-OR-Net integrates three specialized components: a \textit{structural ordinal decomposition module} for robust ranking, an \textit{intra-bucket residual module} for fine-grained regression, and a \textit{targeted high-value augmentation module} for precision on top-tier users. Evaluated on real-world datasets with over 300M users, CC-OR-Net achieves a superior trade-off across all key business metrics, outperforming state-of-the-art methods in creating a holistic and commercially valuable LTV prediction solution.</description><author>cs.LG updates on arXiv.org</author><pubDate>Fri, 16 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2601.10176v1</guid></item><item><title>[arXiv-ML] LangLasso: Interactive Cluster Descriptions through LLM Explanation</title><link>https://arxiv.org/abs/2601.10458</link><description>arXiv:2601.10458v1 Announce Type: cross 
Abstract: Dimensionality reduction is a powerful technique for revealing structure and potential clusters in data. However, as the axes are complex, non-linear combinations of features, they often lack semantic interpretability. Existing visual analytics (VA) methods support cluster interpretation through feature comparison and interactive exploration, but they require technical expertise and intense human effort. We present \textit{LangLasso}, a novel method that complements VA approaches through interactive, natural language descriptions of clusters using large language models (LLMs). It produces human-readable descriptions that make cluster interpretation accessible to non-experts and allow integration of external contextual knowledge beyond the dataset. We systematically evaluate the reliability of these explanations and demonstrate that \langlasso provides an effective first step for engaging broader audiences in cluster interpretation. The tool is available at https://langlasso.vercel.app</description><author>cs.LG updates on arXiv.org</author><pubDate>Fri, 16 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2601.10458v1</guid></item><item><title>[arXiv-ML] Autoencoding Random Forests</title><link>https://arxiv.org/abs/2505.21441</link><description>arXiv:2505.21441v4 Announce Type: replace-cross 
Abstract: We propose a principled method for autoencoding with random forests. Our strategy builds on foundational results from nonparametric statistics and spectral graph theory to learn a low-dimensional embedding of the model that optimally represents relationships in the data. We provide exact and approximate solutions to the decoding problem via constrained optimization, split relabeling, and nearest neighbors regression. These methods effectively invert the compression pipeline, establishing a map from the embedding space back to the input space using splits learned by the ensemble's constituent trees. The resulting decoders are universally consistent under common regularity assumptions. The procedure works with supervised or unsupervised models, providing a window into conditional or joint distributions. We demonstrate various applications of this autoencoder, including powerful new tools for visualization, compression, clustering, and denoising. Experiments illustrate the ease and utility of our method in a wide range of settings, including tabular, image, and genomic data.</description><author>cs.LG updates on arXiv.org</author><pubDate>Fri, 16 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2505.21441v4</guid></item><item><title>[NN] ReTri: Progressive domain bridging via representation disentanglement and triple-level consistency-driven feature alignment for unsupervised domain adaptive medical image segmentation</title><link>https://www.sciencedirect.com/science/article/pii/S0893608026000274?dgcid=rss_sd_all</link><description>&lt;p&gt;Publication date: June 2026&lt;/p&gt;&lt;p&gt;&lt;b&gt;Source:&lt;/b&gt; Neural Networks, Volume 198&lt;/p&gt;&lt;p&gt;Author(s): Xiaoru Gao, Guoyan Zheng&lt;/p&gt;</description><author>ScienceDirect Publication: Neural Networks</author><pubDate>Thu, 15 Jan 2026 18:43:14 GMT</pubDate><guid isPermaLink="true">https://www.sciencedirect.com/science/article/pii/S0893608026000274</guid></item><item><title>[BSPC] MFBRU-Net: Multi-scale feature fusion and boundary refining U-Net for medical image segmentation</title><link>https://www.sciencedirect.com/science/article/pii/S1746809426001473?dgcid=rss_sd_all</link><description>&lt;p&gt;Publication date: 15 May 2026&lt;/p&gt;&lt;p&gt;&lt;b&gt;Source:&lt;/b&gt; Biomedical Signal Processing and Control, Volume 117&lt;/p&gt;&lt;p&gt;Author(s): Han Gao, Xianran Zhang, Zhengpeng Li, Ziteng Wang, Ping Sun, Jiansheng Wu&lt;/p&gt;</description><author>ScienceDirect Publication: Biomedical Signal Processing and Control</author><pubDate>Thu, 15 Jan 2026 12:56:08 GMT</pubDate><guid isPermaLink="true">https://www.sciencedirect.com/science/article/pii/S1746809426001473</guid></item><item><title>[BSPC] Deep feature-based approaches for brain tumor classification and segmentation in medical imaging</title><link>https://www.sciencedirect.com/science/article/pii/S1746809426001576?dgcid=rss_sd_all</link><description>&lt;p&gt;Publication date: 15 May 2026&lt;/p&gt;&lt;p&gt;&lt;b&gt;Source:&lt;/b&gt; Biomedical Signal Processing and Control, Volume 117&lt;/p&gt;&lt;p&gt;Author(s): Agnesh Chandra Yadav, Maheshkumar H. Kolekar&lt;/p&gt;</description><author>ScienceDirect Publication: Biomedical Signal Processing and Control</author><pubDate>Thu, 15 Jan 2026 12:56:08 GMT</pubDate><guid isPermaLink="true">https://www.sciencedirect.com/science/article/pii/S1746809426001576</guid></item><item><title>[PR] Enhancing semi-supervised medical image segmentation via semantic transfer</title><link>https://www.sciencedirect.com/science/article/pii/S0031320326000026?dgcid=rss_sd_all</link><description>&lt;p&gt;Publication date: July 2026&lt;/p&gt;&lt;p&gt;&lt;b&gt;Source:&lt;/b&gt; Pattern Recognition, Volume 175&lt;/p&gt;&lt;p&gt;Author(s): Shiyuan Huang, Shudong Wang, Kuijie Zhang, Wenhao Wu, Yingye Liu, Tiyao Liu, Shanchen Pang&lt;/p&gt;</description><author>ScienceDirect Publication: Pattern Recognition</author><pubDate>Thu, 15 Jan 2026 12:56:06 GMT</pubDate><guid isPermaLink="true">https://www.sciencedirect.com/science/article/pii/S0031320326000026</guid></item><item><title>[arXiv-CV] Variance-Penalized MC-Dropout as a Learned Smoothing Prior for Brain Tumour Segmentation</title><link>https://arxiv.org/abs/2601.08956</link><description>arXiv:2601.08956v1 Announce Type: new 
Abstract: Brain tumor segmentation is essential for diagnosis and treatment planning, yet many CNN and U-Net based approaches produce noisy boundaries in regions of tumor infiltration. We introduce UAMSA-UNet, an Uncertainty-Aware Multi-Scale Attention-based Bayesian U-Net that in- stead leverages Monte Carlo Dropout to learn a data-driven smoothing prior over its predictions, while fusing multi-scale features and attention maps to capture both fine details and global context. Our smoothing-regularized loss augments binary cross-entropy with a variance penalty across stochas- tic forward passes, discouraging spurious fluctuations and yielding spatially coherent masks. On BraTS2023, UAMSA- UNet improves Dice Similarity Coefficient by up to 3.3% and mean IoU by up to 2.7% over U-Net; on BraTS2024, it delivers up to 4.5% Dice and 4.0% IoU gains over the best baseline. Remarkably, it also reduces FLOPs by 42.5% rel- ative to U-Net++ while maintaining higher accuracy. These results demonstrate that, by combining multi-scale attention with a learned smoothing prior, UAMSA-UNet achieves both better segmentation quality and computational efficiency, and provides a flexible foundation for future integration with transformer-based modules for further enhanced segmenta- tion results.</description><author>cs.CV updates on arXiv.org</author><pubDate>Thu, 15 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2601.08956v1</guid></item><item><title>[arXiv-CV] From Performance to Practice: Knowledge-Distilled Segmentator for On-Premises Clinical Workflows</title><link>https://arxiv.org/abs/2601.09191</link><description>arXiv:2601.09191v1 Announce Type: new 
Abstract: Deploying medical image segmentation models in routine clinical workflows is often constrained by on-premises infrastructure, where computational resources are fixed and cloud-based inference may be restricted by governance and security policies. While high-capacity models achieve strong segmentation accuracy, their computational demands hinder practical deployment and long-term maintainability in hospital environments. We present a deployment-oriented framework that leverages knowledge distillation to translate a high-performing segmentation model into a scalable family of compact student models, without modifying the inference pipeline. The proposed approach preserves architectural compatibility with existing clinical systems while enabling systematic capacity reduction. The framework is evaluated on a multi-site brain MRI dataset comprising 1,104 3D volumes, with independent testing on 101 curated cases, and is further examined on abdominal CT to assess cross-modality generalizability. Under aggressive parameter reduction (94%), the distilled student model preserves nearly all of the teacher's segmentation accuracy (98.7%), while achieving substantial efficiency gains, including up to a 67% reduction in CPU inference latency without additional deployment overhead. These results demonstrate that knowledge distillation provides a practical and reliable pathway for converting research-grade segmentation models into maintainable, deployment-ready components for on-premises clinical workflows in real-world health systems.</description><author>cs.CV updates on arXiv.org</author><pubDate>Thu, 15 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2601.09191v1</guid></item><item><title>[arXiv-CV] POWDR: Pathology-preserving Outpainting with Wavelet Diffusion for 3D MRI</title><link>https://arxiv.org/abs/2601.09044</link><description>arXiv:2601.09044v1 Announce Type: cross 
Abstract: Medical imaging datasets often suffer from class imbalance and limited availability of pathology-rich cases, which constrains the performance of machine learning models for segmentation, classification, and vision-language tasks. To address this challenge, we propose POWDR, a pathology-preserving outpainting framework for 3D MRI based on a conditioned wavelet diffusion model. Unlike conventional augmentation or unconditional synthesis, POWDR retains real pathological regions while generating anatomically plausible surrounding tissue, enabling diversity without fabricating lesions.
  Our approach leverages wavelet-domain conditioning to enhance high-frequency detail and mitigate blurring common in latent diffusion models. We introduce a random connected mask training strategy to overcome conditioning-induced collapse and improve diversity outside the lesion. POWDR is evaluated on brain MRI using BraTS datasets and extended to knee MRI to demonstrate tissue-agnostic applicability. Quantitative metrics (FID, SSIM, LPIPS) confirm image realism, while diversity analysis shows significant improvement with random-mask training (cosine similarity reduced from 0.9947 to 0.9580; KL divergence increased from 0.00026 to 0.01494). Clinically relevant assessments reveal gains in tumor segmentation performance using nnU-Net, with Dice scores improving from 0.6992 to 0.7137 when adding 50 synthetic cases. Tissue volume analysis indicates no significant differences for CSF and GM compared to real images.
  These findings highlight POWDR as a practical solution for addressing data scarcity and class imbalance in medical imaging. The method is extensible to multiple anatomies and offers a controllable framework for generating diverse, pathology-preserving synthetic data to support robust model development.</description><author>cs.CV updates on arXiv.org</author><pubDate>Thu, 15 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2601.09044v1</guid></item><item><title>[arXiv-CV] Comprehensive language-image pre-training for 3D medical image understanding</title><link>https://arxiv.org/abs/2510.15042</link><description>arXiv:2510.15042v2 Announce Type: replace 
Abstract: Vision-language pre-training, i.e., aligning images with paired text, is a powerful paradigm to create encoders that can be directly used for tasks such as classification, retrieval, and segmentation. In the 3D medical image domain, these capabilities allow vision-language encoders (VLEs) to support radiologists by retrieving patients with similar abnormalities, predicting likelihoods of abnormality, or, with downstream adaptation, generating radiological reports. While the methodology holds promise, data availability and domain-specific hurdles limit the capabilities of current 3D VLEs.
  In this paper, we overcome these challenges by injecting additional supervision via a report generation objective and combining vision-language with vision-only pre-training. This allows us to leverage both image-only and paired image-text 3D datasets, increasing the total amount of data to which our model is exposed. Through these additional objectives, paired with best practices of the 3D medical imaging domain, we develop the Comprehensive Language-Image Pre-training (COLIPRI) encoder family. Our COLIPRI encoders achieve state-of-the-art performance in report generation, semantic segmentation, classification probing, and zero-shot classification. The model is available at https://huggingface.co/microsoft/colipri.</description><author>cs.CV updates on arXiv.org</author><pubDate>Thu, 15 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2510.15042v2</guid></item><item><title>[arXiv-CV] BARL: Bilateral Alignment in Representation and Label Spaces for Semi-Supervised Volumetric Medical Image Segmentation</title><link>https://arxiv.org/abs/2510.16863</link><description>arXiv:2510.16863v2 Announce Type: replace 
Abstract: Semi-supervised medical image segmentation (SSMIS) seeks to match fully supervised performance while sharply reducing annotation cost. Mainstream SSMIS methods rely on \emph{label-space consistency}, yet they overlook the equally critical \emph{representation-space alignment}. Without harmonizing latent features, models struggle to learn representations that are both discriminative and spatially coherent. To this end, we introduce \textbf{Bilateral Alignment in Representation and Label spaces (BARL)}, a unified framework that couples two collaborative branches and enforces alignment in both spaces. For label-space alignment, inspired by co-training and multi-scale decoding, we devise \textbf{Dual-Path Regularization (DPR)} and \textbf{Progressively Cognitive Bias Correction (PCBC)} to impose fine-grained cross-branch consistency while mitigating error accumulation from coarse to fine scales. For representation-space alignment, we conduct region-level and lesion-instance matching between branches, explicitly capturing the fragmented, complex pathological patterns common in medical imagery. Extensive experiments on four public benchmarks and a proprietary CBCT dataset demonstrate that BARL consistently surpasses state-of-the-art SSMIS methods. Ablative studies further validate the contribution of each component. Code will be released soon.</description><author>cs.CV updates on arXiv.org</author><pubDate>Thu, 15 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2510.16863v2</guid></item><item><title>[arXiv-CV] egoEMOTION: Egocentric Vision and Physiological Signals for Emotion and Personality Recognition in Real-World Tasks</title><link>https://arxiv.org/abs/2510.22129</link><description>arXiv:2510.22129v2 Announce Type: replace 
Abstract: Understanding affect is central to anticipating human behavior, yet current egocentric vision benchmarks largely ignore the person's emotional states that shape their decisions and actions. Existing tasks in egocentric perception focus on physical activities, hand-object interactions, and attention modeling - assuming neutral affect and uniform personality. This limits the ability of vision systems to capture key internal drivers of behavior. In this paper, we present egoEMOTION, the first dataset that couples egocentric visual and physiological signals with dense self-reports of emotion and personality across controlled and real-world scenarios. Our dataset includes over 50 hours of recordings from 43 participants, captured using Meta's Project Aria glasses. Each session provides synchronized eye-tracking video, headmounted photoplethysmography, inertial motion data, and physiological baselines for reference. Participants completed emotion-elicitation tasks and naturalistic activities while self-reporting their affective state using the Circumplex Model and Mikels' Wheel as well as their personality via the Big Five model. We define three benchmark tasks: (1) continuous affect classification (valence, arousal, dominance); (2) discrete emotion classification; and (3) trait-level personality inference. We show that a classical learning-based method, as a simple baseline in real-world affect prediction, produces better estimates from signals captured on egocentric vision systems than processing physiological signals. Our dataset establishes emotion and personality as core dimensions in egocentric perception and opens new directions in affect-driven modeling of behavior, intent, and interaction.</description><author>cs.CV updates on arXiv.org</author><pubDate>Thu, 15 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2510.22129v2</guid></item><item><title>[arXiv-ML] LLM for Large-Scale Optimization Model Auto-Formulation: A Lightweight Few-Shot Learning Approach</title><link>https://arxiv.org/abs/2601.09635</link><description>arXiv:2601.09635v1 Announce Type: cross 
Abstract: Large-scale optimization is a key backbone of modern business decision-making. However, building these models is often labor-intensive and time-consuming. We address this by proposing LEAN-LLM-OPT, a LightwEight AgeNtic workflow construction framework for LLM-assisted large-scale OPTimization auto-formulation. LEAN-LLM-OPT takes as input a problem description together with associated datasets and orchestrates a team of LLM agents to produce an optimization formulation. Specifically, upon receiving a query, two upstream LLM agents dynamically construct a workflow that specifies, step-by-step, how optimization models for similar problems can be formulated. A downstream LLM agent then follows this workflow to generate the final output. Leveraging LLMs' text-processing capabilities and common modeling practices, the workflow decomposes the modeling task into a sequence of structured sub-tasks and offloads mechanical data-handling operations to auxiliary tools. This design alleviates the downstream agent's burden related to planning and data handling, allowing it to focus on the most challenging components that cannot be readily standardized. Extensive simulations show that LEAN-LLM-OPT, instantiated with GPT-4.1 and the open source gpt-oss-20B, achieves strong performance on large-scale optimization modeling tasks and is competitive with state-of-the-art approaches. In addition, in a Singapore Airlines choice-based revenue management use case, LEAN-LLM-OPT demonstrates practical value by achieving leading performance across a range of scenarios. Along the way, we introduce Large-Scale-OR and Air-NRM, the first comprehensive benchmarks for large-scale optimization auto-formulation. The code and data of this work is available at https://github.com/CoraLiang01/lean-llm-opt.</description><author>cs.LG updates on arXiv.org</author><pubDate>Thu, 15 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2601.09635v1</guid></item><item><title>[arXiv-ML] e-Profits: A Business-Aligned Evaluation Metric for Profit-Sensitive Customer Churn Prediction</title><link>https://arxiv.org/abs/2507.08860</link><description>arXiv:2507.08860v2 Announce Type: replace 
Abstract: Retention campaigns in customer relationship management often rely on churn prediction models evaluated using traditional metrics such as AUC and F1-score. However, these metrics fail to reflect financial outcomes and may mislead strategic decisions. We introduce e-Profits, a novel business-aligned evaluation metric that quantifies model performance based on customer lifetime value, retention probability, and intervention costs. Unlike existing profit-based metrics such as Expected Maximum Profit, which assume fixed population-level parameters, e-Profits uses Kaplan-Meier survival analysis to estimate tenure-conditioned (customer-level) one-period retention probabilities and supports granular, per-customer profit evaluation. We benchmark six classifiers across two telecom datasets (IBM Telco and Maven Telecom) and demonstrate that e-Profits reshapes model rankings compared to traditional metrics, revealing financial advantages in models previously overlooked by AUC or F1-score. The metric also enables segment-level insight into which models maximise return on investment for high-value customers. e-Profits provides a transparent, customer-level evaluation framework that bridges predictive modelling and profit-driven decision-making in operational churn management. All source code is available at: https://github.com/Awaismanzoor/eprofits.</description><author>cs.LG updates on arXiv.org</author><pubDate>Thu, 15 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2507.08860v2</guid></item><item><title>[arXiv-ML] Horizon Activation Mapping for Neural Networks in Time Series Forecasting</title><link>https://arxiv.org/abs/2601.02094</link><description>arXiv:2601.02094v2 Announce Type: replace 
Abstract: Neural networks for time series forecasting have relied on error metrics and architecture-specific interpretability approaches for model selection that don't apply across models of different families. To interpret forecasting models agnostic to the types of layers across state-of-the-art model families, we introduce Horizon Activation Mapping (HAM), a visual interpretability technique inspired by grad-CAM that uses gradient norm averages to study the horizon's subseries where grad-CAM studies attention maps over image data. We introduce causal and anti-causal modes to calculate gradient update norm averages across subseries at every timestep and lines of proportionality signifying uniform distributions of the norm averages. Optimization landscape studies with respect to changes in batch sizes, early stopping, train-val-test splits, architectural choices, univariate forecasting and dropouts are studied with respect to performances and subseries in HAM. Interestingly, batch size based differences in activities seem to indicate potential for existence of an exponential approximation across them per epoch relative to each other. Multivariate forecasting models including MLP-based CycleNet, N-Linear, N-HITS, self attention-based FEDformer, Pyraformer, SSM-based SpaceTime and diffusion-based Multi-Resolution DDPM over different horizon sizes trained over the ETTm2 dataset are used for HAM plots in this study. NHITS' neural approximation theorem and SpaceTime's exponential autoregressive activities have been attributed to trends in HAM plots over their training, validation and test sets. In general, HAM can be used for granular model selection, validation set choices and comparisons across different neural network model families.</description><author>cs.LG updates on arXiv.org</author><pubDate>Thu, 15 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2601.02094v2</guid></item><item><title>[arXiv-ML] QuFeX: Quantum feature extraction module for hybrid quantum-classical deep neural networks</title><link>https://arxiv.org/abs/2501.13165</link><description>arXiv:2501.13165v2 Announce Type: replace-cross 
Abstract: We introduce Quantum Feature Extraction (QuFeX), a novel quantum machine learning module. The proposed module enables feature extraction in a reduced-dimensional space, significantly decreasing the number of parallel evaluations required in typical quantum convolutional neural network architectures. Its design allows seamless integration into deep classical neural networks, making it particularly suitable for hybrid quantum-classical models. As an application of QuFeX, we propose Qu-Net -- a hybrid architecture which integrates QuFeX at the bottleneck of a U-Net architecture. The latter is widely used for image segmentation tasks such as medical imaging and autonomous driving. Our numerical analysis indicates that the Qu-Net can achieve superior segmentation performance compared to a U-Net baseline. These results highlight the potential of QuFeX to enhance deep neural networks by leveraging hybrid computational paradigms, providing a path towards a robust framework for real-world applications requiring precise feature extraction.</description><author>cs.LG updates on arXiv.org</author><pubDate>Thu, 15 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2501.13165v2</guid></item><item><title>[arXiv-ML] Learning About Learning: A Physics Path from Spin Glasses to Artificial Intelligence</title><link>https://arxiv.org/abs/2601.07635</link><description>arXiv:2601.07635v2 Announce Type: replace-cross 
Abstract: The Hopfield model, originally inspired by spin-glass physics, occupies a central place at the intersection of statistical mechanics, neural networks, and modern artificial intelligence. Despite its conceptual simplicity and broad applicability -- from associative memory to near-optimal solutions of combinatorial optimization problems -- it is rarely integrated into standard undergraduate physics curricula. In this paper, we present the Hopfield model as a pedagogically rich framework that naturally unifies core topics from undergraduate statistical physics, dynamical systems, linear algebra, and computational methods. We provide a concise and illustrated theoretical introduction grounded in familiar physics concepts, analyze the model's energy function, dynamics, and pattern stability, and discuss practical aspects of simulation, including a freely available simulation code. To support instruction, we conclude with classroom-ready example problems designed to mirror research practice. By explicitly connecting fundamental physics to contemporary AI applications, this work aims to help prepare physics students to understand, apply, and critically engage with the computational tools increasingly central to research, industry, and society.</description><author>cs.LG updates on arXiv.org</author><pubDate>Thu, 15 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2601.07635v2</guid></item><item><title>[BSPC] Pixel-level polyp segmentation network based on parallel feature enhancement and attention mechanism</title><link>https://www.sciencedirect.com/science/article/pii/S1746809426001023?dgcid=rss_sd_all</link><description>&lt;p&gt;Publication date: 1 May 2026&lt;/p&gt;&lt;p&gt;&lt;b&gt;Source:&lt;/b&gt; Biomedical Signal Processing and Control, Volume 116&lt;/p&gt;&lt;p&gt;Author(s): Haiping Xu, Jie Wang, Zuoyong Li, Xuesong Cheng, Shenghua Teng&lt;/p&gt;</description><author>ScienceDirect Publication: Biomedical Signal Processing and Control</author><pubDate>Wed, 14 Jan 2026 12:57:29 GMT</pubDate><guid isPermaLink="true">https://www.sciencedirect.com/science/article/pii/S1746809426001023</guid></item><item><title>[NN] AttCo: Attention-based co-Learning fusion of deep feature representation for medical image segmentation using multimodality</title><link>https://www.sciencedirect.com/science/article/pii/S0893608026000353?dgcid=rss_sd_all</link><description>&lt;p&gt;Publication date: June 2026&lt;/p&gt;&lt;p&gt;&lt;b&gt;Source:&lt;/b&gt; Neural Networks, Volume 198&lt;/p&gt;&lt;p&gt;Author(s): Duy-Phuong Dao, Hyung-Jeong Yang, Soo-Hyung Kim, Sae-Ryung Kang&lt;/p&gt;</description><author>ScienceDirect Publication: Neural Networks</author><pubDate>Wed, 14 Jan 2026 12:57:28 GMT</pubDate><guid isPermaLink="true">https://www.sciencedirect.com/science/article/pii/S0893608026000353</guid></item><item><title>[ESWA] Transferring cross-Dimensional knowledge via proxy task for medical image segmentation</title><link>https://www.sciencedirect.com/science/article/pii/S0957417426000382?dgcid=rss_sd_all</link><description>&lt;p&gt;Publication date: 1 May 2026&lt;/p&gt;&lt;p&gt;&lt;b&gt;Source:&lt;/b&gt; Expert Systems with Applications, Volume 308&lt;/p&gt;&lt;p&gt;Author(s): Cunhan Guo, Heyan Huang, Yang-Hao Zhou, Danjie Han, Changsen Yuan&lt;/p&gt;</description><author>ScienceDirect Publication: Expert Systems with Applications</author><pubDate>Wed, 14 Jan 2026 12:57:27 GMT</pubDate><guid isPermaLink="true">https://www.sciencedirect.com/science/article/pii/S0957417426000382</guid></item><item><title>[arXiv-CV] Rescind: Countering Image Misconduct in Biomedical Publications with Vision-Language and State-Space Modeling</title><link>https://arxiv.org/abs/2601.08040</link><description>arXiv:2601.08040v1 Announce Type: new 
Abstract: Scientific image manipulation in biomedical publications poses a growing threat to research integrity and reproducibility. Unlike natural image forensics, biomedical forgery detection is uniquely challenging due to domain-specific artifacts, complex textures, and unstructured figure layouts. We present the first vision-language guided framework for both generating and detecting biomedical image forgeries. By combining diffusion-based synthesis with vision-language prompting, our method enables realistic and semantically controlled manipulations, including duplication, splicing, and region removal, across diverse biomedical modalities. We introduce Rescind, a large-scale benchmark featuring fine-grained annotations and modality-specific splits, and propose Integscan, a structured state space modeling framework that integrates attention-enhanced visual encoding with prompt-conditioned semantic alignment for precise forgery localization. To ensure semantic fidelity, we incorporate a vision-language model based verification loop that filters generated forgeries based on consistency with intended prompts. Extensive experiments on Rescind and existing benchmarks demonstrate that Integscan achieves state of the art performance in both detection and localization, establishing a strong foundation for automated scientific integrity analysis.</description><author>cs.CV updates on arXiv.org</author><pubDate>Wed, 14 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2601.08040v1</guid></item><item><title>[arXiv-CV] Exploiting DINOv3-Based Self-Supervised Features for Robust Few-Shot Medical Image Segmentation</title><link>https://arxiv.org/abs/2601.08078</link><description>arXiv:2601.08078v1 Announce Type: new 
Abstract: Deep learning-based automatic medical image segmentation plays a critical role in clinical diagnosis and treatment planning but remains challenging in few-shot scenarios due to the scarcity of annotated training data. Recently, self-supervised foundation models such as DINOv3, which were trained on large natural image datasets, have shown strong potential for dense feature extraction that can help with the few-shot learning challenge. Yet, their direct application to medical images is hindered by domain differences. In this work, we propose DINO-AugSeg, a novel framework that leverages DINOv3 features to address the few-shot medical image segmentation challenge. Specifically, we introduce WT-Aug, a wavelet-based feature-level augmentation module that enriches the diversity of DINOv3-extracted features by perturbing frequency components, and CG-Fuse, a contextual information-guided fusion module that exploits cross-attention to integrate semantic-rich low-resolution features with spatially detailed high-resolution features. Extensive experiments on six public benchmarks spanning five imaging modalities, including MRI, CT, ultrasound, endoscopy, and dermoscopy, demonstrate that DINO-AugSeg consistently outperforms existing methods under limited-sample conditions. The results highlight the effectiveness of incorporating wavelet-domain augmentation and contextual fusion for robust feature representation, suggesting DINO-AugSeg as a promising direction for advancing few-shot medical image segmentation. Code and data will be made available on https://github.com/apple1986/DINO-AugSeg.</description><author>cs.CV updates on arXiv.org</author><pubDate>Wed, 14 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2601.08078v1</guid></item><item><title>[arXiv-CV] PathoGen: Diffusion-Based Synthesis of Realistic Lesions in Histopathology Images</title><link>https://arxiv.org/abs/2601.08127</link><description>arXiv:2601.08127v1 Announce Type: new 
Abstract: The development of robust artificial intelligence models for histopathology diagnosis is severely constrained by the scarcity of expert-annotated lesion data, particularly for rare pathologies and underrepresented disease subtypes. While data augmentation offers a potential solution, existing methods fail to generate sufficiently realistic lesion morphologies that preserve the complex spatial relationships and cellular architectures characteristic of histopathological tissues. Here we present PathoGen, a diffusion-based generative model that enables controllable, high-fidelity inpainting of lesions into benign histopathology images. Unlike conventional augmentation techniques, PathoGen leverages the iterative refinement process of diffusion models to synthesize lesions with natural tissue boundaries, preserved cellular structures, and authentic staining characteristics. We validate PathoGen across four diverse datasets representing distinct diagnostic challenges: kidney, skin, breast, and prostate pathology. Quantitative assessment confirms that PathoGen outperforms state-of-the-art generative baselines, including conditional GAN and Stable Diffusion, in image fidelity and distributional similarity. Crucially, we show that augmenting training sets with PathoGen-synthesized lesions enhances downstream segmentation performance compared to traditional geometric augmentations, particularly in data-scarce regimes. Besides, by simultaneously generating realistic morphology and pixel-level ground truth, PathoGen effectively overcomes the manual annotation bottleneck. This approach offers a scalable pathway for developing generalizable medical AI systems despite limited expert-labeled data.</description><author>cs.CV updates on arXiv.org</author><pubDate>Wed, 14 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2601.08127v1</guid></item><item><title>[arXiv-CV] ReCo-KD: Region- and Context-Aware Knowledge Distillation for Efficient 3D Medical Image Segmentation</title><link>https://arxiv.org/abs/2601.08301</link><description>arXiv:2601.08301v1 Announce Type: new 
Abstract: Accurate 3D medical image segmentation is vital for diagnosis and treatment planning, but state-of-the-art models are often too large for clinics with limited computing resources. Lightweight architectures typically suffer significant performance loss. To address these deployment and speed constraints, we propose Region- and Context-aware Knowledge Distillation (ReCo-KD), a training-only framework that transfers both fine-grained anatomical detail and long-range contextual information from a high-capacity teacher to a compact student network. The framework integrates Multi-Scale Structure-Aware Region Distillation (MS-SARD), which applies class-aware masks and scale-normalized weighting to emphasize small but clinically important regions, and Multi-Scale Context Alignment (MS-CA), which aligns teacher-student affinity patterns across feature levels. Implemented on nnU-Net in a backbone-agnostic manner, ReCo-KD requires no custom student design and is easily adapted to other architectures. Experiments on multiple public 3D medical segmentation datasets and a challenging aggregated dataset show that the distilled lightweight model attains accuracy close to the teacher while markedly reducing parameters and inference latency, underscoring its practicality for clinical deployment.</description><author>cs.CV updates on arXiv.org</author><pubDate>Wed, 14 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2601.08301v1</guid></item><item><title>[arXiv-CV] Quantization-Aware Neuromorphic Architecture for Skin Disease Classification on Resource-Constrained Devices</title><link>https://arxiv.org/abs/2507.15958</link><description>arXiv:2507.15958v3 Announce Type: replace-cross 
Abstract: On-device skin lesion analysis is constrained by the compute and energy cost of conventional CNN inference and by the need to update models as new patient data become available. We propose QANA, a quantization-aware CNN backbone embedded in an end-to-end pipeline engineered for conversion-stable neuromorphic execution. QANA replaces conversion-fragile components with spike-compatible transformations by bounding intermediate activations and aligning normalization with low-bit quantization, reducing conversion-induced distortion that disproportionately impacts rare classes. Efficiency is achieved through Ghost-based feature generation under tight FLOP budgets, while spatially-aware efficient channel attention and squeeze-and-excitation recalibrate channels without heavy global operators that are difficult to map to spiking cores. The resulting quantized projection head produces SNN-ready logits and enables incremental updates on edge hardware without full retraining or data offloading. On HAM10000, QANA achieves 91.6% Top-1 accuracy and 91.0% macro F1, improving the strongest converted SNN baseline by 3.5 percentage points in Top-1 accuracy, corresponding to a 4.0% relative gain, and by 12.0 points in macro F1, corresponding to a 15.2% relative gain. On a clinical dataset, QANA achieves 90.8% Top-1 accuracy and 81.7% macro F1, improving the strongest converted SNN baseline by 3.2 points in Top-1 accuracy, which corresponds to a 3.7% relative gain, and by 3.6 points in macro F1, corresponding to a 4.6% relative gain. When deployed on BrainChip Akida, QANA runs in 1.5 ms per image with 1.7 mJ per image, corresponding to 94.6% lower latency and 99.0% lower energy than its GPU-based CNN implementation.</description><author>cs.CV updates on arXiv.org</author><pubDate>Wed, 14 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2507.15958v3</guid></item><item><title>[arXiv-ML] Affect and Effect: Limitations of regularisation-based continual learning in EEG-based emotion classification</title><link>https://arxiv.org/abs/2601.07858</link><description>arXiv:2601.07858v1 Announce Type: new 
Abstract: Generalisation to unseen subjects in EEG-based emotion classification remains a challenge due to high inter-and intra-subject variability. Continual learning (CL) poses a promising solution by learning from a sequence of tasks while mitigating catastrophic forgetting. Regularisation-based CL approaches, such as Elastic Weight Consolidation (EWC), Synaptic Intelligence (SI), and Memory Aware Synapses (MAS), are commonly used as baselines in EEG-based CL studies, yet their suitability for this problem remains underexplored. This study theoretically and empirically finds that regularisation-based CL methods show limited performance for EEG-based emotion classification on the DREAMER and SEED datasets. We identify a fundamental misalignment in the stability-plasticity trade-off, where regularisation-based methods prioritise mitigating catastrophic forgetting (backward transfer) over adapting to new subjects (forward transfer). We investigate this limitation under subject-incremental sequences and observe that: (1) the heuristics for estimating parameter importance become less reliable under noisy data and covariate shift, (2) gradients on parameters deemed important by these heuristics often interfere with gradient updates required for new subjects, moving optimisation away from the minimum, (3) importance values accumulated across tasks over-constrain the model, and (4) performance is sensitive to subject order. Forward transfer showed no statistically significant improvement over sequential fine-tuning (p &gt; 0.05 across approaches and datasets). The high variability of EEG signals means past subjects provide limited value to future subjects. Regularisation-based continual learning approaches are therefore limited for robust generalisation to unseen subjects in EEG-based emotion classification.</description><author>cs.LG updates on arXiv.org</author><pubDate>Wed, 14 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2601.07858v1</guid></item><item><title>[arXiv-ML] Sherry: Hardware-Efficient 1.25-Bit Ternary Quantization via Fine-grained Sparsification</title><link>https://arxiv.org/abs/2601.07892</link><description>arXiv:2601.07892v1 Announce Type: new 
Abstract: The deployment of Large Language Models (LLMs) on resource-constrained edge devices is increasingly hindered by prohibitive memory and computational requirements. While ternary quantization offers a compelling solution by reducing weights to {-1, 0, +1}, current implementations suffer from a fundamental misalignment with commodity hardware. Most existing methods must choose between 2-bit aligned packing, which incurs significant bit wastage, or 1.67-bit irregular packing, which degrades inference speed. To resolve this tension, we propose Sherry, a hardware-efficient ternary quantization framework. Sherry introduces a 3:4 fine-grained sparsity that achieves a regularized 1.25-bit width by packing blocks of four weights into five bits, restoring power-of-two alignment. Furthermore, we identify weight trapping issue in sparse ternary training, which leads to representational collapse. To address this, Sherry introduces Arenas, an annealing residual synapse mechanism that maintains representational diversity during training. Empirical evaluations on LLaMA-3.2 across five benchmarks demonstrate that Sherry matches state-of-the-art ternary performance while significantly reducing model size. Notably, on an Intel i7-14700HX CPU, our 1B model achieves zero accuracy loss compared to SOTA baselines while providing 25% bit savings and 10% speed up. The code is available at https://github.com/Tencent/AngelSlim .</description><author>cs.LG updates on arXiv.org</author><pubDate>Wed, 14 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2601.07892v1</guid></item><item><title>[arXiv-ML] Intra-tree Column Subsampling Hinders XGBoost Learning of Ratio-like Interactions</title><link>https://arxiv.org/abs/2601.08121</link><description>arXiv:2601.08121v1 Announce Type: new 
Abstract: Many applied problems contain signal that becomes clear only after combining multiple raw measurements. Ratios and rates are common examples. In gradient boosted trees, this combination is not an explicit operation: the model must synthesize it through coordinated splits on the component features. We study whether intra-tree column subsampling in XGBoost makes that synthesis harder. We use two synthetic data generating processes with cancellation-style structure. In both, two primitive features share a strong nuisance factor, while the target depends on a smaller differential factor. A log ratio cancels the nuisance and isolates the signal. We vary colsample_bylevel and colsample_bynode over s in {0.4, 0.6, 0.8, 0.9}, emphasizing mild subsampling (s &gt;= 0.8). A control feature set includes the engineered ratio, removing the need for synthesis. Across both processes, intra-tree column subsampling reduces test PR-AUC in the primitives-only setting. In the main process the relative decrease reaches 54 percent when both parameters are set to 0.4. The effect largely disappears when the engineered ratio is present. A path-based co-usage metric drops in the same cells where performance deteriorates. Practically, if ratio-like structure is plausible, either avoid intra-tree subsampling or include the intended ratio features.</description><author>cs.LG updates on arXiv.org</author><pubDate>Wed, 14 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2601.08121v1</guid></item><item><title>[arXiv-ML] TabPFN Through The Looking Glass: An interpretability study of TabPFN and its internal representations</title><link>https://arxiv.org/abs/2601.08181</link><description>arXiv:2601.08181v1 Announce Type: new 
Abstract: Tabular foundational models are pre-trained models designed for a wide range of tabular data tasks. They have shown strong performance across domains, yet their internal representations and learned concepts remain poorly understood. This lack of interpretability makes it important to study how these models process and transform input features. In this work, we analyze the information encoded inside the model's hidden representations and examine how these representations evolve across layers. We run a set of probing experiments that test for the presence of linear regression coefficients, intermediate values from complex expressions, and the final answer in early layers. These experiments allow us to reason about the computations the model performs internally. Our results provide evidence that meaningful and structured information is stored inside the representations of tabular foundational models. We observe clear signals that correspond to both intermediate and final quantities involved in the model's prediction process. This gives insight into how the model refines its inputs and how the final output emerges. Our findings contribute to a deeper understanding of the internal mechanics of tabular foundational models. They show that these models encode concrete and interpretable information, which moves us closer to making their decision processes more transparent and trustworthy.</description><author>cs.LG updates on arXiv.org</author><pubDate>Wed, 14 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2601.08181v1</guid></item><item><title>[arXiv-ML] Taxon: Hierarchical Tax Code Prediction with Semantically Aligned LLM Expert Guidance</title><link>https://arxiv.org/abs/2601.08418</link><description>arXiv:2601.08418v1 Announce Type: new 
Abstract: Tax code prediction is a crucial yet underexplored task in automating invoicing and compliance management for large-scale e-commerce platforms. Each product must be accurately mapped to a node within a multi-level taxonomic hierarchy defined by national standards, where errors lead to financial inconsistencies and regulatory risks. This paper presents Taxon, a semantically aligned and expert-guided framework for hierarchical tax code prediction. Taxon integrates (i) a feature-gating mixture-of-experts architecture that adaptively routes multi-modal features across taxonomy levels, and (ii) a semantic consistency model distilled from large language models acting as domain experts to verify alignment between product titles and official tax definitions. To address noisy supervision in real business records, we design a multi-source training pipeline that combines curated tax databases, invoice validation logs, and merchant registration data to provide both structural and semantic supervision. Extensive experiments on the proprietary TaxCode dataset and public benchmarks demonstrate that Taxon achieves state-of-the-art performance, outperforming strong baselines. Further, an additional full hierarchical paths reconstruction procedure significantly improves structural consistency, yielding the highest overall F1 scores. Taxon has been deployed in production within Alibaba's tax service system, handling an average of over 500,000 tax code queries per day and reaching peak volumes above five million requests during business event with improved accuracy, interpretability, and robustness.</description><author>cs.LG updates on arXiv.org</author><pubDate>Wed, 14 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2601.08418v1</guid></item><item><title>[arXiv-ML] From Word Sequences to Behavioral Sequences: Adapting Modeling and Evaluation Paradigms for Longitudinal NLP</title><link>https://arxiv.org/abs/2601.07988</link><description>arXiv:2601.07988v1 Announce Type: cross 
Abstract: While NLP typically treats documents as independent and unordered samples, in longitudinal studies, this assumption rarely holds: documents are nested within authors and ordered in time, forming person-indexed, time-ordered $\textit{behavioral sequences}$. Here, we demonstrate the need for and propose a longitudinal modeling and evaluation paradigm that consequently updates four parts of the NLP pipeline: (1) evaluation splits aligned to generalization over people ($\textit{cross-sectional}$) and/or time ($\textit{prospective}$); (2) accuracy metrics separating between-person differences from within-person dynamics; (3) sequence inputs to incorporate history by default; and (4) model internals that support different $\textit{coarseness}$ of latent state over histories (pooled summaries, explicit dynamics, or interaction-based models). We demonstrate the issues ensued by traditional pipeline and our proposed improvements on a dataset of 17k daily diary transcripts paired with PTSD symptom severity from 238 participants, finding that traditional document-level evaluation can yield substantially different and sometimes reversed conclusions compared to our ecologically valid modeling and evaluation. We tie our results to a broader discussion motivating a shift from word-sequence evaluation toward $\textit{behavior-sequence}$ paradigms for NLP.</description><author>cs.LG updates on arXiv.org</author><pubDate>Wed, 14 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2601.07988v1</guid></item><item><title>[arXiv-ML] CSQL: Mapping Documents into Causal Databases</title><link>https://arxiv.org/abs/2601.08109</link><description>arXiv:2601.08109v1 Announce Type: cross 
Abstract: We describe a novel system, CSQL, which automatically converts a collection of unstructured text documents into an SQL-queryable causal database (CDB). A CDB differs from a traditional DB: it is designed to answer "why'' questions via causal interventions and structured causal queries. CSQL builds on our earlier system, DEMOCRITUS, which converts documents into thousands of local causal models derived from causal discourse. Unlike RAG-based systems or knowledge-graph based approaches, CSQL supports causal analysis over document collections rather than purely associative retrieval. For example, given an article on the origins of human bipedal walking, CSQL enables queries such as: "What are the strongest causal influences on bipedalism?'' or "Which variables act as causal hubs with the largest downstream influence?'' Beyond single-document case studies, we show that CSQL can also ingest RAG/IE-compiled causal corpora at scale by compiling the Testing Causal Claims (TCC) dataset of economics papers into a causal database containing 265,656 claim instances spanning 45,319 papers, 44 years, and 1,575 reported method strings, thereby enabling corpus-level causal queries and longitudinal analyses in CSQL. Viewed abstractly, CSQL functions as a compiler from unstructured documents into a causal database equipped with a principled algebra of queries, and can be applied broadly across many domains ranging from business, humanities, and science.</description><author>cs.LG updates on arXiv.org</author><pubDate>Wed, 14 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2601.08109v1</guid></item><item><title>[arXiv-ML] MLPlatt: Simple Calibration Framework for Ranking Models</title><link>https://arxiv.org/abs/2601.08345</link><description>arXiv:2601.08345v1 Announce Type: cross 
Abstract: Ranking models are extensively used in e-commerce for relevance estimation. These models often suffer from poor interpretability and no scale calibration, particularly when trained with typical ranking loss functions. This paper addresses the problem of post-hoc calibration of ranking models. We introduce MLPlatt: a simple yet effective ranking model calibration method that preserves the item ordering and converts ranker outputs to interpretable click-through rate (CTR) probabilities usable in downstream tasks. The method is context-aware by design and achieves good calibration metrics globally, and within strata corresponding to different values of a selected categorical field (such as user country or device), which is often important from a business perspective of an E-commerce platform. We demonstrate the superiority of MLPlatt over existing approaches on two datasets, achieving an improvement of over 10\% in F-ECE (Field Expected Calibration Error) compared to other methods. Most importantly, we show that high-quality calibration can be achieved without compromising the ranking quality.</description><author>cs.LG updates on arXiv.org</author><pubDate>Wed, 14 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2601.08345v1</guid></item><item><title>[arXiv-ML] Robust Single-Agent Reinforcement Learning for Regional Traffic Signal Control Under Demand Fluctuations</title><link>https://arxiv.org/abs/2511.00549</link><description>arXiv:2511.00549v2 Announce Type: replace 
Abstract: Traffic congestion, primarily driven by intersection queuing, significantly impacts urban living standards, safety, environmental quality, and economic efficiency. While Traffic Signal Control (TSC) systems hold potential for congestion mitigation, traditional optimization models often fail to capture real-world traffic complexity and dynamics. This study introduces a novel single-agent reinforcement learning (RL) framework for regional adaptive TSC, circumventing the coordination complexities inherent in multi-agent systems through a centralized decision-making paradigm. The model employs an adjacency matrix to unify the encoding of road network topology, real-time queue states derived from probe vehicle data, and current signal timing parameters. Leveraging the efficient learning capabilities of the DreamerV3 world model, the agent learns control policies where actions sequentially select intersections and adjust their signal phase splits to regulate traffic inflow/outflow, analogous to a feedback control system. Reward design prioritizes queue dissipation, directly linking congestion metrics (queue length) to control actions. Simulation experiments conducted in SUMO demonstrate the model's effectiveness: under inference scenarios with multi-level (10%, 20%, 30%) Origin-Destination (OD) demand fluctuations, the framework exhibits robust anti-fluctuation capability and significantly reduces queue lengths. This work establishes a new paradigm for intelligent traffic control compatible with probe vehicle technology. Future research will focus on enhancing practical applicability by incorporating stochastic OD demand fluctuations during training and exploring regional optimization mechanisms for contingency events.</description><author>cs.LG updates on arXiv.org</author><pubDate>Wed, 14 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2511.00549v2</guid></item><item><title>[BSPC] Dimensionality reduction for Explainable AI in skin lesion Classification: A sparse autoencoder and Lime-Based framework</title><link>https://www.sciencedirect.com/science/article/pii/S1746809426001357?dgcid=rss_sd_all</link><description>&lt;p&gt;Publication date: 1 May 2026&lt;/p&gt;&lt;p&gt;&lt;b&gt;Source:&lt;/b&gt; Biomedical Signal Processing and Control, Volume 116&lt;/p&gt;&lt;p&gt;Author(s): S. Praveena, T. Veeramakali&lt;/p&gt;</description><author>ScienceDirect Publication: Biomedical Signal Processing and Control</author><pubDate>Wed, 14 Jan 2026 02:08:39 GMT</pubDate><guid isPermaLink="true">https://www.sciencedirect.com/science/article/pii/S1746809426001357</guid></item><item><title>[BSPC] Exploring Image Processing Techniques for Skin Lesion Detection focusing on Segmentation and Classification: A Systematic Review</title><link>https://www.sciencedirect.com/science/article/pii/S1746809426000649?dgcid=rss_sd_all</link><description>&lt;p&gt;Publication date: 1 May 2026&lt;/p&gt;&lt;p&gt;&lt;b&gt;Source:&lt;/b&gt; Biomedical Signal Processing and Control, Volume 116&lt;/p&gt;&lt;p&gt;Author(s): Renu Bala, Neelam Duhan, Komal Kumar Bhatia&lt;/p&gt;</description><author>ScienceDirect Publication: Biomedical Signal Processing and Control</author><pubDate>Wed, 14 Jan 2026 02:08:39 GMT</pubDate><guid isPermaLink="true">https://www.sciencedirect.com/science/article/pii/S1746809426000649</guid></item><item><title>[arXiv-CV] HyperTopo-Adapters: Geometry- and Topology-Aware Segmentation of Leaf Lesions on Frozen Encoders</title><link>https://arxiv.org/abs/2601.06067</link><description>arXiv:2601.06067v1 Announce Type: new 
Abstract: Leaf-lesion segmentation is topology-sensitive: small merges, splits, or false holes can be biologically meaningful descriptors of biochemical pathways, yet they are weakly penalized by standard pixel-wise losses in Euclidean latents. I explore HyperTopo-Adapters, a lightweight, parameter-efficient head trained on top of a frozen vision encoder, which embeds features on a product manifold -- hyperbolic + Euclidean + spherical (H + E + S) -- to encourage hierarchical separation (H), local linear detail (E), and global closure (S). A topology prior complements Dice/BCE in two forms: (i) persistent-homology (PH) distance for evaluation and selection, and (ii) a differentiable surrogate that combines a soft Euler-characteristic match with total variation regularization for stable training. I introduce warm-ups for both the hyperbolic contrastive term and the topology prior, per-sample evaluation of structure-aware metrics (Boundary-F1, Betti errors, PD distance), and a min-PD within top-K Dice rule for checkpoint selection. On a Kaggle leaf-lesion dataset (N=2,940), early results show consistent gains in boundary and topology metrics (reducing Delta beta_1 hole error by 9%) while Dice/IoU remain competitive. The study is diagnostic by design: I report controlled ablations (curvature learning, latent dimensions, contrastive temperature, surrogate settings), and ongoing tests varying encoder strength (ResNet-50, DeepLabV3, DINOv2/v3), input resolution, PH weight, and partial unfreezing of late blocks. The contribution is an open, reproducible train/eval suite (available at https://github.com/ChimdiWalter/HyperTopo-Adapters) that isolates geometric/topological priors and surfaces failure modes to guide stronger, topology-preserving architectures.</description><author>cs.CV updates on arXiv.org</author><pubDate>Tue, 13 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2601.06067v1</guid></item><item><title>[arXiv-CV] A Unified Attention U-Net Framework for Cross-Modality Tumor Segmentation in MRI and CT</title><link>https://arxiv.org/abs/2601.06187</link><description>arXiv:2601.06187v1 Announce Type: new 
Abstract: This study presents a unified Attention U-Net architecture trained jointly on MRI (BraTS 2021) and CT (LIDC-IDRI) datasets to investigate the generalizability of a single model across diverse imaging modalities and anatomical sites. Our proposed pipeline incorporates modality-harmonized preprocessing, attention-gated skip connections, and a modality-aware Focal Tversky loss function. To the best of our knowledge, this study is among the first to evaluate a single Attention U-Net trained simultaneously on separate MRI (BraTS) and CT (LIDC-IDRI) tumor datasets, without relying on modality-specific encoders or domain adaptation. The unified model demonstrates competitive performance in terms of Dice coefficient, IoU, and AUC on both domains, thereby establishing a robust and reproducible baseline for future research in cross-modality tumor segmentation.</description><author>cs.CV updates on arXiv.org</author><pubDate>Tue, 13 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2601.06187v1</guid></item><item><title>[arXiv-CV] When Imbalance Comes Twice: Active Learning under Simulated Class Imbalance and Label Shift in Binary Semantic Segmentation</title><link>https://arxiv.org/abs/2601.06209</link><description>arXiv:2601.06209v1 Announce Type: new 
Abstract: The aim of Active Learning is to select the most informative samples from an unlabelled set of data. This is useful in cases where the amount of data is large and labelling is expensive, such as in machine vision or medical imaging. Two particularities of machine vision are first, that most of the images produced are free of defects, and second, that the amount of images produced is so big that we cannot store all acquired images. This results, on the one hand, in a strong class imbalance in defect distribution and, on the other hand, in a potential label shift caused by limited storage. To understand how these two forms of imbalance affect active learning algorithms, we propose a simulation study based on two open-source datasets. We artificially create datasets for which we control the levels of class imbalance and label shift. Three standard active learning selection strategies are compared: random sampling, entropy-based selection, and core-set selection. We demonstrate that active learning strategies, and in particular the entropy-based and core-set selections, remain interesting and efficient even for highly imbalanced datasets. We also illustrate and measure the loss of efficiency that occurs in the situation a strong label shift.</description><author>cs.CV updates on arXiv.org</author><pubDate>Tue, 13 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2601.06209v1</guid></item><item><title>[arXiv-CV] SIRR-LMM: Single-image Reflection Removal via Large Multimodal Model</title><link>https://arxiv.org/abs/2601.07209</link><description>arXiv:2601.07209v1 Announce Type: new 
Abstract: Glass surfaces create complex interactions of reflected and transmitted light, making single-image reflection removal (SIRR) challenging. Existing datasets suffer from limited physical realism in synthetic data or insufficient scale in real captures. We introduce a synthetic dataset generation framework that path-traces 3D glass models over real background imagery to create physically accurate reflection scenarios with varied glass properties, camera settings, and post-processing effects. To leverage the capabilities of Large Multimodal Model (LMM), we concatenate the image layers into a single composite input, apply joint captioning, and fine-tune the model using task-specific LoRA rather than full-parameter training. This enables our approach to achieve improved reflection removal and separation performance compared to state-of-the-art methods.</description><author>cs.CV updates on arXiv.org</author><pubDate>Tue, 13 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2601.07209v1</guid></item><item><title>[arXiv-CV] Explainable Deep Radiogenomic Molecular Imaging for MGMT Methylation Prediction in Glioblastoma</title><link>https://arxiv.org/abs/2601.07035</link><description>arXiv:2601.07035v1 Announce Type: cross 
Abstract: Glioblastoma (GBM) is a highly aggressive primary brain tumor with limited therapeutic options and poor prognosis. The methylation status of the O6-methylguanine-DNA methyltransferase (MGMT) gene promoter is a critical molecular biomarker that influences patient response to temozolomide chemotherapy. Traditional methods for determining MGMT status rely on invasive biopsies and are limited by intratumoral heterogeneity and procedural risks. This study presents a radiogenomic molecular imaging analysis framework for the non-invasive prediction of MGMT promoter methylation using multi-parametric magnetic resonance imaging (mpMRI).
  Our approach integrates radiomics, deep learning, and explainable artificial intelligence (XAI) to analyze MRI-derived imaging phenotypes and correlate them with molecular labels. Radiomic features are extracted from FLAIR, T1-weighted, T1-contrast-enhanced, and T2-weighted MRI sequences, while a 3D convolutional neural network learns deep representations from the same modalities. These complementary features are fused using both early fusion and attention-based strategies and classified to predict MGMT methylation status.
  To enhance clinical interpretability, we apply XAI methods such as Grad-CAM and SHAP to visualize and explain model decisions. The proposed framework is trained on the RSNA-MICCAI Radiogenomic Classification dataset and externally validated on the BraTS 2021 dataset. This work advances the field of molecular imaging by demonstrating the potential of AI-driven radiogenomics for precision oncology, supporting non-invasive, accurate, and interpretable prediction of clinically actionable molecular biomarkers in GBM.</description><author>cs.CV updates on arXiv.org</author><pubDate>Tue, 13 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2601.07035v1</guid></item><item><title>[arXiv-CV] A Multimodal Dataset of Student Oral Presentations with Sensors and Evaluation Data</title><link>https://arxiv.org/abs/2601.07576</link><description>arXiv:2601.07576v1 Announce Type: cross 
Abstract: Oral presentation skills are a critical component of higher education, yet comprehensive datasets capturing real-world student performance across multiple modalities remain scarce. To address this gap, we present SOPHIAS (Student Oral Presentation monitoring for Holistic Insights &amp; Analytics using Sensors), a 12-hour multimodal dataset containing recordings of 50 oral presentations (10-15-minute presentation followed by 5-15-minute Q&amp;amp;A) delivered by 65 undergraduate and master's students at the Universidad Autonoma de Madrid. SOPHIAS integrates eight synchronized sensor streams from high-definition webcams, ambient and webcam audio, eye-tracking glasses, smartwatch physiological sensors, and clicker, keyboard, and mouse interactions. In addition, the dataset includes slides and rubric-based evaluations from teachers, peers, and self-assessments, along with timestamped contextual annotations. The dataset captures presentations conducted in real classroom settings, preserving authentic student behaviors, interactions, and physiological responses. SOPHIAS enables the exploration of relationships between multimodal behavioral and physiological signals and presentation performance, supports the study of peer assessment, and provides a benchmark for developing automated feedback and Multimodal Learning Analytics tools. The dataset is publicly available for research through GitHub and Science Data Bank.</description><author>cs.CV updates on arXiv.org</author><pubDate>Tue, 13 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2601.07576v1</guid></item><item><title>[arXiv-CV] Does DINOv3 Set a New Medical Vision Standard?</title><link>https://arxiv.org/abs/2509.06467</link><description>arXiv:2509.06467v2 Announce Type: replace 
Abstract: The advent of large-scale vision foundation models, pre-trained on diverse natural images, has marked a paradigm shift in computer vision. However, how the frontier vision foundation models' efficacies transfer to specialized domains remains such as medical imaging remains an open question. This report investigates whether DINOv3, a state-of-the-art self-supervised vision transformer (ViT) that features strong capability in dense prediction tasks, can directly serve as a powerful, unified encoder for medical vision tasks without domain-specific pre-training. To answer this, we benchmark DINOv3 across common medical vision tasks, including 2D/3D classification and segmentation on a wide range of medical imaging modalities. We systematically analyze its scalability by varying model sizes and input image resolutions. Our findings reveal that DINOv3 shows impressive performance and establishes a formidable new baseline. Remarkably, it can even outperform medical-specific foundation models like BiomedCLIP and CT-Net on several tasks, despite being trained solely on natural images. However, we identify clear limitations: The model's features degrade in scenarios requiring deep domain specialization, such as in Whole-Slide Pathological Images (WSIs), Electron Microscopy (EM), and Positron Emission Tomography (PET). Furthermore, we observe that DINOv3 does not consistently obey scaling law in the medical domain; performance does not reliably increase with larger models or finer feature resolutions, showing diverse scaling behaviors across tasks. Ultimately, our work establishes DINOv3 as a strong baseline, whose powerful visual features can serve as a robust prior for multiple complex medical tasks. This opens promising future directions, such as leveraging its features to enforce multiview consistency in 3D reconstruction.</description><author>cs.CV updates on arXiv.org</author><pubDate>Tue, 13 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2509.06467v2</guid></item><item><title>[arXiv-CV] Parameter-efficient fine-tuning (PEFT) of Vision Foundation Models for Atypical Mitotic Figure Classification</title><link>https://arxiv.org/abs/2509.16935</link><description>arXiv:2509.16935v2 Announce Type: replace 
Abstract: Atypical mitotic figures (AMFs) are rare abnormal cell divisions associated with tumor aggressiveness and poor prognosis. Their detection remains a significant challenge due to subtle morphological cues, class imbalance, and inter-observer variability among pathologists. The MIDOG 2025 challenge introduced a dedicated track for atypical mitosis classification, enabling systematic evaluation of deep learning methods. In this study, we investigated the use of large vision foundation models, including Virchow, Virchow2, and UNI, with Low-Rank Adaptation (LoRA) for parameter-efficient fine-tuning. We conducted extensive experiments with different LoRA ranks, as well as random and group-based data splits, to analyze robustness under varied conditions. Our best approach, Virchow with LoRA rank 8 and ensemble of three-fold cross-validation, achieved a balanced accuracy of 88.37% on the preliminary test set, ranking joint 9th in the challenge leaderboard. These results highlight the promise of foundation models with efficient adaptation strategies for the classification of atypical mitosis, while underscoring the need for improvements in specificity and domain generalization.</description><author>cs.CV updates on arXiv.org</author><pubDate>Tue, 13 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2509.16935v2</guid></item><item><title>[arXiv-CV] Encoding Structural Constraints into Segment Anything Models via Probabilistic Graphical Models</title><link>https://arxiv.org/abs/2509.21750</link><description>arXiv:2509.21750v2 Announce Type: replace 
Abstract: While the Segment Anything Model (SAM) has achieved remarkable success in image segmentation, its direct application to medical imaging remains hindered by fundamental challenges, including ambiguous boundaries, insufficient modeling of anatomical relationships, and the absence of uncertainty quantification. To address these limitations, we introduce KG-SAM, a knowledge-guided framework that synergistically integrates anatomical priors with boundary refinement and uncertainty estimation. Specifically, KG-SAM incorporates (i) a medical knowledge graph to encode fine-grained anatomical relationships, (ii) an energy-based Conditional Random Field (CRF) to enforce anatomically consistent predictions, and (iii) an uncertainty-aware fusion module to enhance reliability in high-stakes clinical scenarios. Extensive experiments across multi-center medical datasets demonstrate the effectiveness of our approach: KG-SAM achieves an average Dice score of 82.69% on prostate segmentation and delivers substantial gains in abdominal segmentation, reaching 78.05% on MRI and 79.68% on CT. These results establish KG-SAM as a robust and generalizable framework for advancing medical image segmentation.</description><author>cs.CV updates on arXiv.org</author><pubDate>Tue, 13 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2509.21750v2</guid></item><item><title>[arXiv-CV] ReBrain: Brain MRI Reconstruction from Sparse CT Slice via Retrieval-Augmented Diffusion</title><link>https://arxiv.org/abs/2511.17068</link><description>arXiv:2511.17068v3 Announce Type: replace 
Abstract: Magnetic Resonance Imaging (MRI) plays a crucial role in brain disease diagnosis, but it is not always feasible for certain patients due to physical or clinical constraints. Recent studies attempt to synthesize MRI from Computed Tomography (CT) scans; however, low-dose protocols often result in highly sparse CT volumes with poor through-plane resolution, making accurate reconstruction of the full brain MRI volume particularly challenging. To address this, we propose ReBrain, a retrieval-augmented diffusion framework for brain MRI reconstruction. Given any 3D CT scan with limited slices, we first employ a Brownian Bridge Diffusion Model (BBDM) to synthesize MRI slices along the 2D dimension. Simultaneously, we retrieve structurally and pathologically similar CT slices from a comprehensive prior database via a fine-tuned retrieval model. These retrieved slices are used as references, incorporated through a ControlNet branch to guide the generation of intermediate MRI slices and ensure structural continuity. We further account for rare retrieval failures when the database lacks suitable references and apply spherical linear interpolation to provide supplementary guidance. Extensive experiments on SynthRAD2023 and BraTS demonstrate that ReBrain achieves state-of-the-art performance in cross-modal reconstruction under sparse conditions.</description><author>cs.CV updates on arXiv.org</author><pubDate>Tue, 13 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2511.17068v3</guid></item><item><title>[arXiv-CV] Diagnostic Performance of Universal-Learning Ultrasound AI Across Multiple Organs and Tasks: the UUSIC25 Challenge</title><link>https://arxiv.org/abs/2512.17279</link><description>arXiv:2512.17279v2 Announce Type: replace 
Abstract: IMPORTANCE: Modern ultrasound systems are universal diagnostic tools capable of imaging the entire body. However, current AI solutions remain fragmented into single-task tools. This critical gap between hardware versatility and software specificity limits workflow integration and clinical utility.
  OBJECTIVE: To evaluate the diagnostic accuracy, versatility, and efficiency of single general-purpose deep learning models for multi-organ classification and segmentation.
  DESIGN: The Universal UltraSound Image Challenge 2025 (UUSIC25) involved developing algorithms on 11,644 images aggregated from 12 sources (9 public, 3 private). Evaluation used an independent, multi-center private test set of 2,479 images, including data from a center completely unseen during training to assess generalization.
  OUTCOMES: Diagnostic performance (Dice Similarity Coefficient [DSC]; Area Under the Receiver Operating Characteristic Curve [AUC]) and computational efficiency (inference time, GPU memory).
  RESULTS: Of 15 valid algorithms, the top model (SMART) achieved a macro-averaged DSC of 0.854 across 5 segmentation tasks and AUC of 0.766 for binary classification. Models demonstrated high capability in anatomical segmentation (e.g., fetal head DSC: 0.942) but variability in complex diagnostic tasks subject to domain shift. Specifically, in breast cancer molecular subtyping, the top model's performance dropped from an AUC of 0.571 (internal) to 0.508 (unseen external center), highlighting the challenge of generalization.
  CONCLUSIONS: General-purpose AI models can achieve high accuracy and efficiency across multiple tasks using a single architecture. However, significant performance degradation on unseen data suggests domain generalization is critical for future clinical deployment.</description><author>cs.CV updates on arXiv.org</author><pubDate>Tue, 13 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2512.17279v2</guid></item><item><title>[arXiv-ML] A Foundation Model Approach for Fetal Stress Prediction During Labor From cardiotocography (CTG) recordings</title><link>https://arxiv.org/abs/2601.06149</link><description>arXiv:2601.06149v1 Announce Type: new 
Abstract: Intrapartum cardiotocography (CTG) is widely used for fetal monitoring during labor, yet its interpretation suffers from high inter-observer variability and limited predictive accuracy. Deep learning approaches have been constrained by the scarcity of CTG recordings with clinical outcome labels. We present the first application of self-supervised pre-training to intrapartum CTG analysis, leveraging 2,444 hours of unlabeled recordings for masked pre-training followed by fine-tuning on the 552-recording CTU-UHB benchmark. Using a PatchTST transformer architecture with a channel-asymmetric masking scheme designed for fetal heart rate reconstruction, we achieve an area under the receiver operating characteristic curve of 0.83 on the full test set and 0.853 on uncomplicated vaginal deliveries, exceeding previously reported results on this benchmark (0.68-0.75). Error analysis reveals that false-positive alerts typically correspond to CTG patterns judged concerning on retrospective clinical review, suggesting clinically meaningful predictions even when umbilical pH is normal. We release standardized dataset splits and model weights to enable reproducible benchmarking. Our results demonstrate that self-supervised pre-training can address data scarcity in fetal monitoring, offering a path toward reliable decision support in the labor room.</description><author>cs.LG updates on arXiv.org</author><pubDate>Tue, 13 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2601.06149v1</guid></item><item><title>[arXiv-ML] Cyber Threat Detection and Vulnerability Assessment System using Generative AI and Large Language Model</title><link>https://arxiv.org/abs/2601.06213</link><description>arXiv:2601.06213v1 Announce Type: cross 
Abstract: Background: Cyber-attacks have evolved rapidly in recent years, many individuals and business owners have been affected by cyber-attacks in various ways. Cyber-attacks include various threats such as ransomware, malware, phishing, and Denial of Service (DoS)-related attacks. Challenges: Traditional models such as Generative Artificial Intelligence (AI) and Security Bidirectional Encoder Representations from Transformers (BERT) were implemented to detect cyber threats. However, the existing Security BERT model has a limited contextual understanding of text data, which has less impact on detecting cyber-attacks. Proposed Methodology: To overcome the above-mentioned challenges, Robustly Optimized Bidirectional Encoder Representations from Transformers Pretraining Approach (RoBERTa) model is proposed which consists of diverse words of vocabulary understanding. Initially, data are extracted from a Packet Capture (PCAP) file and encrypted using Fully Harmonic Encryption (FHE). Subsequently, a Byte-level and Byte Pair Encoding (BBPE) tokenizer was used to generate tokens and help maintain the vocabulary for the encrypted values. Then, these values are applied to the RoBERTa model of the transformer with extensive training. Finally, Softmax is used for the detection and classification of attacks. The proposed RoBERTa model achieved better results than the existing BERT model in terms of accuracy (0.99), recall (0.91), and precision (0.89) respectively.</description><author>cs.LG updates on arXiv.org</author><pubDate>Tue, 13 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2601.06213v1</guid></item><item><title>[arXiv-ML] A Backpropagation-Free Feedback-Hebbian Network for Continual Learning Dynamics</title><link>https://arxiv.org/abs/2601.06758</link><description>arXiv:2601.06758v1 Announce Type: cross 
Abstract: Feedback-rich neural architectures can regenerate earlier representations and inject temporal context, making them a natural setting for strictly local synaptic plasticity. We ask whether a minimal, backpropagation-free feedback--Hebbian system can already express interpretable continual-learning--relevant behaviors under controlled training schedules. We introduce a compact prediction--reconstruction architecture with two feedforward layers for supervised association learning and two dedicated feedback layers trained to reconstruct earlier activity and re-inject it as additive temporal context. All synapses are updated by a unified local rule combining centered Hebbian covariance, Oja-style stabilization, and a local supervised drive where targets are available, requiring no weight transport or global error backpropagation. On a small two-pair association task, we characterize learning through layer-wise activity snapshots, connectivity trajectories (row/column means of learned weights), and a normalized retention index across phases. Under sequential A-&gt;B training, forward output connectivity exhibits a long-term depression (LTD)-like suppression of the earlier association while feedback connectivity preserves an A-related trace during acquisition of B. Under deterministic interleaving A,B,A,B,..., both associations are concurrently maintained rather than sequentially suppressed. Architectural controls and rule-term ablations isolate the role of dedicated feedback in regeneration and co-maintenance, and the role of the local supervised term in output selectivity and unlearning. Together, the results show that a compact feedback pathway trained with local plasticity can support regeneration and continual-learning--relevant dynamics in a minimal, mechanistically transparent setting.</description><author>cs.LG updates on arXiv.org</author><pubDate>Tue, 13 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2601.06758v1</guid></item><item><title>[arXiv-ML] Unity Forests: Improving Interaction Modelling and Interpretability in Random Forests</title><link>https://arxiv.org/abs/2601.07003</link><description>arXiv:2601.07003v1 Announce Type: cross 
Abstract: Random forests (RFs) are widely used for prediction and variable importance analysis and are often believed to capture any types of interactions via recursive splitting. However, since the splits are chosen locally, interactions are only reliably captured when at least one involved covariate has a marginal effect. We introduce unity forests (UFOs), an RF variant designed to better exploit interactions involving covariates without marginal effects. In UFOs, the first few splits of each tree are optimized jointly across a random covariate subset to form a "tree root" capturing such interactions; the remainder is grown conventionally. We further propose the unity variable importance measure (VIM), which is based on out-of-bag split criterion values from the tree roots. Here, only a small fraction of tree root splits with the highest in-bag criterion values are considered per covariate, reflecting that covariates with purely interaction-based effects are discriminative only if a split in an interacting covariate occurred earlier in the tree. Finally, we introduce covariate-representative tree roots (CRTRs), which select representative tree roots per covariate and provide interpretable insight into the conditions - marginal or interactive - under which each covariate has its strongest effects. In a simulation study, the unity VIM reliably identified interacting covariates without marginal effects, unlike conventional RF-based VIMs. In a large-scale real-data comparison, UFOs achieved higher discrimination and predictive accuracy than standard RFs, with comparable calibration. The CRTRs reproduced the covariates' true effect types reliably in simulated data and provided interesting insights in a real data analysis.</description><author>cs.LG updates on arXiv.org</author><pubDate>Tue, 13 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2601.07003v1</guid></item><item><title>[arXiv-ML] Covariance-Driven Regression Trees: Reducing Overfitting in CART</title><link>https://arxiv.org/abs/2601.07281</link><description>arXiv:2601.07281v1 Announce Type: cross 
Abstract: Decision trees are powerful machine learning algorithms, widely used in fields such as economics and medicine for their simplicity and interpretability. However, decision trees such as CART are prone to overfitting, especially when grown deep or the sample size is small. Conventional methods to reduce overfitting include pre-pruning and post-pruning, which constrain the growth of uninformative branches. In this paper, we propose a complementary approach by introducing a covariance-driven splitting criterion for regression trees (CovRT). This method is more robust to overfitting than the empirical risk minimization criterion used in CART, as it produces more balanced and stable splits and more effectively identifies covariates with true signals. We establish an oracle inequality of CovRT and prove that its predictive accuracy is comparable to that of CART in high-dimensional settings. We find that CovRT achieves superior prediction accuracy compared to CART in both simulations and real-world tasks.</description><author>cs.LG updates on arXiv.org</author><pubDate>Tue, 13 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2601.07281v1</guid></item><item><title>[arXiv-ML] Learning About Learning: A Physics Path from Spin Glasses to Artificial Intelligence</title><link>https://arxiv.org/abs/2601.07635</link><description>arXiv:2601.07635v1 Announce Type: cross 
Abstract: The Hopfield model, originally inspired by spin-glass physics, occupies a central place at the intersection of statistical mechanics, neural networks, and modern artificial intelligence. Despite its conceptual simplicity and broad applicability -- from associative memory to near-optimal solutions of combinatorial optimization problems -- it is rarely integrated into standard undergraduate physics curricula. In this paper, we present the Hopfield model as a pedagogically rich framework that naturally unifies core topics from undergraduate statistical physics, dynamical systems, linear algebra, and computational methods. We provide a concise and illustrated theoretical introduction grounded in familiar physics concepts, analyze the model's energy function, dynamics, and pattern stability, and discuss practical aspects of simulation, including a freely available simulation code. To support instruction, we conclude with classroom-ready example problems designed to mirror research practice. By explicitly connecting fundamental physics to contemporary AI applications, this work aims to help prepare physics students to understand, apply, and critically engage with the computational tools increasingly central to research, industry, and society.</description><author>cs.LG updates on arXiv.org</author><pubDate>Tue, 13 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2601.07635v1</guid></item><item><title>[arXiv-ML] Learning to accelerate Krasnosel'skii-Mann fixed-point iterations with guarantees</title><link>https://arxiv.org/abs/2601.07665</link><description>arXiv:2601.07665v1 Announce Type: cross 
Abstract: We introduce a principled learning to optimize (L2O) framework for solving fixed-point problems involving general nonexpansive mappings. Our idea is to deliberately inject summable perturbations into a standard Krasnosel'skii-Mann iteration to improve its average-case performance over a specific distribution of problems while retaining its convergence guarantees. Under a metric sub-regularity assumption, we prove that the proposed parametrization includes only iterations that locally achieve linear convergence-up to a vanishing bias term-and that it encompasses all iterations that do so at a sufficiently fast rate. We then demonstrate how our framework can be used to augment several widely-used operator splitting methods to accelerate the solution of structured monotone inclusion problems, and validate our approach on a best approximation problem using an L2O-augmented Douglas-Rachford splitting algorithm.</description><author>cs.LG updates on arXiv.org</author><pubDate>Tue, 13 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2601.07665v1</guid></item><item><title>[arXiv-ML] Failure-Aware RL: Reliable Offline-to-Online Reinforcement Learning with Self-Recovery for Real-World Manipulation</title><link>https://arxiv.org/abs/2601.07821</link><description>arXiv:2601.07821v1 Announce Type: cross 
Abstract: Post-training algorithms based on deep reinforcement learning can push the limits of robotic models for specific objectives, such as generalizability, accuracy, and robustness. However, Intervention-requiring Failures (IR Failures) (e.g., a robot spilling water or breaking fragile glass) during real-world exploration happen inevitably, hindering the practical deployment of such a paradigm. To tackle this, we introduce Failure-Aware Offline-to-Online Reinforcement Learning (FARL), a new paradigm minimizing failures during real-world reinforcement learning. We create FailureBench, a benchmark that incorporates common failure scenarios requiring human intervention, and propose an algorithm that integrates a world-model-based safety critic and a recovery policy trained offline to prevent failures during online exploration. Extensive simulation and real-world experiments demonstrate the effectiveness of FARL in significantly reducing IR Failures while improving performance and generalization during online reinforcement learning post-training. FARL reduces IR Failures by 73.1% while elevating performance by 11.3% on average during real-world RL post-training. Videos and code are available at https://failure-aware-rl.github.io.</description><author>cs.LG updates on arXiv.org</author><pubDate>Tue, 13 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2601.07821v1</guid></item><item><title>[arXiv-ML] Prophet as a Reproducible Forecasting Framework: A Methodological Guide for Business and Financial Analytics</title><link>https://arxiv.org/abs/2601.05929</link><description>arXiv:2601.05929v2 Announce Type: replace 
Abstract: Reproducibility remains a persistent challenge in forecasting research and practice, particularly in business and financial analytics, where forecasts inform high-stakes decisions. Traditional forecasting methods, while theoretically interpretable, often require extensive manual tuning and are difficult to replicate in proprietary environments. Machine learning approaches offer predictive flexibility but introduce challenges related to interpretability, stochastic training procedures, and cross-environment reproducibility. This paper examines Prophet, an open-source forecasting framework developed by Meta, as a reproducibility-enabling solution that balances interpretability, standardized workflows, and accessibility. Rather than proposing a new algorithm, this study evaluates how Prophet's additive structure, open-source implementation, and standardized workflow contribute to transparent and replicable forecasting practice. Using publicly available financial and retail datasets, we compare the performance and interpretability of Prophet with multiple ARIMA specifications (auto-selected, manually specified, and seasonal variants) and Random Forest, under a controlled and fully documented experimental design. This multi-model comparison provides a robust assessment of Prophet's relative performance and reproducibility advantages. Through concrete Python examples, we demonstrate how Prophet facilitates efficient forecasting workflows and integration with analytical pipelines. The study positions Prophet within the broader context of reproducible research. It highlights Prophet's role as a methodological building block that supports verification, auditability, and methodological rigor. This work provides researchers and practitioners with a practical reference framework for reproducible forecasting in Python-based research workflows.</description><author>cs.LG updates on arXiv.org</author><pubDate>Tue, 13 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2601.05929v2</guid></item><item><title>[BSPC] Depth-conditioned adversarial learning with dual discriminators for polyp segmentation</title><link>https://www.sciencedirect.com/science/article/pii/S1746809426001412?dgcid=rss_sd_all</link><description>&lt;p&gt;Publication date: 1 May 2026&lt;/p&gt;&lt;p&gt;&lt;b&gt;Source:&lt;/b&gt; Biomedical Signal Processing and Control, Volume 116&lt;/p&gt;&lt;p&gt;Author(s): Haiyu Wang, Jianan Zhang, Xueyu Liu, Yongfei Wu, Di Huang&lt;/p&gt;</description><author>ScienceDirect Publication: Biomedical Signal Processing and Control</author><pubDate>Mon, 12 Jan 2026 18:39:30 GMT</pubDate><guid isPermaLink="true">https://www.sciencedirect.com/science/article/pii/S1746809426001412</guid></item><item><title>[arXiv-CV] Prompt-Free SAM-Based Multi-Task Framework for Breast Ultrasound Lesion Segmentation and Classification</title><link>https://arxiv.org/abs/2601.05498</link><description>arXiv:2601.05498v1 Announce Type: new 
Abstract: Accurate tumor segmentation and classification in breast ultrasound (BUS) imaging remain challenging due to low contrast, speckle noise, and diverse lesion morphology. This study presents a multi-task deep learning framework that jointly performs lesion segmentation and diagnostic classification using embeddings from the Segment Anything Model (SAM) vision encoder. Unlike prompt-based SAM variants, our approach employs a prompt-free, fully supervised adaptation where high-dimensional SAM features are decoded through either a lightweight convolutional head or a UNet-inspired decoder for pixel-wise segmentation. The classification branch is enhanced via mask-guided attention, allowing the model to focus on lesion-relevant features while suppressing background artifacts. Experiments on the PRECISE 2025 breast ultrasound dataset, split per class into 80 percent training and 20 percent testing, show that the proposed method achieves a Dice Similarity Coefficient (DSC) of 0.887 and an accuracy of 92.3 percent, ranking among the top entries on the PRECISE challenge leaderboard. These results demonstrate that SAM-based representations, when coupled with segmentation-guided learning, significantly improve both lesion delineation and diagnostic prediction in breast ultrasound imaging.</description><author>cs.CV updates on arXiv.org</author><pubDate>Mon, 12 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2601.05498v1</guid></item><item><title>[arXiv-CV] Bidirectional Channel-selective Semantic Interaction for Semi-Supervised Medical Segmentation</title><link>https://arxiv.org/abs/2601.05855</link><description>arXiv:2601.05855v1 Announce Type: new 
Abstract: Semi-supervised medical image segmentation is an effective method for addressing scenarios with limited labeled data. Existing methods mainly rely on frameworks such as mean teacher and dual-stream consistency learning. These approaches often face issues like error accumulation and model structural complexity, while also neglecting the interaction between labeled and unlabeled data streams. To overcome these challenges, we propose a Bidirectional Channel-selective Semantic Interaction~(BCSI) framework for semi-supervised medical image segmentation. First, we propose a Semantic-Spatial Perturbation~(SSP) mechanism, which disturbs the data using two strong augmentation operations and leverages unsupervised learning with pseudo-labels from weak augmentations. Additionally, we employ consistency on the predictions from the two strong augmentations to further improve model stability and robustness. Second, to reduce noise during the interaction between labeled and unlabeled data, we propose a Channel-selective Router~(CR) component, which dynamically selects the most relevant channels for information exchange. This mechanism ensures that only highly relevant features are activated, minimizing unnecessary interference. Finally, the Bidirectional Channel-wise Interaction~(BCI) strategy is employed to supplement additional semantic information and enhance the representation of important channels. Experimental results on multiple benchmarking 3D medical datasets demonstrate that the proposed method outperforms existing semi-supervised approaches.</description><author>cs.CV updates on arXiv.org</author><pubDate>Mon, 12 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2601.05855v1</guid></item><item><title>[arXiv-ML] Tensor-DTI: Enhancing Biomolecular Interaction Prediction with Contrastive Embedding Learning</title><link>https://arxiv.org/abs/2601.05792</link><description>arXiv:2601.05792v1 Announce Type: new 
Abstract: Accurate drug-target interaction (DTI) prediction is essential for computational drug discovery, yet existing models often rely on single-modality predefined molecular descriptors or sequence-based embeddings with limited representativeness. We propose Tensor-DTI, a contrastive learning framework that integrates multimodal embeddings from molecular graphs, protein language models, and binding-site predictions to improve interaction modeling. Tensor-DTI employs a siamese dual-encoder architecture, enabling it to capture both chemical and structural interaction features while distinguishing interacting from non-interacting pairs. Evaluations on multiple DTI benchmarks demonstrate that Tensor-DTI outperforms existing sequence-based and graph-based models. We also conduct large-scale inference experiments on CDK2 across billion-scale chemical libraries, where Tensor-DTI produces chemically plausible hit distributions even when CDK2 is withheld from training. In enrichment studies against Glide docking and Boltz-2 co-folder, Tensor-DTI remains competitive on CDK2 and improves the screening budget required to recover moderate fractions of high-affinity ligands on out-of-family targets under strict family-holdout splits. Additionally, we explore its applicability to protein-RNA and peptide-protein interactions. Our findings highlight the benefits of integrating multimodal information with contrastive objectives to enhance interaction-prediction accuracy and to provide more interpretable and reliability-aware models for virtual screening.</description><author>cs.LG updates on arXiv.org</author><pubDate>Mon, 12 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2601.05792v1</guid></item><item><title>[arXiv-ML] Fusion Matters: Length-Aware Analysis of Positional-Encoding Fusion in Transformers</title><link>https://arxiv.org/abs/2601.05807</link><description>arXiv:2601.05807v1 Announce Type: new 
Abstract: Transformers require positional encodings to represent sequence order, yet most prior work focuses on designing new positional encodings rather than examining how positional information is fused with token embeddings. In this paper, we study whether the fusion mechanism itself affects performance, particularly in long-sequence settings. We conduct a controlled empirical study comparing three canonical fusion strategies--element-wise addition, concatenation with projection, and scalar gated fusion--under identical Transformer architectures, data splits, and random seeds. Experiments on three text classification datasets spanning short (AG News), medium (IMDB), and long (ArXiv) sequences show that fusion choice has negligible impact on short texts but produces consistent gains on long documents. To verify that these gains are structural rather than stochastic, we perform paired-seed analysis and cross-dataset comparison across sequence-length regimes. Additional experiments on the ArXiv dataset indicate that the benefit of learnable fusion generalizes across multiple positional encoding families. Finally, we explore a lightweight convolutional gating mechanism that introduces local inductive bias at the fusion level, evaluated on long documents only. Our results indicate that positional-encoding fusion is a non-trivial design choice for long-sequence Transformers and should be treated as an explicit modeling decision rather than a fixed default.</description><author>cs.LG updates on arXiv.org</author><pubDate>Mon, 12 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2601.05807v1</guid></item><item><title>[arXiv-ML] Prophet as a Repro ducible Forecasting Framework: A Methodological Guide for Business and Financial Analytics</title><link>https://arxiv.org/abs/2601.05929</link><description>arXiv:2601.05929v1 Announce Type: new 
Abstract: Reproducibility remains a persistent challenge in forecasting research and practice, particularly in business and financial analytics where forecasts inform high-stakes decisions. Traditional forecasting methods, while theoretically interpretable, often require extensive manual tuning and are difficult to replicate in proprietary environments. Machine learning approaches offer predictive flexibility but introduce challenges related to interpretability, stochastic training procedures, and cross-environment reproducibility. This paper examines Prophet, an open-source forecasting framework developed by Meta, as a reproducibility-enabling solution that balances interpretability, standardized workflows, and accessibility. Rather than proposing a new algorithm, this study evaluates how Prophet's additive structure, open-source implementation, and standardized workflow contribute to transparent and replicable forecasting practice. Using publicly available financial and retail datasets, we compare Prophet's performance and interpretability with multiple ARIMA specifications (auto-selected, manually specified, and seasonal variants) and Random Forest under a controlled and fully documented experimental design. This multi-model comparison provides a robust assessment of Prophet's relative performance and reproducibility advantages. Through concrete Python examples, we demonstrate how Prophet facilitates efficient forecasting workflows and integration with analytical pipelines. The study positions Prophet within the broader context of reproducible research. It highlights Prophet's role as a methodological building block that supports verification, auditability, and methodological rigor. This work provides researchers and practitioners with a practical reference framework for reproducible forecasting in Python-based research workflows.</description><author>cs.LG updates on arXiv.org</author><pubDate>Mon, 12 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2601.05929v1</guid></item><item><title>[arXiv-ML] SCOPE: Sequential Causal Optimization of Process Interventions</title><link>https://arxiv.org/abs/2512.17629</link><description>arXiv:2512.17629v3 Announce Type: replace 
Abstract: Prescriptive Process Monitoring (PresPM) recommends interventions during business processes to optimize key performance indicators (KPIs). In realistic settings, interventions are rarely isolated: organizations need to align sequences of interventions to jointly steer the outcome of a case. Existing PresPM approaches fall short in this respect. Many focus on a single intervention decision, while others treat multiple interventions independently, ignoring how they interact over time. Methods that do address these dependencies depend either on simulation or data augmentation to approximate the process to train a Reinforcement Learning (RL) agent, which can create a reality gap and introduce bias. We introduce SCOPE, a PresPM approach that learns aligned sequential intervention recommendations. SCOPE employs backward induction to estimate the effect of each candidate intervention action, propagating its impact from the final decision point back to the first. By leveraging causal learners, our method can utilize observational data directly, unlike methods that require constructing process approximations for reinforcement learning. Experiments on both an existing synthetic dataset and a new semi-synthetic dataset show that SCOPE consistently outperforms state-of-the-art PresPM techniques in optimizing the KPI. The novel semi-synthetic setup, based on a real-life event log, is provided as a reusable benchmark for future work on sequential PresPM.</description><author>cs.LG updates on arXiv.org</author><pubDate>Mon, 12 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2512.17629v3</guid></item><item><title>[BSPC] MACNet:Multiscale attention cross-sharing networks for colorectal polyp segmentation</title><link>https://www.sciencedirect.com/science/article/pii/S1746809426000042?dgcid=rss_sd_all</link><description>&lt;p&gt;Publication date: 1 May 2026&lt;/p&gt;&lt;p&gt;&lt;b&gt;Source:&lt;/b&gt; Biomedical Signal Processing and Control, Volume 116&lt;/p&gt;&lt;p&gt;Author(s): Xinhui Jiang, Chunmiao Wei, Xiaolin Li, Zhicheng Dai&lt;/p&gt;</description><author>ScienceDirect Publication: Biomedical Signal Processing and Control</author><pubDate>Sun, 11 Jan 2026 18:35:03 GMT</pubDate><guid isPermaLink="true">https://www.sciencedirect.com/science/article/pii/S1746809426000042</guid></item><item><title>[BSPC] CSA-Net: Class self-attention based feature extraction network for polyp segmentation</title><link>https://www.sciencedirect.com/science/article/pii/S174680942600042X?dgcid=rss_sd_all</link><description>&lt;p&gt;Publication date: 1 May 2026&lt;/p&gt;&lt;p&gt;&lt;b&gt;Source:&lt;/b&gt; Biomedical Signal Processing and Control, Volume 116&lt;/p&gt;&lt;p&gt;Author(s): Xuehu Wang, Xiangqian Liu, Jin Lu, Yuhao Wang, Chao Xue, Han Yu, Yongchang Zheng, Chen Geng, Chengwei Guo, Xiaoping Yin&lt;/p&gt;</description><author>ScienceDirect Publication: Biomedical Signal Processing and Control</author><pubDate>Sun, 11 Jan 2026 18:35:03 GMT</pubDate><guid isPermaLink="true">https://www.sciencedirect.com/science/article/pii/S174680942600042X</guid></item><item><title>[BSPC] EGNet: A boundary-region closed-loop network for medical image segmentation with fuzzy lesions</title><link>https://www.sciencedirect.com/science/article/pii/S1746809426001564?dgcid=rss_sd_all</link><description>&lt;p&gt;Publication date: 1 May 2026&lt;/p&gt;&lt;p&gt;&lt;b&gt;Source:&lt;/b&gt; Biomedical Signal Processing and Control, Volume 116&lt;/p&gt;&lt;p&gt;Author(s): Shuwen Wu, Qilong Li, Ruixue Xia, Chongsheng Zhang&lt;/p&gt;</description><author>ScienceDirect Publication: Biomedical Signal Processing and Control</author><pubDate>Sun, 11 Jan 2026 18:35:03 GMT</pubDate><guid isPermaLink="true">https://www.sciencedirect.com/science/article/pii/S1746809426001564</guid></item><item><title>[BSPC] Hawk-Net: Medical image segmentation and classification using multi-scale convolutional self-attention-based image processor with DK-CNN-Mamba-xAttention Fusion Network</title><link>https://www.sciencedirect.com/science/article/pii/S1746809425019123?dgcid=rss_sd_all</link><description>&lt;p&gt;Publication date: 15 April 2026&lt;/p&gt;&lt;p&gt;&lt;b&gt;Source:&lt;/b&gt; Biomedical Signal Processing and Control, Volume 115&lt;/p&gt;&lt;p&gt;Author(s): Mahmudul Hasan, Md. Kamrul Hasan&lt;/p&gt;</description><author>ScienceDirect Publication: Biomedical Signal Processing and Control</author><pubDate>Sun, 11 Jan 2026 13:09:00 GMT</pubDate><guid isPermaLink="true">https://www.sciencedirect.com/science/article/pii/S1746809425019123</guid></item><item><title>[BSPC] EFFM: Rethinking feature fusion mechanisms for medical image segmentation</title><link>https://www.sciencedirect.com/science/article/pii/S1746809425018452?dgcid=rss_sd_all</link><description>&lt;p&gt;Publication date: 15 April 2026&lt;/p&gt;&lt;p&gt;&lt;b&gt;Source:&lt;/b&gt; Biomedical Signal Processing and Control, Volume 115&lt;/p&gt;&lt;p&gt;Author(s): Xiaoyan Zhang, Zheng Zhao, Yongqin Zhang, Chunlin Yu, Xiangfu Meng, Shuai Li&lt;/p&gt;</description><author>ScienceDirect Publication: Biomedical Signal Processing and Control</author><pubDate>Sun, 11 Jan 2026 13:09:00 GMT</pubDate><guid isPermaLink="true">https://www.sciencedirect.com/science/article/pii/S1746809425018452</guid></item><item><title>[BSPC] Applications of a novel deep neural network to the classification of liver steatosis and breast lesions in ultrasound images</title><link>https://www.sciencedirect.com/science/article/pii/S1746809425019068?dgcid=rss_sd_all</link><description>&lt;p&gt;Publication date: 15 April 2026&lt;/p&gt;&lt;p&gt;&lt;b&gt;Source:&lt;/b&gt; Biomedical Signal Processing and Control, Volume 115&lt;/p&gt;&lt;p&gt;Author(s): Özlem Polat, Zümray Dokur, Tamer Ölmez&lt;/p&gt;</description><author>ScienceDirect Publication: Biomedical Signal Processing and Control</author><pubDate>Sun, 11 Jan 2026 13:09:00 GMT</pubDate><guid isPermaLink="true">https://www.sciencedirect.com/science/article/pii/S1746809425019068</guid></item><item><title>[BSPC] Frequency matters: Dual Domain Consistency for Semi-supervised Skin Lesion Segmentation</title><link>https://www.sciencedirect.com/science/article/pii/S1746809425019421?dgcid=rss_sd_all</link><description>&lt;p&gt;Publication date: 15 April 2026&lt;/p&gt;&lt;p&gt;&lt;b&gt;Source:&lt;/b&gt; Biomedical Signal Processing and Control, Volume 115&lt;/p&gt;&lt;p&gt;Author(s): Haoran Xi, Shan Ling, Min He, Xiaolin Li&lt;/p&gt;</description><author>ScienceDirect Publication: Biomedical Signal Processing and Control</author><pubDate>Sun, 11 Jan 2026 13:09:00 GMT</pubDate><guid isPermaLink="true">https://www.sciencedirect.com/science/article/pii/S1746809425019421</guid></item><item><title>[BSPC] Flexible dilated convolution and bidirectional pyramid approach to polyp segmentation</title><link>https://www.sciencedirect.com/science/article/pii/S1746809425019202?dgcid=rss_sd_all</link><description>&lt;p&gt;Publication date: 15 April 2026&lt;/p&gt;&lt;p&gt;&lt;b&gt;Source:&lt;/b&gt; Biomedical Signal Processing and Control, Volume 115&lt;/p&gt;&lt;p&gt;Author(s): Yunfei Yin, Zumnan Timbi, Zheng Yuan, Sijing Xiong, MD Tanvir Islam, Argho Dey, Swachha Ray&lt;/p&gt;</description><author>ScienceDirect Publication: Biomedical Signal Processing and Control</author><pubDate>Sun, 11 Jan 2026 13:09:00 GMT</pubDate><guid isPermaLink="true">https://www.sciencedirect.com/science/article/pii/S1746809425019202</guid></item><item><title>[BSPC] Enhanced diagnosis of skin lesions through torsional wave propagation and probabilistic inverse problem algorithms: An experimental study</title><link>https://www.sciencedirect.com/science/article/pii/S174680942501907X?dgcid=rss_sd_all</link><description>&lt;p&gt;Publication date: 15 April 2026&lt;/p&gt;&lt;p&gt;&lt;b&gt;Source:&lt;/b&gt; Biomedical Signal Processing and Control, Volume 115&lt;/p&gt;&lt;p&gt;Author(s): Yousef Almashakbeh, Hirad Shamimi, Inas H Faris, J.L. Martín-Rodríguez, Antonio Callejas, Guillermo Rus&lt;/p&gt;</description><author>ScienceDirect Publication: Biomedical Signal Processing and Control</author><pubDate>Sun, 11 Jan 2026 13:09:00 GMT</pubDate><guid isPermaLink="true">https://www.sciencedirect.com/science/article/pii/S174680942501907X</guid></item><item><title>[BSPC] DiffMoE-UNet: A differential transformer with Mixture-of-Experts for accurate medical image segmentation</title><link>https://www.sciencedirect.com/science/article/pii/S174680942501924X?dgcid=rss_sd_all</link><description>&lt;p&gt;Publication date: 15 April 2026&lt;/p&gt;&lt;p&gt;&lt;b&gt;Source:&lt;/b&gt; Biomedical Signal Processing and Control, Volume 115&lt;/p&gt;&lt;p&gt;Author(s): Jaouad Tagnamas, Hiba Ramadan, Ali Yahyaouy, Hamid Tairi&lt;/p&gt;</description><author>ScienceDirect Publication: Biomedical Signal Processing and Control</author><pubDate>Sun, 11 Jan 2026 13:09:00 GMT</pubDate><guid isPermaLink="true">https://www.sciencedirect.com/science/article/pii/S174680942501924X</guid></item><item><title>[BSPC] Uncertainty-weighted feature alignment and information interaction network for multi-scale medical image segmentation</title><link>https://www.sciencedirect.com/science/article/pii/S174680942501910X?dgcid=rss_sd_all</link><description>&lt;p&gt;Publication date: 15 April 2026&lt;/p&gt;&lt;p&gt;&lt;b&gt;Source:&lt;/b&gt; Biomedical Signal Processing and Control, Volume 115&lt;/p&gt;&lt;p&gt;Author(s): Min Zhang, Junxia Wang, Junkai Wang, Yuanjie Zheng&lt;/p&gt;</description><author>ScienceDirect Publication: Biomedical Signal Processing and Control</author><pubDate>Sun, 11 Jan 2026 13:09:00 GMT</pubDate><guid isPermaLink="true">https://www.sciencedirect.com/science/article/pii/S174680942501910X</guid></item><item><title>[BSPC] DisFreAda: Distribution-driven multi-frequency adaptive network for generalizable medical image segmentation</title><link>https://www.sciencedirect.com/science/article/pii/S1746809426000236?dgcid=rss_sd_all</link><description>&lt;p&gt;Publication date: 1 May 2026&lt;/p&gt;&lt;p&gt;&lt;b&gt;Source:&lt;/b&gt; Biomedical Signal Processing and Control, Volume 116&lt;/p&gt;&lt;p&gt;Author(s): Minjun Wang, Houjin Chen, Yanfeng Li, Jia Sun, Luyifu Chen, Peng Liang&lt;/p&gt;</description><author>ScienceDirect Publication: Biomedical Signal Processing and Control</author><pubDate>Sun, 11 Jan 2026 13:09:00 GMT</pubDate><guid isPermaLink="true">https://www.sciencedirect.com/science/article/pii/S1746809426000236</guid></item><item><title>[BSPC] Morphology-enhanced CAM-guided SAM for weakly supervised breast lesion segmentation</title><link>https://www.sciencedirect.com/science/article/pii/S1746809426000637?dgcid=rss_sd_all</link><description>&lt;p&gt;Publication date: 1 May 2026&lt;/p&gt;&lt;p&gt;&lt;b&gt;Source:&lt;/b&gt; Biomedical Signal Processing and Control, Volume 116&lt;/p&gt;&lt;p&gt;Author(s): Xin Yue, Qing Zhao, Xiaoling Liu, Jianqiang Li, Jing Bai, Changwei Song, Suqin Liu, Rodrigo Moreno, Zhikai Yang, Stefano E. Romero, Gabriel Jimenez, Guanghui Fu&lt;/p&gt;</description><author>ScienceDirect Publication: Biomedical Signal Processing and Control</author><pubDate>Sun, 11 Jan 2026 13:09:00 GMT</pubDate><guid isPermaLink="true">https://www.sciencedirect.com/science/article/pii/S1746809426000637</guid></item><item><title>[BSPC] Medical image segmentation based on 3D PDC with Swin Transformer</title><link>https://www.sciencedirect.com/science/article/pii/S1746809426000704?dgcid=rss_sd_all</link><description>&lt;p&gt;Publication date: 1 May 2026&lt;/p&gt;&lt;p&gt;&lt;b&gt;Source:&lt;/b&gt; Biomedical Signal Processing and Control, Volume 116&lt;/p&gt;&lt;p&gt;Author(s): Lin Fan, Xiaojia Ding, Zhongmin Wang, Hai Wang, Rong Zhang&lt;/p&gt;</description><author>ScienceDirect Publication: Biomedical Signal Processing and Control</author><pubDate>Sun, 11 Jan 2026 13:09:00 GMT</pubDate><guid isPermaLink="true">https://www.sciencedirect.com/science/article/pii/S1746809426000704</guid></item><item><title>[EAAI] Cross-Granularity Fusion Vision Mamba UNet for medical image segmentation</title><link>https://www.sciencedirect.com/science/article/pii/S0952197626000679?dgcid=rss_sd_all</link><description>&lt;p&gt;Publication date: 1 March 2026&lt;/p&gt;&lt;p&gt;&lt;b&gt;Source:&lt;/b&gt; Engineering Applications of Artificial Intelligence, Volume 167, Part 1&lt;/p&gt;&lt;p&gt;Author(s): Tuersunjiang Baidi, Zitong Ren, Kurban Ubul, Alimjan Aysa, Boyuan Li, Shihao Wang&lt;/p&gt;</description><author>ScienceDirect Publication: Engineering Applications of Artificial Intelligence</author><pubDate>Sun, 11 Jan 2026 13:09:00 GMT</pubDate><guid isPermaLink="true">https://www.sciencedirect.com/science/article/pii/S0952197626000679</guid></item><item><title>[AIM] Siamese evolutionary masking: Enhancing the generalization of self-supervised medical image segmentation model</title><link>https://www.sciencedirect.com/science/article/pii/S0933365726000011?dgcid=rss_sd_all</link><description>&lt;p&gt;Publication date: Available online 7 January 2026&lt;/p&gt;&lt;p&gt;&lt;b&gt;Source:&lt;/b&gt; Artificial Intelligence in Medicine&lt;/p&gt;&lt;p&gt;Author(s): Yichen Zhi, Hongxia Bie, Jiali Wang, Zhao Jing&lt;/p&gt;</description><author>ScienceDirect Publication: Artificial Intelligence in Medicine</author><pubDate>Sun, 11 Jan 2026 13:09:00 GMT</pubDate><guid isPermaLink="true">https://www.sciencedirect.com/science/article/pii/S0933365726000011</guid></item><item><title>[NC] UP2D: Uncertainty-aware progressive pseudo-label denoising for source-free domain adaptive medical image segmentation</title><link>https://www.sciencedirect.com/science/article/pii/S0925231226000561?dgcid=rss_sd_all</link><description>&lt;p&gt;Publication date: Available online 9 January 2026&lt;/p&gt;&lt;p&gt;&lt;b&gt;Source:&lt;/b&gt; Neurocomputing&lt;/p&gt;&lt;p&gt;Author(s): Thanh-Huy Nguyen, Quang-Khai Bui-Tran, Manh D. Ho, Thinh B. Lam, Vi Vu, Hoang-Thien Nguyen, Phat Huynh, Ulas Bagci&lt;/p&gt;</description><author>ScienceDirect Publication: Neurocomputing</author><pubDate>Sun, 11 Jan 2026 13:08:59 GMT</pubDate><guid isPermaLink="true">https://www.sciencedirect.com/science/article/pii/S0925231226000561</guid></item><item><title>[KBS] BACFormer: A robust boundary-aware transformer for medical image segmentation</title><link>https://www.sciencedirect.com/science/article/pii/S0950705125022439?dgcid=rss_sd_all</link><description>&lt;p&gt;Publication date: 28 February 2026&lt;/p&gt;&lt;p&gt;&lt;b&gt;Source:&lt;/b&gt; Knowledge-Based Systems, Volume 335&lt;/p&gt;&lt;p&gt;Author(s): Zhiyong Huang, Mingyu Wang, Mingyang Hou, Zhi Yu, Shiwei Wang, Xiaoyu Li, Jiahong Wang, Yan Yan, Yushi Liu&lt;/p&gt;</description><author>ScienceDirect Publication: Knowledge-Based Systems</author><pubDate>Sun, 11 Jan 2026 13:08:59 GMT</pubDate><guid isPermaLink="true">https://www.sciencedirect.com/science/article/pii/S0950705125022439</guid></item><item><title>[KBS] Multimodality-based framework for enhanced skin lesion recognition over federated learning</title><link>https://www.sciencedirect.com/science/article/pii/S0950705125021859?dgcid=rss_sd_all</link><description>&lt;p&gt;Publication date: 28 February 2026&lt;/p&gt;&lt;p&gt;&lt;b&gt;Source:&lt;/b&gt; Knowledge-Based Systems, Volume 335&lt;/p&gt;&lt;p&gt;Author(s): Abdul Hai Karimi, Taimoor Khan, Chang Choi&lt;/p&gt;</description><author>ScienceDirect Publication: Knowledge-Based Systems</author><pubDate>Sun, 11 Jan 2026 13:08:59 GMT</pubDate><guid isPermaLink="true">https://www.sciencedirect.com/science/article/pii/S0950705125021859</guid></item><item><title>[ESWA] WBDM-ECRF: A bridge diffusion model with efficient conditional random field for skin lesion segmentation</title><link>https://www.sciencedirect.com/science/article/pii/S0957417425046159?dgcid=rss_sd_all</link><description>&lt;p&gt;Publication date: 25 April 2026&lt;/p&gt;&lt;p&gt;&lt;b&gt;Source:&lt;/b&gt; Expert Systems with Applications, Volume 307&lt;/p&gt;&lt;p&gt;Author(s): Hefeng Ji, Jing Xiao, Jimin Liu, Haoyong Yu, Chao Chen&lt;/p&gt;</description><author>ScienceDirect Publication: Expert Systems with Applications</author><pubDate>Sun, 11 Jan 2026 13:08:59 GMT</pubDate><guid isPermaLink="true">https://www.sciencedirect.com/science/article/pii/S0957417425046159</guid></item><item><title>[ESWA] MED-Net: Leveraging multi-visual encoding and Manhattan-distance pooling for robust medical image segmentation</title><link>https://www.sciencedirect.com/science/article/pii/S0957417426000175?dgcid=rss_sd_all</link><description>&lt;p&gt;Publication date: 1 May 2026&lt;/p&gt;&lt;p&gt;&lt;b&gt;Source:&lt;/b&gt; Expert Systems with Applications, Volume 308&lt;/p&gt;&lt;p&gt;Author(s): Zhaozhao Su, Yuxuan Luo, Shiji Song, Zhiqiang Zhu, Jiajie Wang, Zhihua Fang, Bowen Wang, Liejun Wang&lt;/p&gt;</description><author>ScienceDirect Publication: Expert Systems with Applications</author><pubDate>Sun, 11 Jan 2026 13:08:59 GMT</pubDate><guid isPermaLink="true">https://www.sciencedirect.com/science/article/pii/S0957417426000175</guid></item><item><title>[ESWA] A colon polyp segmentation network via collaborative decision-making of mixture of experts</title><link>https://www.sciencedirect.com/science/article/pii/S095741742600031X?dgcid=rss_sd_all</link><description>&lt;p&gt;Publication date: 1 May 2026&lt;/p&gt;&lt;p&gt;&lt;b&gt;Source:&lt;/b&gt; Expert Systems with Applications, Volume 308&lt;/p&gt;&lt;p&gt;Author(s): Wenchao Zhang, Wenhui Ye, Zhenhua Yu, Jianguo Ju&lt;/p&gt;</description><author>ScienceDirect Publication: Expert Systems with Applications</author><pubDate>Sun, 11 Jan 2026 13:08:59 GMT</pubDate><guid isPermaLink="true">https://www.sciencedirect.com/science/article/pii/S095741742600031X</guid></item><item><title>[PR] MFSF&lt;sup&gt;2&lt;/sup&gt;-NET: towards improved skin lesion diagnosis via multi-scale and multi-frequency-domain fusion</title><link>https://www.sciencedirect.com/science/article/pii/S003132032501581X?dgcid=rss_sd_all</link><description>&lt;p&gt;Publication date: June 2026&lt;/p&gt;&lt;p&gt;&lt;b&gt;Source:&lt;/b&gt; Pattern Recognition, Volume 174&lt;/p&gt;&lt;p&gt;Author(s): Hui Liu, Yibo Dou, Meng Cao, Kai Wang, Yunmin Zou, Guoxiang Ma, Chaohui Li&lt;/p&gt;</description><author>ScienceDirect Publication: Pattern Recognition</author><pubDate>Sun, 11 Jan 2026 13:08:58 GMT</pubDate><guid isPermaLink="true">https://www.sciencedirect.com/science/article/pii/S003132032501581X</guid></item><item><title>[PR] Efficient breast cancer segmentation via Brownian Bridge diffusion with semantic fusion strategy</title><link>https://www.sciencedirect.com/science/article/pii/S0031320325016565?dgcid=rss_sd_all</link><description>&lt;p&gt;Publication date: June 2026&lt;/p&gt;&lt;p&gt;&lt;b&gt;Source:&lt;/b&gt; Pattern Recognition, Volume 174&lt;/p&gt;&lt;p&gt;Author(s): Feiyan Feng, Tianyu Liu, Fulin Zheng, Yanshen Sun, Hong Wang&lt;/p&gt;</description><author>ScienceDirect Publication: Pattern Recognition</author><pubDate>Sun, 11 Jan 2026 13:08:58 GMT</pubDate><guid isPermaLink="true">https://www.sciencedirect.com/science/article/pii/S0031320325016565</guid></item><item><title>[PR] Empowering 2D neural network for 3D medical image segmentation via neighborhood information fusion</title><link>https://www.sciencedirect.com/science/article/pii/S003132032501698X?dgcid=rss_sd_all</link><description>&lt;p&gt;Publication date: July 2026&lt;/p&gt;&lt;p&gt;&lt;b&gt;Source:&lt;/b&gt; Pattern Recognition, Volume 175&lt;/p&gt;&lt;p&gt;Author(s): Qiankun Li, Xiaolong Huang, Yani Zhang, Bo Fang, Duo Hong, Junxin Chen&lt;/p&gt;</description><author>ScienceDirect Publication: Pattern Recognition</author><pubDate>Sun, 11 Jan 2026 13:08:58 GMT</pubDate><guid isPermaLink="true">https://www.sciencedirect.com/science/article/pii/S003132032501698X</guid></item><item><title>[MedIA] SicTTA: Single image continual test time adaptation for medical image segmentation</title><link>https://www.sciencedirect.com/science/article/pii/S1361841525004050?dgcid=rss_sd_all</link><description>&lt;p&gt;Publication date: February 2026&lt;/p&gt;&lt;p&gt;&lt;b&gt;Source:&lt;/b&gt; Medical Image Analysis, Volume 108&lt;/p&gt;&lt;p&gt;Author(s): Jianghao Wu, Xinya Liu, Guotai Wang, Shaoting Zhang&lt;/p&gt;</description><author>ScienceDirect Publication: Medical Image Analysis</author><pubDate>Sun, 11 Jan 2026 13:08:58 GMT</pubDate><guid isPermaLink="true">https://www.sciencedirect.com/science/article/pii/S1361841525004050</guid></item><item><title>[MedIA] Adaptive mix for semi-supervised medical image segmentation</title><link>https://www.sciencedirect.com/science/article/pii/S1361841525004037?dgcid=rss_sd_all</link><description>&lt;p&gt;Publication date: February 2026&lt;/p&gt;&lt;p&gt;&lt;b&gt;Source:&lt;/b&gt; Medical Image Analysis, Volume 108&lt;/p&gt;&lt;p&gt;Author(s): Zhiqiang Shen, Peng Cao, Junming Su, Jinzhu Yang, Osmar R. Zaiane&lt;/p&gt;</description><author>ScienceDirect Publication: Medical Image Analysis</author><pubDate>Sun, 11 Jan 2026 13:08:58 GMT</pubDate><guid isPermaLink="true">https://www.sciencedirect.com/science/article/pii/S1361841525004037</guid></item><item><title>[MedIA] Test-time generative augmentation for medical image segmentation</title><link>https://www.sciencedirect.com/science/article/pii/S1361841525004487?dgcid=rss_sd_all</link><description>&lt;p&gt;Publication date: March 2026&lt;/p&gt;&lt;p&gt;&lt;b&gt;Source:&lt;/b&gt; Medical Image Analysis, Volume 109&lt;/p&gt;&lt;p&gt;Author(s): Xiao Ma, Yuhui Tao, Zetian Zhang, Yuhan Zhang, Xi Wang, Sheng Zhang, Zexuan Ji, Yizhe Zhang, Qiang Chen, Guang Yang&lt;/p&gt;</description><author>ScienceDirect Publication: Medical Image Analysis</author><pubDate>Sun, 11 Jan 2026 13:08:58 GMT</pubDate><guid isPermaLink="true">https://www.sciencedirect.com/science/article/pii/S1361841525004487</guid></item><item><title>[MedIA] CHAP: Channel-spatial hierarchical adversarial perturbation for semi-supervised medical image segmentation</title><link>https://www.sciencedirect.com/science/article/pii/S1361841525004645?dgcid=rss_sd_all</link><description>&lt;p&gt;Publication date: March 2026&lt;/p&gt;&lt;p&gt;&lt;b&gt;Source:&lt;/b&gt; Medical Image Analysis, Volume 109&lt;/p&gt;&lt;p&gt;Author(s): Si-Ping Zhou, Zhi-Fang Gong, Kai-Ni Wang, Ping Zhou, Yang Chen, Guang-Quan Zhou&lt;/p&gt;</description><author>ScienceDirect Publication: Medical Image Analysis</author><pubDate>Sun, 11 Jan 2026 13:08:58 GMT</pubDate><guid isPermaLink="true">https://www.sciencedirect.com/science/article/pii/S1361841525004645</guid></item><item><title>[TIP] SGNet: Style-Guided Network With Temporal Compensation for Unpaired Low-Light Colonoscopy Video Enhancement</title><link>http://ieeexplore.ieee.org/document/11306257</link><description>A low-light colonoscopy video enhancement method is needed as poor illumination in colonoscopy can hinder accurate disease diagnosis and adversely affect surgical procedures. Existing low-light video enhancement methods usually apply a frame-by-frame enhancement strategy without considering the temporal correlation between them, which often causes a flickering problem. In addition, most methods are designed for endoscopic devices with fixed imaging styles and cannot be easily adapted to different devices. In this paper, we propose a Style-Guided Network (SGNet) for unpaired Low-Light Colonoscopy Video Enhancement (LLCVE). Given that collecting content-consistent paired videos is difficult, SGNet adopts a CycleGAN-based framework to convert low-light videos to normal-light videos, in which a Temporal Compensation (TC) module and a Style Guidance (SG) module are proposed to alleviate the flickering problem and achieve flexible style transfer, respectively. The TC module compensates for a low-light frame by learning the correlated feature of its adjacent frames, thereby improving the temporal smoothness of the enhanced video. The SG module encodes the text of the imaging style and adaptively explores its intrinsic relationships with video features to obtain style representations, which are then used to guide the subsequent enhancement process. Extensive experiments on a curated database show that SGNet achieves promising performance on the LLCVE task, outperforming state-of-the-art methods in both quantitative metrics and visual quality.</description><author>IEEE Transactions on Image Processing - new TOC</author><pubDate>Mon, 22 Dec 2025 13:19:02 GMT</pubDate><guid isPermaLink="true">http://ieeexplore.ieee.org/document/11306257</guid></item><item><title>[MP] Uncertainty‐guided test‐time optimization for personalizing segmentation models in longitudinal medical imaging</title><link>https://aapm.onlinelibrary.wiley.com/doi/10.1002/mp.70206?af=R</link><description>Medical Physics, Volume 53, Issue 1, January 2026.</description><author>Wiley: Medical Physics: Table of Contents</author><pubDate>Mon, 22 Dec 2025 04:06:14 GMT</pubDate><guid isPermaLink="true">10.1002/mp.70206</guid></item><item><title>[MP] Accelerating vision foundation model for efficient medical image segmentation</title><link>https://aapm.onlinelibrary.wiley.com/doi/10.1002/mp.70193?af=R</link><description>Medical Physics, Volume 53, Issue 1, January 2026.</description><author>Wiley: Medical Physics: Table of Contents</author><pubDate>Sat, 20 Dec 2025 03:50:16 GMT</pubDate><guid isPermaLink="true">10.1002/mp.70193</guid></item><item><title>[TIP] Uncertainty-Guided Adaptive Correction for Semi-Supervised Medical Image Segmentation</title><link>http://ieeexplore.ieee.org/document/11273087</link><description>Consistent perturbation strategies have emerged as a dominant paradigm in semi-supervised medical image segmentation. Nevertheless, prevailing approaches inadequately address two critical challenges: 1) prediction errors induced by data uncertainty from distribution shifts, and 2) loss instability caused by model uncertainty in parameter generalization. To overcome these limitations, we propose an Uncertainty-Guided Adaptive Correction (UGAC) framework with three key innovations. First, we develop a dual-path uncertainty rectification mechanism that employs normalized entropy measures to detect error-prone regions in unlabeled predictions, followed by bilateral correction through confidence-weighted fusion. Second, we introduce adversarial consistency constraints that leverage labeled data to discriminate authentic segmentation patterns, effectively regularizing uncertainty propagation in unlabeled predictions through spectral normalization. Third, we architect a frequency-aware segmentation backbone through our novel Freqfusion module, which performs adaptive spectral decomposition during feature decoding to explicitly disentangle high-frequency (boundary-aware) and low-frequency (structural) components, thereby enhancing anatomical boundary sensitivity. Comprehensive evaluations on MM-WHS, BUSI, M&amp;amp;Ms and PROMISE12 datasets demonstrate UGAC’s superior performance. The proposed framework exhibits robust generalizability across CT, MRI, and ultrasound modalities, while achieving significantly lower computational complexity than baseline UNet implementations. The code will be available at https://github.com/SIGMACX/UGAC.</description><author>IEEE Transactions on Image Processing - new TOC</author><pubDate>Wed, 03 Dec 2025 13:18:43 GMT</pubDate><guid isPermaLink="true">http://ieeexplore.ieee.org/document/11273087</guid></item><item><title>[TPAMI] H2OT: Hierarchical Hourglass Tokenizer for Efficient Video Pose Transformers</title><link>http://ieeexplore.ieee.org/document/11155208</link><description>Transformers have been successfully applied in the field of video-based 3D human pose estimation. However, the high computational costs of these video pose transformers (VPTs) make them impractical on resource-constrained devices. In this paper, we present a hierarchical plug-and-play pruning-and-recovering framework, called Hierarchical Hourglass Tokenizer (H2OT), for efficient transformer-based 3D human pose estimation from videos. H2OT begins with progressively pruning pose tokens of redundant frames and ends with recovering full-length sequences, resulting in a few pose tokens in the intermediate transformer blocks and thus improving the model efficiency. It works with two key modules, namely, a Token Pruning Module (TPM) and a Token Recovering Module (TRM). TPM dynamically selects a few representative tokens to eliminate the redundancy of video frames, while TRM restores the detailed spatio-temporal information based on the selected tokens, thereby expanding the network output to the original full-length temporal resolution for fast inference. Our method is general-purpose: it can be easily incorporated into common VPT models on both seq2seq and seq2frame pipelines while effectively accommodating different token pruning and recovery strategies. In addition, our H2OT reveals that maintaining the full pose sequence is unnecessary, and a few pose tokens of representative frames can achieve both high efficiency and estimation accuracy. Extensive experiments on multiple benchmark datasets demonstrate both the effectiveness and efficiency of the proposed method.</description><author>IEEE Transactions on Pattern Analysis and Machine Intelligence - new TOC</author><pubDate>Wed, 10 Sep 2025 13:15:34 GMT</pubDate><guid isPermaLink="true">http://ieeexplore.ieee.org/document/11155208</guid></item><item><title>[JBHI] SegTom: A 3D Volumetric Medical Image Segmentation Framework for Thoracoabdominal Multi-Organ Anatomical Structures</title><link>http://ieeexplore.ieee.org/document/11151753</link><description>Accurate segmentation of thoracoabdominal anatomical structures in three-dimensional medical imaging modalities is fundamental for informed clinical decision-making across a wide array of medical disciplines. Current approaches often struggle to efficiently and comprehensively process this region’s intricate and heterogeneous anatomical information, leading to suboptimal outcomes in diagnosis, treatment planning, and disease management. To address this challenge, we introduce SegTom, a novel volumetric segmentation framework equipped with a cutting-edge SegTom Block specifically engineered to effectively capture the complex anatomical representations inherent to the thoracoabdominal region. This SegTom Block incorporates a hierarchical anatomical-representation decomposition to facilitate efficient information exchange by decomposing the computationally intensive self-attention mechanism and cost-effectively aggregating the extracted representations. Rigorous validation of SegTom across nine diverse datasets, encompassing both computed tomography (CT) and magnetic resonance imaging (MRI) modalities, consistently demonstrates high performance across a broad spectrum of anatomical structures. Specifically, SegTom achieves a mean Dice similarity coefficient (DSC) of 87.29% for cardiac segmentation on the MM-WHS MRI dataset, 83.48% for multi-organ segmentation on the BTCV abdominal CT dataset, and 92.01% for airway segmentation on a dedicated CT dataset.</description><author>IEEE Journal of Biomedical and Health Informatics - new TOC</author><pubDate>Fri, 05 Sep 2025 13:16:46 GMT</pubDate><guid isPermaLink="true">http://ieeexplore.ieee.org/document/11151753</guid></item><item><title>[TMI] EPDiff: Erasure Perception Diffusion Model for Unsupervised Anomaly Detection in Preoperative Multimodal Images</title><link>http://ieeexplore.ieee.org/document/11121881</link><description>Unsupervised anomaly detection (UAD) methods typically detect anomalies by learning and reconstructing the normative distribution. However, since anomalies constantly invade and affect their surroundings, sub-healthy areas in the junction present structural deformations that could be easily misidentified as anomalies, posing difficulties for UAD methods that solely learn the normative distribution. The use of multimodal images can facilitate to address the above challenges, as they can provide complementary information of anomalies. Therefore, this paper propose a novel method for UAD in preoperative multimodal images, called Erasure Perception Diffusion model (EPDiff). First, the Local Erasure Progressive Training (LEPT) framework is designed to better rebuild sub-healthy structures around anomalies through the diffusion model with a two-phase process. Initially, healthy images are used to capture deviation features labeled as potential anomalies. Then, these anomalies are locally erased in multimodal images to progressively learn sub-healthy structures, obtaining a more detailed reconstruction around anomalies. Second, the Global Structural Perception (GSP) module is developed in the diffusion model to realize global structural representation and correlation within images and between modalities through interactions of high-level semantic information. In addition, a training-free module, named Multimodal Attention Fusion (MAF) module, is presented for weighted fusion of anomaly maps between different modalities and obtaining binary anomaly outputs. Experimental results show that EPDiff improves the AUPRC and mDice scores by 2% and 3.9% on BraTS2021, and by 5.2% and 4.5% on Shifts over the state-of-the-art methods, which proves the applicability of EPDiff in diverse anomaly diagnosis. The code is available at https://github.com/wjiazheng/EPDiff</description><author>IEEE Transactions on Medical Imaging - new TOC</author><pubDate>Mon, 11 Aug 2025 13:16:40 GMT</pubDate><guid isPermaLink="true">http://ieeexplore.ieee.org/document/11121881</guid></item><item><title>[TMI] GM-ABS: Promptable Generalist Model Drives Active Barely Supervised Training in Specialist Model for 3D Medical Image Segmentation</title><link>http://ieeexplore.ieee.org/document/11119675</link><description>Semi-supervised learning (SSL) has greatly advanced 3D medical image segmentation by alleviating the need for intensive labeling by radiologists. While previous efforts focused on model-centric advancements, the emergence of foundational generalist models like the Segment Anything Model (SAM) is expected to reshape the SSL landscape. Although these generalists usually show performance gaps relative to previous specialists in medical imaging, they possess impressive zero-shot segmentation abilities with manual prompts. Thus, this capability could serve as “free lunch” for training specialists, offering future SSL a promising data-centric perspective, especially revolutionizing both pseudo and expert labeling strategies to enhance the data pool. In this regard, we propose the Generalist Model-driven Active Barely Supervised (GM-ABS) learning paradigm, for developing specialized 3D segmentation models under extremely limited (barely) annotation budgets, e.g., merely cross-labeling three slices per selected scan. In specific, building upon a basic mean-teacher SSL framework, GM-ABS modernizes the SSL paradigm with two key data-centric designs: (i) Specialist-generalist collaboration, where the in-training specialist leverages class-specific positional prompts derived from class prototypes to interact with the frozen class-agnostic generalist across multiple views to achieve noisy-yet-effective label augmentation. Then, the specialist robustly assimilates the augmented knowledge via noise-tolerant collaborative learning. (ii) Expert-model collaboration that promotes active cross-labeling with notably low labeling efforts. This design progressively furnishes the specialist with informative and efficient supervision via a human-in-the-loop manner, which in turn benefits the quality of class-specific prompts. Extensive experiments on three benchmark datasets highlight the promising performance of GM-ABS over recent SSL approaches under extremely constrained labeling resources.</description><author>IEEE Transactions on Medical Imaging - new TOC</author><pubDate>Thu, 07 Aug 2025 13:17:44 GMT</pubDate><guid isPermaLink="true">http://ieeexplore.ieee.org/document/11119675</guid></item><item><title>[TMI] Collaborative Learning of Augmentation and Disentanglement for Semi-Supervised Domain Generalized Medical Image Segmentation</title><link>http://ieeexplore.ieee.org/document/11115113</link><description>This paper explores a challenging yet realistic scenario: semi-supervised domain generalization (SSDG) that includes label scarcity and domain shift problems. We pinpoint that the limitations of previous SSDG methods lie in 1) neglecting the difference between domain shifts existing within a training dataset (intra-domain shift, IDS) and those occurring between training and testing datasets (cross-domain shift, CDS) and 2) overlooking the interplay between label scarcity and domain shifts, resulting in these methods merely stitching together semi-supervised learning (SSL) and domain generalization (DG) techniques. Considering these limitations, we propose a novel perspective to decompose SSDG into the combination of unsupervised domain adaptation (UDA) and DG problems. To this end, we design a causal augmentation and disentanglement framework (CausalAD) for semi-supervised domain generalized medical image segmentation. Concretely, CausalAD involves two collaborative processes: an augmentation process, which utilizes disentangled style factors to perform style augmentation for UDA, and a disentanglement process, which decouples domain-invariant (content) and domain-variant (noise and style) features for DG. Furthermore, we propose a proxy-based self-paced training strategy (ProSPT) to guide the training of CausalAD by gradually selecting unlabeled image pixels with high-quality pseudo labels in a self-paced training manner. Finally, we introduce a hierarchical structural causal model (HSCM) to explain the intuition and concept behind our method. Extensive experiments in the cross-sequence, cross-site, and cross-modality semi-supervised domain generalized medical image segmentation settings show the effectiveness of CausalAD and its superiority over the state-of-the-art. The code is available at https://github.com/Senyh/CausalAD</description><author>IEEE Transactions on Medical Imaging - new TOC</author><pubDate>Wed, 06 Aug 2025 13:17:46 GMT</pubDate><guid isPermaLink="true">http://ieeexplore.ieee.org/document/11115113</guid></item><item><title>[TMI] A Trust-Guided Approach to MR Image Reconstruction With Side Information</title><link>http://ieeexplore.ieee.org/document/11105520</link><description>Reducing MRI scan times can improve patient care and lower healthcare costs. Many acceleration methods are designed to reconstruct diagnostic-quality images from sparse k-space data, via an ill-posed or ill-conditioned linear inverse problem (LIP). To address the resulting ambiguities, it is crucial to incorporate prior knowledge into the optimization problem, e.g., in the form of regularization. Another form of prior knowledge less commonly used in medical imaging is the readily available auxiliary data (a.k.a. side information) obtained from sources other than the current acquisition. In this paper, we present the Trust-Guided Variational Network (TGVN), an end-to-end deep learning framework that effectively and reliably integrates side information into LIPs. We demonstrate its effectiveness in multi-coil, multi-contrast MRI reconstruction, where incomplete or low-SNR measurements from one contrast are used as side information to reconstruct high-quality images of another contrast from heavily under-sampled data. TGVN is robust across different contrasts, anatomies, and field strengths. Compared to baselines utilizing side information, TGVN achieves superior image quality while preserving subtle pathological features even at challenging acceleration levels, drastically speeding up acquisition while minimizing hallucinations. Source code and dataset splits are available on github.com/sodicksonlab/TGVN</description><author>IEEE Transactions on Medical Imaging - new TOC</author><pubDate>Thu, 31 Jul 2025 13:17:16 GMT</pubDate><guid isPermaLink="true">http://ieeexplore.ieee.org/document/11105520</guid></item><item><title>[TMI] Dual Cross-Image Semantic Consistency With Self-Aware Pseudo Labeling for Semi-Supervised Medical Image Segmentation</title><link>http://ieeexplore.ieee.org/document/11104231</link><description>Semi-supervised learning has proven highly effective in tackling the challenge of limited labeled training data in medical image segmentation. In general, current approaches, which rely on intra-image pixel-wise consistency training via pseudo-labeling, overlook the consistency at more comprehensive semantic levels (e.g., object region) and suffer from severe discrepancy of extracted features resulting from an imbalanced number of labeled and unlabeled data. To overcome these limitations, we present a new Dual Cross-image Semantic Consistency (DuCiSC) learning framework, for semi-supervised medical image segmentation. Concretely, beyond enforcing pixel-wise semantic consistency, DuCiSC proposes dual paradigms to encourage region-level semantic consistency across: 1) labeled and unlabeled images; and 2) labeled and fused images, by explicitly aligning their prototypes. Relying on the dual paradigms, DuCiSC can effectively establish consistent cross-image semantics via prototype representations, thereby addressing the feature discrepancy issue. Moreover, we devise a novel self-aware confidence estimation strategy to accurately select reliable pseudo labels, allowing for exploiting the training dynamics of unlabeled data. Our DuCiSC method is extensively validated on four datasets, including two popular binary benchmarks in segmenting the left atrium and pancreas, a multi-class Automatic Cardiac Diagnosis Challenge dataset, and a challenging scenario of segmenting the inferior alveolar nerve that features complicated anatomical structures, showing superior segmentation results over previous state-of-the-art approaches. Our code is publicly available at https://github.com/ShanghaiTech-IMPACT/DuCiSC</description><author>IEEE Transactions on Medical Imaging - new TOC</author><pubDate>Wed, 30 Jul 2025 13:17:43 GMT</pubDate><guid isPermaLink="true">http://ieeexplore.ieee.org/document/11104231</guid></item><item><title>[TMI] 3D Deep-Learning-Based Segmentation of Human Skin Sweat Glands and Their 3D Morphological Response to Temperature Variations</title><link>http://ieeexplore.ieee.org/document/11098795</link><description>Skin, the primary regulator of heat exchange, relies on sweat glands for thermoregulation. Alterations in sweat gland morphology play a crucial role in various pathological conditions and clinical diagnoses. Current methods for observing sweat gland morphology are limited by their two-dimensional, in vitro, and destructive nature, underscoring the urgent need for real-time, non-invasive, quantifiable technologies. We proposed a novel three-dimensional (3D) transformer-based segmentation framework, enabling quite precise 3D sweat gland segmentation from skin volume data captured by optical coherence tomography (OCT). We quantitatively reveal, for the first time, 3D sweat gland morphological changes with temperature: for instance, volume, surface area, and length increase by 42.0%, 26.4%, and 12.8% at 43°C vs. 10°C (all p &lt;0.001), while S/V ratio decreases (p =0.01). By establishing a benchmark for normal sweat gland morphology and offering a real-time, non-invasive tool for quantifying 3D structural parameters, our approach facilitates the study of individual variability and pathological changes in sweat gland morphology, contributing to advancements in dermatological research and clinical applications.</description><author>IEEE Transactions on Medical Imaging - new TOC</author><pubDate>Tue, 29 Jul 2025 13:17:02 GMT</pubDate><guid isPermaLink="true">http://ieeexplore.ieee.org/document/11098795</guid></item><item><title>[TMI] Visualization of Breast Cancer Using Contrast-Enhanced Optical Coherence Elastography Based on Tissue Heterogeneity</title><link>http://ieeexplore.ieee.org/document/11098988</link><description>By mapping the mechanical properties of tissue, elastography can improve identification of breast cancer. On the macro-scale, ultrasound elastography and magnetic resonance elastography have emerged as effective clinical methods for the diagnosis of tumors. On the micro-scale, optical coherence elastography (OCE) shows promise for intraoperative tumor margin assessment during breast-conserving surgery. Whilst several OCE studies have demonstrated strong potential, the mechanical models used require the assumption of uniaxial stress throughout the sample. However, breast tissue is heterogeneous and contains compressible features (e.g., ducts and blood vessels) and collagen-rich fibrotic features (e.g., stroma). This heterogeneity can invalidate the assumption of uniaxial stress and reduce the accuracy of OCE, often making it challenging to interpret images. Here, we demonstrate a new variant of OCE based on mapping the Euler angle, i.e., the angle between the principal compression and the loading axis induced by tissue heterogeneity, which removes the assumption of uniaxial deformation. This is enabled by a hybrid three-dimensional (3-D) displacement estimation method that combines phase-sensitive detection and complex cross-correlation, providing access to the 3-D displacement and 3-D strain tensor on the micro-scale. Through experiments on phantoms, we demonstrate that an accuracy of 98.6%, a sensitivity of 0.95° (i.e., 16.58 mrad), and a spatial resolution as high as  $36~\mu $ m can be achieved in Euler angle imaging. We demonstrate the potential of Euler angle imaging for visualization of breast cancer. Through close correspondence with histology, our results show that mapping the Euler angle provides additional contrast to both optical coherence tomography and a compression OCE technique in identifying cancer. Mapping the Euler angle in breast tissue may provide a new biomarker for intraoperative tumor margin assessment.</description><author>IEEE Transactions on Medical Imaging - new TOC</author><pubDate>Tue, 29 Jul 2025 13:17:02 GMT</pubDate><guid isPermaLink="true">http://ieeexplore.ieee.org/document/11098988</guid></item><item><title>[TMI] A Deep Learning Multimodal Fusion-Based Method for Cell and Nucleus Segmentation</title><link>http://ieeexplore.ieee.org/document/11096725</link><description>In recent years, deep learning has been widely utilized in the fields of biomedical image segmentation and cellular image analysis. Supervised deep neural networks trained on annotated data have demonstrated good performance in tasks related to cell and nucleus segmentation. However, the use of supervised models necessitates carefully constructed training data and a substantial amount of ground truth information. Unfortunately, high-quality annotated data for cellular images are scarce. To address the issue of limited datasets, we propose a cell and nucleus segmentation method based on deep learning multimodal fusion. The proposed method includes three modules: a segmentation fundamental module, a multimodal prompter module, and an object output module. This comprehensive approach enables cell and nucleus segmentation tasks to be performed without the need for retraining on new data. The segmentation fundamental module is the core of the framework, as it provides essential segmentation capabilities. By leveraging preexisting models trained on natural imagery, this module effectively performs cell segmentation by incorporating prior knowledge. The multimodal prompter module, a pretrained model, aids in combining image and textual information. It employs a data fusion technique for multiple modalities to deliver prompts that steer the network’s output, thereby avoiding the constraints inherent to single-modality approaches. The object output module combines the inputs from the preceding modules to generate the final segmentation output. The experimental validation confirms the superiority of the proposed method, which outperforms comparative methods in cell and nucleus segmentation tasks and has promise for future applications in cell tracking.</description><author>IEEE Transactions on Medical Imaging - new TOC</author><pubDate>Fri, 25 Jul 2025 13:18:29 GMT</pubDate><guid isPermaLink="true">http://ieeexplore.ieee.org/document/11096725</guid></item><item><title>[TMI] SarAdapter: Prioritizing Attention on Semantic-Aware Representative Tokens for Enhanced Medical Image Segmentation</title><link>http://ieeexplore.ieee.org/document/11089976</link><description>Transformer-based segmentation methods exhibit considerable potential in medical image analysis. However, their improved performance often comes with increased computational complexity, limiting their application in resource-constrained medical settings. Prior methods follow two independent tracks: (i) accelerating existing networks via semantic-aware routing, and (ii) optimizing token adapter design to enhance network performance. Despite directness, they encounter unavoidable defects (e.g., inflexible acceleration techniques or non-discriminative processing) limiting further improvements of quality-complexity trade-off. To address these shortcomings, we integrate these schemes by proposing the semantic-aware adapter (SarAdapter), which employs a semantic-based routing strategy, leveraging neural operators (ViT and CNN) of varying complexities. Specifically, it merges semantically similar tokens volume into low-resolution regions while preserving semantically distinct tokens as high-resolution regions. Additionally, we introduce a Mixed-adapter unit, which adaptively selects convolutional operators of varying complexities to better model regions at different scales. We evaluate our method on four medical datasets from three modalities and show that it achieves a superior balance between accuracy, model size, and efficiency. Notably, our proposed method achieves state-of-the-art segmentation quality on the Synapse dataset while reducing the number of tokens by 65.6%, signifying a substantial improvement in the efficiency of ViTs for the segmentation task.</description><author>IEEE Transactions on Medical Imaging - new TOC</author><pubDate>Tue, 22 Jul 2025 13:16:21 GMT</pubDate><guid isPermaLink="true">http://ieeexplore.ieee.org/document/11089976</guid></item><item><title>[TMI] EICSeg: Universal Medical Image Segmentation via Explicit In-Context Learning</title><link>http://ieeexplore.ieee.org/document/11090002</link><description>Deep learning models for medical image segmentation often struggle with task-specific characteristics, limiting their generalization to unseen tasks with new anatomies, labels, or modalities. Retraining or fine-tuning these models requires substantial human effort and computational resources. To address this, in-context learning (ICL) has emerged as a promising paradigm, enabling query image segmentation by conditioning on example image-mask pairs provided as prompts. Unlike previous approaches that rely on implicit modeling or non-end-to-end pipelines, we redefine the core interaction mechanism in ICL as an explicit retrieval process, termed E-ICL, benefiting from the emergence of vision foundation models (VFMs). E-ICL captures dense correspondences between queries and prompts at minimal learning cost and leverages them to dynamically weight multi-class prompt masks. Built upon E-ICL, we propose EICSeg, the first end-to-end ICL framework that integrates complementary VFMs for universal medical image segmentation. Specifically, we introduce a lightweight SD-Adapter to bridge the distinct functionalities of the VFMs, enabling more accurate segmentation predictions. To fully exploit the potential of EICSeg, we further design a scalable self-prompt training strategy and an adaptive token-to-image prompt selection mechanism, facilitating both efficient training and inference. EICSeg is trained on 47 datasets covering diverse modalities and segmentation targets. Experiments on nine unseen datasets demonstrate its strong few-shot generalization ability, achieving an average Dice score of 74.0%, outperforming existing in-context and few-shot methods by 4.5%, and reducing the gap to task-specific models to 10.8%. Even with a single prompt, EICSeg achieves a competitive average Dice score of 60.1%. Notably, it performs automatic segmentation without manual prompt engineering, delivering results comparable to interactive models while requiring minimal labeled data. Source code will be available at https://github.com/zerone-fg/EICSeg</description><author>IEEE Transactions on Medical Imaging - new TOC</author><pubDate>Tue, 22 Jul 2025 13:16:21 GMT</pubDate><guid isPermaLink="true">http://ieeexplore.ieee.org/document/11090002</guid></item><item><title>[TMI] REDNet: Reliable Evidential Discounting Network for Multi-Modality Medical Image Segmentation</title><link>http://ieeexplore.ieee.org/document/11087642</link><description>In the field of computer-aided diagnosis, particularly for tumor diseases, segmentation is a prerequisite and primary step. Multi-modality images become essential for achieving accurate segmentation, which offer critical insights beyond the limitations of single-modality data. However, different modalities and images may suffer from different types of data imperfection, such as intensity non-uniformity, motion artifact, and low quality due to hardware limitations, which challenge image segmentation algorithms. To address this challenge, we propose a Reliable Evidential Discounting Network (REDNet), which is composed of three main modules: 1) the Intra-modality Consistency Evaluation Module (ICEM) measuring the data cohesion within the same modality; 2) the Cross-modality Difference Aggregation Module (CDAM) identifing data discrepancy across modalities; 3) the Discounting Fusion Module (DFM) processing the multi-modality evidence by applying discounting strategies to fuse the data. This approach maintains segmentation accuracy by effectively integrating multi-modality evidence, while discounting the influence of lower-quality data, ensuring reliable results despite the presence of image imperfections. We evaluated REDNet on two distinct datasets, BRATS2021 and an in-house pancreas dataset from Changhai Hospital. REDNet outperforms other methods, particularly in scenarios with imperfect image sources, and achieves reliable results in multi-modality tumor segmentation.</description><author>IEEE Transactions on Medical Imaging - new TOC</author><pubDate>Mon, 21 Jul 2025 13:19:10 GMT</pubDate><guid isPermaLink="true">http://ieeexplore.ieee.org/document/11087642</guid></item><item><title>[TMI] SegMamba-V2: Long-Range Sequential Modeling Mamba for General 3-D Medical Image Segmentation</title><link>http://ieeexplore.ieee.org/document/11084842</link><description>The Transformer architecture has demonstrated remarkable results in 3D medical image segmentation due to its capability of modeling global relationships. However, it poses a significant computational burden when processing high-dimensional medical images. Mamba, as a State Space Model (SSM), has recently emerged as a notable approach for modeling long-range dependencies in sequential data. Although a substantial amount of Mamba-based research has focused on natural language and 2D image processing, few studies explore the capability of Mamba on 3D medical images. In this paper, we propose SegMamba-V2, a novel 3D medical image segmentation model, to effectively capture long-range dependencies within whole-volume features at each scale. To achieve this goal, we first devise a hierarchical scale downsampling strategy to enhance the receptive field and mitigate information loss during downsampling. Furthermore, we design a novel tri-orientated spatial Mamba block that extends the global dependency modeling process from one plane to three orthogonal planes to improve feature representation capability. Moreover, we collect and annotate a large-scale dataset (named CRC-2000) with fine-grained categories to facilitate benchmarking evaluation in 3D colorectal cancer (CRC) segmentation. We evaluate the effectiveness of our SegMamba-V2 on CRC-2000 and three other large-scale 3D medical image segmentation datasets, covering various modalities, organs, and segmentation targets. Experimental results demonstrate that our Segmamba-V2 outperforms state-of-the-art methods by a significant margin, which indicates the universality and effectiveness of the proposed model on 3D medical image segmentation tasks. The code for SegMamba-V2 is publicly available at: https://github.com/ge-xing/SegMamba-V2</description><author>IEEE Transactions on Medical Imaging - new TOC</author><pubDate>Fri, 18 Jul 2025 13:16:20 GMT</pubDate><guid isPermaLink="true">http://ieeexplore.ieee.org/document/11084842</guid></item><item><title>[TBME] Assessment of Breast Composition With a Transmission-Based Microwave Imaging System</title><link>http://ieeexplore.ieee.org/document/11059861</link><description>Breast density is a key risk factor for breast cancer, but it is typically unknown before a first mammogram. Microwave imaging, proposed for cancer detection and monitoring, offers potential for measuring the composition of the breast. Objective: Assess the potential of microwave imaging as a method for estimating breast composition via correlation with mammogram metrics. Methods: Transmission based microwave imaging was applied to a cohort of 110 participants with prior mammograms. Several techniques were developed to estimate breast composition from microwave images, including average permittivity calculation, image thresholding and segmentation, and estimation of the fraction of glandular tissue in each pixel. These measures were compared to breast density category and percent density available from mammograms. Results: Average permittivity from microwave images correlated strongly with mammogram-based metrics. For the average permittivity, statistical analysis using one-way ANOVA revealed significant group differences across the various breast density categories. Thresholding and segmentation involved more detailed analysis of the images, and showed potential as alternative approaches to differentiating between breast composition categories. Conclusions: This study represents the largest cohort of healthy participants in which microwave breast images were compared with breast composition data available from clinical imaging. The cohort is well balanced across all categories. It highlights microwave imaging as a safe, portable, and affordable tool for non-invasive breast composition assessment and early cancer risk detection. Significance: The correlation between microwave imaging and mammogram-based breast density metrics highlights the potential for microwave imaging as a novel method for assessment of breast composition.</description><author>IEEE Transactions on Biomedical Engineering - new TOC</author><pubDate>Mon, 30 Jun 2025 13:19:40 GMT</pubDate><guid isPermaLink="true">http://ieeexplore.ieee.org/document/11059861</guid></item><item><title>[TBME] Prompt Learning With Bounding Box Constraints for Medical Image Segmentation</title><link>http://ieeexplore.ieee.org/document/11049003</link><description>Pixel-wise annotations are notoriously labourious and costly to obtain in the medical domain. To mitigate this burden, weakly supervised approaches based on bounding box annotations—much easier to acquire—offer a practical alternative. Vision foundation models have recently shown noteworthy segmentation performance when provided with prompts such as points or bounding boxes. Prompt learning exploits these models by adapting them to downstream tasks and automating segmentation, thereby reducing user intervention. However, existing prompt learning approaches depend on fully annotated segmentation masks. This paper proposes a novel framework that combines the representational power of foundation models with the annotation efficiency of weakly supervised segmentation. More specifically, our approach automates prompt generation for foundation models using only bounding box annotations. Our proposed optimization scheme integrates multiple constraints derived from box annotations with pseudo-labels generated by the prompted foundation model. Extensive experiments across multi-modal datasets reveal that, using the Segment Anything Model (SAM) as backbone, our weakly supervised method achieves an average Dice score of 84.90% in a limited data setting, outperforming existing fully-supervised and weakly-supervised approaches.</description><author>IEEE Transactions on Biomedical Engineering - new TOC</author><pubDate>Tue, 24 Jun 2025 13:17:26 GMT</pubDate><guid isPermaLink="true">http://ieeexplore.ieee.org/document/11049003</guid></item></channel></rss>