<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>My Customized Papers</title><link>https://github.com/your_username/your_repo</link><description>Aggregated research papers</description><language>en-US</language><lastBuildDate>Mon, 12 Jan 2026 02:09:43 GMT</lastBuildDate><generator>rfeed v1.1.1</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>[BSPC] MACNet:Multiscale attention cross-sharing networks for colorectal polyp segmentation</title><link>https://www.sciencedirect.com/science/article/pii/S1746809426000042?dgcid=rss_sd_all</link><description>&lt;p&gt;Publication date: 1 May 2026&lt;/p&gt;&lt;p&gt;&lt;b&gt;Source:&lt;/b&gt; Biomedical Signal Processing and Control, Volume 116&lt;/p&gt;&lt;p&gt;Author(s): Xinhui Jiang, Chunmiao Wei, Xiaolin Li, Zhicheng Dai&lt;/p&gt;</description><author>ScienceDirect Publication: Biomedical Signal Processing and Control</author><pubDate>Sun, 11 Jan 2026 18:35:03 GMT</pubDate><guid isPermaLink="true">https://www.sciencedirect.com/science/article/pii/S1746809426000042</guid></item><item><title>[BSPC] CSA-Net: Class self-attention based feature extraction network for polyp segmentation</title><link>https://www.sciencedirect.com/science/article/pii/S174680942600042X?dgcid=rss_sd_all</link><description>&lt;p&gt;Publication date: 1 May 2026&lt;/p&gt;&lt;p&gt;&lt;b&gt;Source:&lt;/b&gt; Biomedical Signal Processing and Control, Volume 116&lt;/p&gt;&lt;p&gt;Author(s): Xuehu Wang, Xiangqian Liu, Jin Lu, Yuhao Wang, Chao Xue, Han Yu, Yongchang Zheng, Chen Geng, Chengwei Guo, Xiaoping Yin&lt;/p&gt;</description><author>ScienceDirect Publication: Biomedical Signal Processing and Control</author><pubDate>Sun, 11 Jan 2026 18:35:03 GMT</pubDate><guid isPermaLink="true">https://www.sciencedirect.com/science/article/pii/S174680942600042X</guid></item><item><title>[BSPC] EGNet: A boundary-region closed-loop network for medical image segmentation with fuzzy lesions</title><link>https://www.sciencedirect.com/science/article/pii/S1746809426001564?dgcid=rss_sd_all</link><description>&lt;p&gt;Publication date: 1 May 2026&lt;/p&gt;&lt;p&gt;&lt;b&gt;Source:&lt;/b&gt; Biomedical Signal Processing and Control, Volume 116&lt;/p&gt;&lt;p&gt;Author(s): Shuwen Wu, Qilong Li, Ruixue Xia, Chongsheng Zhang&lt;/p&gt;</description><author>ScienceDirect Publication: Biomedical Signal Processing and Control</author><pubDate>Sun, 11 Jan 2026 18:35:03 GMT</pubDate><guid isPermaLink="true">https://www.sciencedirect.com/science/article/pii/S1746809426001564</guid></item><item><title>[BSPC] Hawk-Net: Medical image segmentation and classification using multi-scale convolutional self-attention-based image processor with DK-CNN-Mamba-xAttention Fusion Network</title><link>https://www.sciencedirect.com/science/article/pii/S1746809425019123?dgcid=rss_sd_all</link><description>&lt;p&gt;Publication date: 15 April 2026&lt;/p&gt;&lt;p&gt;&lt;b&gt;Source:&lt;/b&gt; Biomedical Signal Processing and Control, Volume 115&lt;/p&gt;&lt;p&gt;Author(s): Mahmudul Hasan, Md. Kamrul Hasan&lt;/p&gt;</description><author>ScienceDirect Publication: Biomedical Signal Processing and Control</author><pubDate>Sun, 11 Jan 2026 13:09:00 GMT</pubDate><guid isPermaLink="true">https://www.sciencedirect.com/science/article/pii/S1746809425019123</guid></item><item><title>[BSPC] EFFM: Rethinking feature fusion mechanisms for medical image segmentation</title><link>https://www.sciencedirect.com/science/article/pii/S1746809425018452?dgcid=rss_sd_all</link><description>&lt;p&gt;Publication date: 15 April 2026&lt;/p&gt;&lt;p&gt;&lt;b&gt;Source:&lt;/b&gt; Biomedical Signal Processing and Control, Volume 115&lt;/p&gt;&lt;p&gt;Author(s): Xiaoyan Zhang, Zheng Zhao, Yongqin Zhang, Chunlin Yu, Xiangfu Meng, Shuai Li&lt;/p&gt;</description><author>ScienceDirect Publication: Biomedical Signal Processing and Control</author><pubDate>Sun, 11 Jan 2026 13:09:00 GMT</pubDate><guid isPermaLink="true">https://www.sciencedirect.com/science/article/pii/S1746809425018452</guid></item><item><title>[BSPC] Applications of a novel deep neural network to the classification of liver steatosis and breast lesions in ultrasound images</title><link>https://www.sciencedirect.com/science/article/pii/S1746809425019068?dgcid=rss_sd_all</link><description>&lt;p&gt;Publication date: 15 April 2026&lt;/p&gt;&lt;p&gt;&lt;b&gt;Source:&lt;/b&gt; Biomedical Signal Processing and Control, Volume 115&lt;/p&gt;&lt;p&gt;Author(s): Özlem Polat, Zümray Dokur, Tamer Ölmez&lt;/p&gt;</description><author>ScienceDirect Publication: Biomedical Signal Processing and Control</author><pubDate>Sun, 11 Jan 2026 13:09:00 GMT</pubDate><guid isPermaLink="true">https://www.sciencedirect.com/science/article/pii/S1746809425019068</guid></item><item><title>[BSPC] Frequency matters: Dual Domain Consistency for Semi-supervised Skin Lesion Segmentation</title><link>https://www.sciencedirect.com/science/article/pii/S1746809425019421?dgcid=rss_sd_all</link><description>&lt;p&gt;Publication date: 15 April 2026&lt;/p&gt;&lt;p&gt;&lt;b&gt;Source:&lt;/b&gt; Biomedical Signal Processing and Control, Volume 115&lt;/p&gt;&lt;p&gt;Author(s): Haoran Xi, Shan Ling, Min He, Xiaolin Li&lt;/p&gt;</description><author>ScienceDirect Publication: Biomedical Signal Processing and Control</author><pubDate>Sun, 11 Jan 2026 13:09:00 GMT</pubDate><guid isPermaLink="true">https://www.sciencedirect.com/science/article/pii/S1746809425019421</guid></item><item><title>[BSPC] Flexible dilated convolution and bidirectional pyramid approach to polyp segmentation</title><link>https://www.sciencedirect.com/science/article/pii/S1746809425019202?dgcid=rss_sd_all</link><description>&lt;p&gt;Publication date: 15 April 2026&lt;/p&gt;&lt;p&gt;&lt;b&gt;Source:&lt;/b&gt; Biomedical Signal Processing and Control, Volume 115&lt;/p&gt;&lt;p&gt;Author(s): Yunfei Yin, Zumnan Timbi, Zheng Yuan, Sijing Xiong, MD Tanvir Islam, Argho Dey, Swachha Ray&lt;/p&gt;</description><author>ScienceDirect Publication: Biomedical Signal Processing and Control</author><pubDate>Sun, 11 Jan 2026 13:09:00 GMT</pubDate><guid isPermaLink="true">https://www.sciencedirect.com/science/article/pii/S1746809425019202</guid></item><item><title>[BSPC] Enhanced diagnosis of skin lesions through torsional wave propagation and probabilistic inverse problem algorithms: An experimental study</title><link>https://www.sciencedirect.com/science/article/pii/S174680942501907X?dgcid=rss_sd_all</link><description>&lt;p&gt;Publication date: 15 April 2026&lt;/p&gt;&lt;p&gt;&lt;b&gt;Source:&lt;/b&gt; Biomedical Signal Processing and Control, Volume 115&lt;/p&gt;&lt;p&gt;Author(s): Yousef Almashakbeh, Hirad Shamimi, Inas H Faris, J.L. Martín-Rodríguez, Antonio Callejas, Guillermo Rus&lt;/p&gt;</description><author>ScienceDirect Publication: Biomedical Signal Processing and Control</author><pubDate>Sun, 11 Jan 2026 13:09:00 GMT</pubDate><guid isPermaLink="true">https://www.sciencedirect.com/science/article/pii/S174680942501907X</guid></item><item><title>[BSPC] DiffMoE-UNet: A differential transformer with Mixture-of-Experts for accurate medical image segmentation</title><link>https://www.sciencedirect.com/science/article/pii/S174680942501924X?dgcid=rss_sd_all</link><description>&lt;p&gt;Publication date: 15 April 2026&lt;/p&gt;&lt;p&gt;&lt;b&gt;Source:&lt;/b&gt; Biomedical Signal Processing and Control, Volume 115&lt;/p&gt;&lt;p&gt;Author(s): Jaouad Tagnamas, Hiba Ramadan, Ali Yahyaouy, Hamid Tairi&lt;/p&gt;</description><author>ScienceDirect Publication: Biomedical Signal Processing and Control</author><pubDate>Sun, 11 Jan 2026 13:09:00 GMT</pubDate><guid isPermaLink="true">https://www.sciencedirect.com/science/article/pii/S174680942501924X</guid></item><item><title>[BSPC] Uncertainty-weighted feature alignment and information interaction network for multi-scale medical image segmentation</title><link>https://www.sciencedirect.com/science/article/pii/S174680942501910X?dgcid=rss_sd_all</link><description>&lt;p&gt;Publication date: 15 April 2026&lt;/p&gt;&lt;p&gt;&lt;b&gt;Source:&lt;/b&gt; Biomedical Signal Processing and Control, Volume 115&lt;/p&gt;&lt;p&gt;Author(s): Min Zhang, Junxia Wang, Junkai Wang, Yuanjie Zheng&lt;/p&gt;</description><author>ScienceDirect Publication: Biomedical Signal Processing and Control</author><pubDate>Sun, 11 Jan 2026 13:09:00 GMT</pubDate><guid isPermaLink="true">https://www.sciencedirect.com/science/article/pii/S174680942501910X</guid></item><item><title>[BSPC] DisFreAda: Distribution-driven multi-frequency adaptive network for generalizable medical image segmentation</title><link>https://www.sciencedirect.com/science/article/pii/S1746809426000236?dgcid=rss_sd_all</link><description>&lt;p&gt;Publication date: 1 May 2026&lt;/p&gt;&lt;p&gt;&lt;b&gt;Source:&lt;/b&gt; Biomedical Signal Processing and Control, Volume 116&lt;/p&gt;&lt;p&gt;Author(s): Minjun Wang, Houjin Chen, Yanfeng Li, Jia Sun, Luyifu Chen, Peng Liang&lt;/p&gt;</description><author>ScienceDirect Publication: Biomedical Signal Processing and Control</author><pubDate>Sun, 11 Jan 2026 13:09:00 GMT</pubDate><guid isPermaLink="true">https://www.sciencedirect.com/science/article/pii/S1746809426000236</guid></item><item><title>[BSPC] Morphology-enhanced CAM-guided SAM for weakly supervised breast lesion segmentation</title><link>https://www.sciencedirect.com/science/article/pii/S1746809426000637?dgcid=rss_sd_all</link><description>&lt;p&gt;Publication date: 1 May 2026&lt;/p&gt;&lt;p&gt;&lt;b&gt;Source:&lt;/b&gt; Biomedical Signal Processing and Control, Volume 116&lt;/p&gt;&lt;p&gt;Author(s): Xin Yue, Qing Zhao, Xiaoling Liu, Jianqiang Li, Jing Bai, Changwei Song, Suqin Liu, Rodrigo Moreno, Zhikai Yang, Stefano E. Romero, Gabriel Jimenez, Guanghui Fu&lt;/p&gt;</description><author>ScienceDirect Publication: Biomedical Signal Processing and Control</author><pubDate>Sun, 11 Jan 2026 13:09:00 GMT</pubDate><guid isPermaLink="true">https://www.sciencedirect.com/science/article/pii/S1746809426000637</guid></item><item><title>[BSPC] Medical image segmentation based on 3D PDC with Swin Transformer</title><link>https://www.sciencedirect.com/science/article/pii/S1746809426000704?dgcid=rss_sd_all</link><description>&lt;p&gt;Publication date: 1 May 2026&lt;/p&gt;&lt;p&gt;&lt;b&gt;Source:&lt;/b&gt; Biomedical Signal Processing and Control, Volume 116&lt;/p&gt;&lt;p&gt;Author(s): Lin Fan, Xiaojia Ding, Zhongmin Wang, Hai Wang, Rong Zhang&lt;/p&gt;</description><author>ScienceDirect Publication: Biomedical Signal Processing and Control</author><pubDate>Sun, 11 Jan 2026 13:09:00 GMT</pubDate><guid isPermaLink="true">https://www.sciencedirect.com/science/article/pii/S1746809426000704</guid></item><item><title>[EAAI] Cross-Granularity Fusion Vision Mamba UNet for medical image segmentation</title><link>https://www.sciencedirect.com/science/article/pii/S0952197626000679?dgcid=rss_sd_all</link><description>&lt;p&gt;Publication date: 1 March 2026&lt;/p&gt;&lt;p&gt;&lt;b&gt;Source:&lt;/b&gt; Engineering Applications of Artificial Intelligence, Volume 167, Part 1&lt;/p&gt;&lt;p&gt;Author(s): Tuersunjiang Baidi, Zitong Ren, Kurban Ubul, Alimjan Aysa, Boyuan Li, Shihao Wang&lt;/p&gt;</description><author>ScienceDirect Publication: Engineering Applications of Artificial Intelligence</author><pubDate>Sun, 11 Jan 2026 13:09:00 GMT</pubDate><guid isPermaLink="true">https://www.sciencedirect.com/science/article/pii/S0952197626000679</guid></item><item><title>[AIM] Siamese evolutionary masking: Enhancing the generalization of self-supervised medical image segmentation model</title><link>https://www.sciencedirect.com/science/article/pii/S0933365726000011?dgcid=rss_sd_all</link><description>&lt;p&gt;Publication date: Available online 7 January 2026&lt;/p&gt;&lt;p&gt;&lt;b&gt;Source:&lt;/b&gt; Artificial Intelligence in Medicine&lt;/p&gt;&lt;p&gt;Author(s): Yichen Zhi, Hongxia Bie, Jiali Wang, Zhao Jing&lt;/p&gt;</description><author>ScienceDirect Publication: Artificial Intelligence in Medicine</author><pubDate>Sun, 11 Jan 2026 13:09:00 GMT</pubDate><guid isPermaLink="true">https://www.sciencedirect.com/science/article/pii/S0933365726000011</guid></item><item><title>[NC] UP2D: Uncertainty-aware progressive pseudo-label denoising for source-free domain adaptive medical image segmentation</title><link>https://www.sciencedirect.com/science/article/pii/S0925231226000561?dgcid=rss_sd_all</link><description>&lt;p&gt;Publication date: Available online 9 January 2026&lt;/p&gt;&lt;p&gt;&lt;b&gt;Source:&lt;/b&gt; Neurocomputing&lt;/p&gt;&lt;p&gt;Author(s): Thanh-Huy Nguyen, Quang-Khai Bui-Tran, Manh D. Ho, Thinh B. Lam, Vi Vu, Hoang-Thien Nguyen, Phat Huynh, Ulas Bagci&lt;/p&gt;</description><author>ScienceDirect Publication: Neurocomputing</author><pubDate>Sun, 11 Jan 2026 13:08:59 GMT</pubDate><guid isPermaLink="true">https://www.sciencedirect.com/science/article/pii/S0925231226000561</guid></item><item><title>[KBS] BACFormer: A robust boundary-aware transformer for medical image segmentation</title><link>https://www.sciencedirect.com/science/article/pii/S0950705125022439?dgcid=rss_sd_all</link><description>&lt;p&gt;Publication date: 28 February 2026&lt;/p&gt;&lt;p&gt;&lt;b&gt;Source:&lt;/b&gt; Knowledge-Based Systems, Volume 335&lt;/p&gt;&lt;p&gt;Author(s): Zhiyong Huang, Mingyu Wang, Mingyang Hou, Zhi Yu, Shiwei Wang, Xiaoyu Li, Jiahong Wang, Yan Yan, Yushi Liu&lt;/p&gt;</description><author>ScienceDirect Publication: Knowledge-Based Systems</author><pubDate>Sun, 11 Jan 2026 13:08:59 GMT</pubDate><guid isPermaLink="true">https://www.sciencedirect.com/science/article/pii/S0950705125022439</guid></item><item><title>[KBS] Multimodality-based framework for enhanced skin lesion recognition over federated learning</title><link>https://www.sciencedirect.com/science/article/pii/S0950705125021859?dgcid=rss_sd_all</link><description>&lt;p&gt;Publication date: 28 February 2026&lt;/p&gt;&lt;p&gt;&lt;b&gt;Source:&lt;/b&gt; Knowledge-Based Systems, Volume 335&lt;/p&gt;&lt;p&gt;Author(s): Abdul Hai Karimi, Taimoor Khan, Chang Choi&lt;/p&gt;</description><author>ScienceDirect Publication: Knowledge-Based Systems</author><pubDate>Sun, 11 Jan 2026 13:08:59 GMT</pubDate><guid isPermaLink="true">https://www.sciencedirect.com/science/article/pii/S0950705125021859</guid></item><item><title>[ESWA] WBDM-ECRF: A bridge diffusion model with efficient conditional random field for skin lesion segmentation</title><link>https://www.sciencedirect.com/science/article/pii/S0957417425046159?dgcid=rss_sd_all</link><description>&lt;p&gt;Publication date: 25 April 2026&lt;/p&gt;&lt;p&gt;&lt;b&gt;Source:&lt;/b&gt; Expert Systems with Applications, Volume 307&lt;/p&gt;&lt;p&gt;Author(s): Hefeng Ji, Jing Xiao, Jimin Liu, Haoyong Yu, Chao Chen&lt;/p&gt;</description><author>ScienceDirect Publication: Expert Systems with Applications</author><pubDate>Sun, 11 Jan 2026 13:08:59 GMT</pubDate><guid isPermaLink="true">https://www.sciencedirect.com/science/article/pii/S0957417425046159</guid></item><item><title>[ESWA] MED-Net: Leveraging multi-visual encoding and Manhattan-distance pooling for robust medical image segmentation</title><link>https://www.sciencedirect.com/science/article/pii/S0957417426000175?dgcid=rss_sd_all</link><description>&lt;p&gt;Publication date: 1 May 2026&lt;/p&gt;&lt;p&gt;&lt;b&gt;Source:&lt;/b&gt; Expert Systems with Applications, Volume 308&lt;/p&gt;&lt;p&gt;Author(s): Zhaozhao Su, Yuxuan Luo, Shiji Song, Zhiqiang Zhu, Jiajie Wang, Zhihua Fang, Bowen Wang, Liejun Wang&lt;/p&gt;</description><author>ScienceDirect Publication: Expert Systems with Applications</author><pubDate>Sun, 11 Jan 2026 13:08:59 GMT</pubDate><guid isPermaLink="true">https://www.sciencedirect.com/science/article/pii/S0957417426000175</guid></item><item><title>[ESWA] A colon polyp segmentation network via collaborative decision-making of mixture of experts</title><link>https://www.sciencedirect.com/science/article/pii/S095741742600031X?dgcid=rss_sd_all</link><description>&lt;p&gt;Publication date: 1 May 2026&lt;/p&gt;&lt;p&gt;&lt;b&gt;Source:&lt;/b&gt; Expert Systems with Applications, Volume 308&lt;/p&gt;&lt;p&gt;Author(s): Wenchao Zhang, Wenhui Ye, Zhenhua Yu, Jianguo Ju&lt;/p&gt;</description><author>ScienceDirect Publication: Expert Systems with Applications</author><pubDate>Sun, 11 Jan 2026 13:08:59 GMT</pubDate><guid isPermaLink="true">https://www.sciencedirect.com/science/article/pii/S095741742600031X</guid></item><item><title>[PR] MFSF&lt;sup&gt;2&lt;/sup&gt;-NET: towards improved skin lesion diagnosis via multi-scale and multi-frequency-domain fusion</title><link>https://www.sciencedirect.com/science/article/pii/S003132032501581X?dgcid=rss_sd_all</link><description>&lt;p&gt;Publication date: June 2026&lt;/p&gt;&lt;p&gt;&lt;b&gt;Source:&lt;/b&gt; Pattern Recognition, Volume 174&lt;/p&gt;&lt;p&gt;Author(s): Hui Liu, Yibo Dou, Meng Cao, Kai Wang, Yunmin Zou, Guoxiang Ma, Chaohui Li&lt;/p&gt;</description><author>ScienceDirect Publication: Pattern Recognition</author><pubDate>Sun, 11 Jan 2026 13:08:58 GMT</pubDate><guid isPermaLink="true">https://www.sciencedirect.com/science/article/pii/S003132032501581X</guid></item><item><title>[PR] Efficient breast cancer segmentation via Brownian Bridge diffusion with semantic fusion strategy</title><link>https://www.sciencedirect.com/science/article/pii/S0031320325016565?dgcid=rss_sd_all</link><description>&lt;p&gt;Publication date: June 2026&lt;/p&gt;&lt;p&gt;&lt;b&gt;Source:&lt;/b&gt; Pattern Recognition, Volume 174&lt;/p&gt;&lt;p&gt;Author(s): Feiyan Feng, Tianyu Liu, Fulin Zheng, Yanshen Sun, Hong Wang&lt;/p&gt;</description><author>ScienceDirect Publication: Pattern Recognition</author><pubDate>Sun, 11 Jan 2026 13:08:58 GMT</pubDate><guid isPermaLink="true">https://www.sciencedirect.com/science/article/pii/S0031320325016565</guid></item><item><title>[PR] Empowering 2D neural network for 3D medical image segmentation via neighborhood information fusion</title><link>https://www.sciencedirect.com/science/article/pii/S003132032501698X?dgcid=rss_sd_all</link><description>&lt;p&gt;Publication date: July 2026&lt;/p&gt;&lt;p&gt;&lt;b&gt;Source:&lt;/b&gt; Pattern Recognition, Volume 175&lt;/p&gt;&lt;p&gt;Author(s): Qiankun Li, Xiaolong Huang, Yani Zhang, Bo Fang, Duo Hong, Junxin Chen&lt;/p&gt;</description><author>ScienceDirect Publication: Pattern Recognition</author><pubDate>Sun, 11 Jan 2026 13:08:58 GMT</pubDate><guid isPermaLink="true">https://www.sciencedirect.com/science/article/pii/S003132032501698X</guid></item><item><title>[MedIA] SicTTA: Single image continual test time adaptation for medical image segmentation</title><link>https://www.sciencedirect.com/science/article/pii/S1361841525004050?dgcid=rss_sd_all</link><description>&lt;p&gt;Publication date: February 2026&lt;/p&gt;&lt;p&gt;&lt;b&gt;Source:&lt;/b&gt; Medical Image Analysis, Volume 108&lt;/p&gt;&lt;p&gt;Author(s): Jianghao Wu, Xinya Liu, Guotai Wang, Shaoting Zhang&lt;/p&gt;</description><author>ScienceDirect Publication: Medical Image Analysis</author><pubDate>Sun, 11 Jan 2026 13:08:58 GMT</pubDate><guid isPermaLink="true">https://www.sciencedirect.com/science/article/pii/S1361841525004050</guid></item><item><title>[MedIA] Adaptive mix for semi-supervised medical image segmentation</title><link>https://www.sciencedirect.com/science/article/pii/S1361841525004037?dgcid=rss_sd_all</link><description>&lt;p&gt;Publication date: February 2026&lt;/p&gt;&lt;p&gt;&lt;b&gt;Source:&lt;/b&gt; Medical Image Analysis, Volume 108&lt;/p&gt;&lt;p&gt;Author(s): Zhiqiang Shen, Peng Cao, Junming Su, Jinzhu Yang, Osmar R. Zaiane&lt;/p&gt;</description><author>ScienceDirect Publication: Medical Image Analysis</author><pubDate>Sun, 11 Jan 2026 13:08:58 GMT</pubDate><guid isPermaLink="true">https://www.sciencedirect.com/science/article/pii/S1361841525004037</guid></item><item><title>[MedIA] Test-time generative augmentation for medical image segmentation</title><link>https://www.sciencedirect.com/science/article/pii/S1361841525004487?dgcid=rss_sd_all</link><description>&lt;p&gt;Publication date: March 2026&lt;/p&gt;&lt;p&gt;&lt;b&gt;Source:&lt;/b&gt; Medical Image Analysis, Volume 109&lt;/p&gt;&lt;p&gt;Author(s): Xiao Ma, Yuhui Tao, Zetian Zhang, Yuhan Zhang, Xi Wang, Sheng Zhang, Zexuan Ji, Yizhe Zhang, Qiang Chen, Guang Yang&lt;/p&gt;</description><author>ScienceDirect Publication: Medical Image Analysis</author><pubDate>Sun, 11 Jan 2026 13:08:58 GMT</pubDate><guid isPermaLink="true">https://www.sciencedirect.com/science/article/pii/S1361841525004487</guid></item><item><title>[MedIA] CHAP: Channel-spatial hierarchical adversarial perturbation for semi-supervised medical image segmentation</title><link>https://www.sciencedirect.com/science/article/pii/S1361841525004645?dgcid=rss_sd_all</link><description>&lt;p&gt;Publication date: March 2026&lt;/p&gt;&lt;p&gt;&lt;b&gt;Source:&lt;/b&gt; Medical Image Analysis, Volume 109&lt;/p&gt;&lt;p&gt;Author(s): Si-Ping Zhou, Zhi-Fang Gong, Kai-Ni Wang, Ping Zhou, Yang Chen, Guang-Quan Zhou&lt;/p&gt;</description><author>ScienceDirect Publication: Medical Image Analysis</author><pubDate>Sun, 11 Jan 2026 13:08:58 GMT</pubDate><guid isPermaLink="true">https://www.sciencedirect.com/science/article/pii/S1361841525004645</guid></item><item><title>[MP] Uncertainty‐guided test‐time optimization for personalizing segmentation models in longitudinal medical imaging</title><link>https://aapm.onlinelibrary.wiley.com/doi/10.1002/mp.70206?af=R</link><description>Medical Physics, Volume 53, Issue 1, January 2026.</description><author>Wiley: Medical Physics: Table of Contents</author><pubDate>Mon, 22 Dec 2025 04:06:14 GMT</pubDate><guid isPermaLink="true">10.1002/mp.70206</guid></item><item><title>[MP] Accelerating vision foundation model for efficient medical image segmentation</title><link>https://aapm.onlinelibrary.wiley.com/doi/10.1002/mp.70193?af=R</link><description>Medical Physics, Volume 53, Issue 1, January 2026.</description><author>Wiley: Medical Physics: Table of Contents</author><pubDate>Sat, 20 Dec 2025 03:50:16 GMT</pubDate><guid isPermaLink="true">10.1002/mp.70193</guid></item><item><title>[TIP] Uncertainty-Guided Adaptive Correction for Semi-Supervised Medical Image Segmentation</title><link>http://ieeexplore.ieee.org/document/11273087</link><description>Consistent perturbation strategies have emerged as a dominant paradigm in semi-supervised medical image segmentation. Nevertheless, prevailing approaches inadequately address two critical challenges: 1) prediction errors induced by data uncertainty from distribution shifts, and 2) loss instability caused by model uncertainty in parameter generalization. To overcome these limitations, we propose an Uncertainty-Guided Adaptive Correction (UGAC) framework with three key innovations. First, we develop a dual-path uncertainty rectification mechanism that employs normalized entropy measures to detect error-prone regions in unlabeled predictions, followed by bilateral correction through confidence-weighted fusion. Second, we introduce adversarial consistency constraints that leverage labeled data to discriminate authentic segmentation patterns, effectively regularizing uncertainty propagation in unlabeled predictions through spectral normalization. Third, we architect a frequency-aware segmentation backbone through our novel Freqfusion module, which performs adaptive spectral decomposition during feature decoding to explicitly disentangle high-frequency (boundary-aware) and low-frequency (structural) components, thereby enhancing anatomical boundary sensitivity. Comprehensive evaluations on MM-WHS, BUSI, M&amp;amp;Ms and PROMISE12 datasets demonstrate UGAC’s superior performance. The proposed framework exhibits robust generalizability across CT, MRI, and ultrasound modalities, while achieving significantly lower computational complexity than baseline UNet implementations. The code will be available at https://github.com/SIGMACX/UGAC.</description><author>IEEE Transactions on Image Processing - new TOC</author><pubDate>Wed, 03 Dec 2025 13:18:43 GMT</pubDate><guid isPermaLink="true">http://ieeexplore.ieee.org/document/11273087</guid></item><item><title>[TPAMI] H2OT: Hierarchical Hourglass Tokenizer for Efficient Video Pose Transformers</title><link>http://ieeexplore.ieee.org/document/11155208</link><description>Transformers have been successfully applied in the field of video-based 3D human pose estimation. However, the high computational costs of these video pose transformers (VPTs) make them impractical on resource-constrained devices. In this paper, we present a hierarchical plug-and-play pruning-and-recovering framework, called Hierarchical Hourglass Tokenizer (H2OT), for efficient transformer-based 3D human pose estimation from videos. H2OT begins with progressively pruning pose tokens of redundant frames and ends with recovering full-length sequences, resulting in a few pose tokens in the intermediate transformer blocks and thus improving the model efficiency. It works with two key modules, namely, a Token Pruning Module (TPM) and a Token Recovering Module (TRM). TPM dynamically selects a few representative tokens to eliminate the redundancy of video frames, while TRM restores the detailed spatio-temporal information based on the selected tokens, thereby expanding the network output to the original full-length temporal resolution for fast inference. Our method is general-purpose: it can be easily incorporated into common VPT models on both seq2seq and seq2frame pipelines while effectively accommodating different token pruning and recovery strategies. In addition, our H2OT reveals that maintaining the full pose sequence is unnecessary, and a few pose tokens of representative frames can achieve both high efficiency and estimation accuracy. Extensive experiments on multiple benchmark datasets demonstrate both the effectiveness and efficiency of the proposed method.</description><author>IEEE Transactions on Pattern Analysis and Machine Intelligence - new TOC</author><pubDate>Wed, 10 Sep 2025 13:15:34 GMT</pubDate><guid isPermaLink="true">http://ieeexplore.ieee.org/document/11155208</guid></item><item><title>[JBHI] SegTom: A 3D Volumetric Medical Image Segmentation Framework for Thoracoabdominal Multi-Organ Anatomical Structures</title><link>http://ieeexplore.ieee.org/document/11151753</link><description>Accurate segmentation of thoracoabdominal anatomical structures in three-dimensional medical imaging modalities is fundamental for informed clinical decision-making across a wide array of medical disciplines. Current approaches often struggle to efficiently and comprehensively process this region’s intricate and heterogeneous anatomical information, leading to suboptimal outcomes in diagnosis, treatment planning, and disease management. To address this challenge, we introduce SegTom, a novel volumetric segmentation framework equipped with a cutting-edge SegTom Block specifically engineered to effectively capture the complex anatomical representations inherent to the thoracoabdominal region. This SegTom Block incorporates a hierarchical anatomical-representation decomposition to facilitate efficient information exchange by decomposing the computationally intensive self-attention mechanism and cost-effectively aggregating the extracted representations. Rigorous validation of SegTom across nine diverse datasets, encompassing both computed tomography (CT) and magnetic resonance imaging (MRI) modalities, consistently demonstrates high performance across a broad spectrum of anatomical structures. Specifically, SegTom achieves a mean Dice similarity coefficient (DSC) of 87.29% for cardiac segmentation on the MM-WHS MRI dataset, 83.48% for multi-organ segmentation on the BTCV abdominal CT dataset, and 92.01% for airway segmentation on a dedicated CT dataset.</description><author>IEEE Journal of Biomedical and Health Informatics - new TOC</author><pubDate>Fri, 05 Sep 2025 13:16:46 GMT</pubDate><guid isPermaLink="true">http://ieeexplore.ieee.org/document/11151753</guid></item><item><title>[TMI] EPDiff: Erasure Perception Diffusion Model for Unsupervised Anomaly Detection in Preoperative Multimodal Images</title><link>http://ieeexplore.ieee.org/document/11121881</link><description>Unsupervised anomaly detection (UAD) methods typically detect anomalies by learning and reconstructing the normative distribution. However, since anomalies constantly invade and affect their surroundings, sub-healthy areas in the junction present structural deformations that could be easily misidentified as anomalies, posing difficulties for UAD methods that solely learn the normative distribution. The use of multimodal images can facilitate to address the above challenges, as they can provide complementary information of anomalies. Therefore, this paper propose a novel method for UAD in preoperative multimodal images, called Erasure Perception Diffusion model (EPDiff). First, the Local Erasure Progressive Training (LEPT) framework is designed to better rebuild sub-healthy structures around anomalies through the diffusion model with a two-phase process. Initially, healthy images are used to capture deviation features labeled as potential anomalies. Then, these anomalies are locally erased in multimodal images to progressively learn sub-healthy structures, obtaining a more detailed reconstruction around anomalies. Second, the Global Structural Perception (GSP) module is developed in the diffusion model to realize global structural representation and correlation within images and between modalities through interactions of high-level semantic information. In addition, a training-free module, named Multimodal Attention Fusion (MAF) module, is presented for weighted fusion of anomaly maps between different modalities and obtaining binary anomaly outputs. Experimental results show that EPDiff improves the AUPRC and mDice scores by 2% and 3.9% on BraTS2021, and by 5.2% and 4.5% on Shifts over the state-of-the-art methods, which proves the applicability of EPDiff in diverse anomaly diagnosis. The code is available at https://github.com/wjiazheng/EPDiff</description><author>IEEE Transactions on Medical Imaging - new TOC</author><pubDate>Mon, 11 Aug 2025 13:16:40 GMT</pubDate><guid isPermaLink="true">http://ieeexplore.ieee.org/document/11121881</guid></item><item><title>[TMI] GM-ABS: Promptable Generalist Model Drives Active Barely Supervised Training in Specialist Model for 3D Medical Image Segmentation</title><link>http://ieeexplore.ieee.org/document/11119675</link><description>Semi-supervised learning (SSL) has greatly advanced 3D medical image segmentation by alleviating the need for intensive labeling by radiologists. While previous efforts focused on model-centric advancements, the emergence of foundational generalist models like the Segment Anything Model (SAM) is expected to reshape the SSL landscape. Although these generalists usually show performance gaps relative to previous specialists in medical imaging, they possess impressive zero-shot segmentation abilities with manual prompts. Thus, this capability could serve as “free lunch” for training specialists, offering future SSL a promising data-centric perspective, especially revolutionizing both pseudo and expert labeling strategies to enhance the data pool. In this regard, we propose the Generalist Model-driven Active Barely Supervised (GM-ABS) learning paradigm, for developing specialized 3D segmentation models under extremely limited (barely) annotation budgets, e.g., merely cross-labeling three slices per selected scan. In specific, building upon a basic mean-teacher SSL framework, GM-ABS modernizes the SSL paradigm with two key data-centric designs: (i) Specialist-generalist collaboration, where the in-training specialist leverages class-specific positional prompts derived from class prototypes to interact with the frozen class-agnostic generalist across multiple views to achieve noisy-yet-effective label augmentation. Then, the specialist robustly assimilates the augmented knowledge via noise-tolerant collaborative learning. (ii) Expert-model collaboration that promotes active cross-labeling with notably low labeling efforts. This design progressively furnishes the specialist with informative and efficient supervision via a human-in-the-loop manner, which in turn benefits the quality of class-specific prompts. Extensive experiments on three benchmark datasets highlight the promising performance of GM-ABS over recent SSL approaches under extremely constrained labeling resources.</description><author>IEEE Transactions on Medical Imaging - new TOC</author><pubDate>Thu, 07 Aug 2025 13:17:44 GMT</pubDate><guid isPermaLink="true">http://ieeexplore.ieee.org/document/11119675</guid></item><item><title>[TMI] Collaborative Learning of Augmentation and Disentanglement for Semi-Supervised Domain Generalized Medical Image Segmentation</title><link>http://ieeexplore.ieee.org/document/11115113</link><description>This paper explores a challenging yet realistic scenario: semi-supervised domain generalization (SSDG) that includes label scarcity and domain shift problems. We pinpoint that the limitations of previous SSDG methods lie in 1) neglecting the difference between domain shifts existing within a training dataset (intra-domain shift, IDS) and those occurring between training and testing datasets (cross-domain shift, CDS) and 2) overlooking the interplay between label scarcity and domain shifts, resulting in these methods merely stitching together semi-supervised learning (SSL) and domain generalization (DG) techniques. Considering these limitations, we propose a novel perspective to decompose SSDG into the combination of unsupervised domain adaptation (UDA) and DG problems. To this end, we design a causal augmentation and disentanglement framework (CausalAD) for semi-supervised domain generalized medical image segmentation. Concretely, CausalAD involves two collaborative processes: an augmentation process, which utilizes disentangled style factors to perform style augmentation for UDA, and a disentanglement process, which decouples domain-invariant (content) and domain-variant (noise and style) features for DG. Furthermore, we propose a proxy-based self-paced training strategy (ProSPT) to guide the training of CausalAD by gradually selecting unlabeled image pixels with high-quality pseudo labels in a self-paced training manner. Finally, we introduce a hierarchical structural causal model (HSCM) to explain the intuition and concept behind our method. Extensive experiments in the cross-sequence, cross-site, and cross-modality semi-supervised domain generalized medical image segmentation settings show the effectiveness of CausalAD and its superiority over the state-of-the-art. The code is available at https://github.com/Senyh/CausalAD</description><author>IEEE Transactions on Medical Imaging - new TOC</author><pubDate>Wed, 06 Aug 2025 13:17:46 GMT</pubDate><guid isPermaLink="true">http://ieeexplore.ieee.org/document/11115113</guid></item><item><title>[TMI] A Trust-Guided Approach to MR Image Reconstruction With Side Information</title><link>http://ieeexplore.ieee.org/document/11105520</link><description>Reducing MRI scan times can improve patient care and lower healthcare costs. Many acceleration methods are designed to reconstruct diagnostic-quality images from sparse k-space data, via an ill-posed or ill-conditioned linear inverse problem (LIP). To address the resulting ambiguities, it is crucial to incorporate prior knowledge into the optimization problem, e.g., in the form of regularization. Another form of prior knowledge less commonly used in medical imaging is the readily available auxiliary data (a.k.a. side information) obtained from sources other than the current acquisition. In this paper, we present the Trust-Guided Variational Network (TGVN), an end-to-end deep learning framework that effectively and reliably integrates side information into LIPs. We demonstrate its effectiveness in multi-coil, multi-contrast MRI reconstruction, where incomplete or low-SNR measurements from one contrast are used as side information to reconstruct high-quality images of another contrast from heavily under-sampled data. TGVN is robust across different contrasts, anatomies, and field strengths. Compared to baselines utilizing side information, TGVN achieves superior image quality while preserving subtle pathological features even at challenging acceleration levels, drastically speeding up acquisition while minimizing hallucinations. Source code and dataset splits are available on github.com/sodicksonlab/TGVN</description><author>IEEE Transactions on Medical Imaging - new TOC</author><pubDate>Thu, 31 Jul 2025 13:17:16 GMT</pubDate><guid isPermaLink="true">http://ieeexplore.ieee.org/document/11105520</guid></item><item><title>[TMI] Dual Cross-Image Semantic Consistency With Self-Aware Pseudo Labeling for Semi-Supervised Medical Image Segmentation</title><link>http://ieeexplore.ieee.org/document/11104231</link><description>Semi-supervised learning has proven highly effective in tackling the challenge of limited labeled training data in medical image segmentation. In general, current approaches, which rely on intra-image pixel-wise consistency training via pseudo-labeling, overlook the consistency at more comprehensive semantic levels (e.g., object region) and suffer from severe discrepancy of extracted features resulting from an imbalanced number of labeled and unlabeled data. To overcome these limitations, we present a new Dual Cross-image Semantic Consistency (DuCiSC) learning framework, for semi-supervised medical image segmentation. Concretely, beyond enforcing pixel-wise semantic consistency, DuCiSC proposes dual paradigms to encourage region-level semantic consistency across: 1) labeled and unlabeled images; and 2) labeled and fused images, by explicitly aligning their prototypes. Relying on the dual paradigms, DuCiSC can effectively establish consistent cross-image semantics via prototype representations, thereby addressing the feature discrepancy issue. Moreover, we devise a novel self-aware confidence estimation strategy to accurately select reliable pseudo labels, allowing for exploiting the training dynamics of unlabeled data. Our DuCiSC method is extensively validated on four datasets, including two popular binary benchmarks in segmenting the left atrium and pancreas, a multi-class Automatic Cardiac Diagnosis Challenge dataset, and a challenging scenario of segmenting the inferior alveolar nerve that features complicated anatomical structures, showing superior segmentation results over previous state-of-the-art approaches. Our code is publicly available at https://github.com/ShanghaiTech-IMPACT/DuCiSC</description><author>IEEE Transactions on Medical Imaging - new TOC</author><pubDate>Wed, 30 Jul 2025 13:17:43 GMT</pubDate><guid isPermaLink="true">http://ieeexplore.ieee.org/document/11104231</guid></item><item><title>[TMI] 3D Deep-Learning-Based Segmentation of Human Skin Sweat Glands and Their 3D Morphological Response to Temperature Variations</title><link>http://ieeexplore.ieee.org/document/11098795</link><description>Skin, the primary regulator of heat exchange, relies on sweat glands for thermoregulation. Alterations in sweat gland morphology play a crucial role in various pathological conditions and clinical diagnoses. Current methods for observing sweat gland morphology are limited by their two-dimensional, in vitro, and destructive nature, underscoring the urgent need for real-time, non-invasive, quantifiable technologies. We proposed a novel three-dimensional (3D) transformer-based segmentation framework, enabling quite precise 3D sweat gland segmentation from skin volume data captured by optical coherence tomography (OCT). We quantitatively reveal, for the first time, 3D sweat gland morphological changes with temperature: for instance, volume, surface area, and length increase by 42.0%, 26.4%, and 12.8% at 43°C vs. 10°C (all p &lt;0.001), while S/V ratio decreases (p =0.01). By establishing a benchmark for normal sweat gland morphology and offering a real-time, non-invasive tool for quantifying 3D structural parameters, our approach facilitates the study of individual variability and pathological changes in sweat gland morphology, contributing to advancements in dermatological research and clinical applications.</description><author>IEEE Transactions on Medical Imaging - new TOC</author><pubDate>Tue, 29 Jul 2025 13:17:02 GMT</pubDate><guid isPermaLink="true">http://ieeexplore.ieee.org/document/11098795</guid></item><item><title>[TMI] Visualization of Breast Cancer Using Contrast-Enhanced Optical Coherence Elastography Based on Tissue Heterogeneity</title><link>http://ieeexplore.ieee.org/document/11098988</link><description>By mapping the mechanical properties of tissue, elastography can improve identification of breast cancer. On the macro-scale, ultrasound elastography and magnetic resonance elastography have emerged as effective clinical methods for the diagnosis of tumors. On the micro-scale, optical coherence elastography (OCE) shows promise for intraoperative tumor margin assessment during breast-conserving surgery. Whilst several OCE studies have demonstrated strong potential, the mechanical models used require the assumption of uniaxial stress throughout the sample. However, breast tissue is heterogeneous and contains compressible features (e.g., ducts and blood vessels) and collagen-rich fibrotic features (e.g., stroma). This heterogeneity can invalidate the assumption of uniaxial stress and reduce the accuracy of OCE, often making it challenging to interpret images. Here, we demonstrate a new variant of OCE based on mapping the Euler angle, i.e., the angle between the principal compression and the loading axis induced by tissue heterogeneity, which removes the assumption of uniaxial deformation. This is enabled by a hybrid three-dimensional (3-D) displacement estimation method that combines phase-sensitive detection and complex cross-correlation, providing access to the 3-D displacement and 3-D strain tensor on the micro-scale. Through experiments on phantoms, we demonstrate that an accuracy of 98.6%, a sensitivity of 0.95° (i.e., 16.58 mrad), and a spatial resolution as high as  $36~\mu $ m can be achieved in Euler angle imaging. We demonstrate the potential of Euler angle imaging for visualization of breast cancer. Through close correspondence with histology, our results show that mapping the Euler angle provides additional contrast to both optical coherence tomography and a compression OCE technique in identifying cancer. Mapping the Euler angle in breast tissue may provide a new biomarker for intraoperative tumor margin assessment.</description><author>IEEE Transactions on Medical Imaging - new TOC</author><pubDate>Tue, 29 Jul 2025 13:17:02 GMT</pubDate><guid isPermaLink="true">http://ieeexplore.ieee.org/document/11098988</guid></item><item><title>[TMI] A Deep Learning Multimodal Fusion-Based Method for Cell and Nucleus Segmentation</title><link>http://ieeexplore.ieee.org/document/11096725</link><description>In recent years, deep learning has been widely utilized in the fields of biomedical image segmentation and cellular image analysis. Supervised deep neural networks trained on annotated data have demonstrated good performance in tasks related to cell and nucleus segmentation. However, the use of supervised models necessitates carefully constructed training data and a substantial amount of ground truth information. Unfortunately, high-quality annotated data for cellular images are scarce. To address the issue of limited datasets, we propose a cell and nucleus segmentation method based on deep learning multimodal fusion. The proposed method includes three modules: a segmentation fundamental module, a multimodal prompter module, and an object output module. This comprehensive approach enables cell and nucleus segmentation tasks to be performed without the need for retraining on new data. The segmentation fundamental module is the core of the framework, as it provides essential segmentation capabilities. By leveraging preexisting models trained on natural imagery, this module effectively performs cell segmentation by incorporating prior knowledge. The multimodal prompter module, a pretrained model, aids in combining image and textual information. It employs a data fusion technique for multiple modalities to deliver prompts that steer the network’s output, thereby avoiding the constraints inherent to single-modality approaches. The object output module combines the inputs from the preceding modules to generate the final segmentation output. The experimental validation confirms the superiority of the proposed method, which outperforms comparative methods in cell and nucleus segmentation tasks and has promise for future applications in cell tracking.</description><author>IEEE Transactions on Medical Imaging - new TOC</author><pubDate>Fri, 25 Jul 2025 13:18:29 GMT</pubDate><guid isPermaLink="true">http://ieeexplore.ieee.org/document/11096725</guid></item><item><title>[TMI] SarAdapter: Prioritizing Attention on Semantic-Aware Representative Tokens for Enhanced Medical Image Segmentation</title><link>http://ieeexplore.ieee.org/document/11089976</link><description>Transformer-based segmentation methods exhibit considerable potential in medical image analysis. However, their improved performance often comes with increased computational complexity, limiting their application in resource-constrained medical settings. Prior methods follow two independent tracks: (i) accelerating existing networks via semantic-aware routing, and (ii) optimizing token adapter design to enhance network performance. Despite directness, they encounter unavoidable defects (e.g., inflexible acceleration techniques or non-discriminative processing) limiting further improvements of quality-complexity trade-off. To address these shortcomings, we integrate these schemes by proposing the semantic-aware adapter (SarAdapter), which employs a semantic-based routing strategy, leveraging neural operators (ViT and CNN) of varying complexities. Specifically, it merges semantically similar tokens volume into low-resolution regions while preserving semantically distinct tokens as high-resolution regions. Additionally, we introduce a Mixed-adapter unit, which adaptively selects convolutional operators of varying complexities to better model regions at different scales. We evaluate our method on four medical datasets from three modalities and show that it achieves a superior balance between accuracy, model size, and efficiency. Notably, our proposed method achieves state-of-the-art segmentation quality on the Synapse dataset while reducing the number of tokens by 65.6%, signifying a substantial improvement in the efficiency of ViTs for the segmentation task.</description><author>IEEE Transactions on Medical Imaging - new TOC</author><pubDate>Tue, 22 Jul 2025 13:16:21 GMT</pubDate><guid isPermaLink="true">http://ieeexplore.ieee.org/document/11089976</guid></item><item><title>[TMI] EICSeg: Universal Medical Image Segmentation via Explicit In-Context Learning</title><link>http://ieeexplore.ieee.org/document/11090002</link><description>Deep learning models for medical image segmentation often struggle with task-specific characteristics, limiting their generalization to unseen tasks with new anatomies, labels, or modalities. Retraining or fine-tuning these models requires substantial human effort and computational resources. To address this, in-context learning (ICL) has emerged as a promising paradigm, enabling query image segmentation by conditioning on example image-mask pairs provided as prompts. Unlike previous approaches that rely on implicit modeling or non-end-to-end pipelines, we redefine the core interaction mechanism in ICL as an explicit retrieval process, termed E-ICL, benefiting from the emergence of vision foundation models (VFMs). E-ICL captures dense correspondences between queries and prompts at minimal learning cost and leverages them to dynamically weight multi-class prompt masks. Built upon E-ICL, we propose EICSeg, the first end-to-end ICL framework that integrates complementary VFMs for universal medical image segmentation. Specifically, we introduce a lightweight SD-Adapter to bridge the distinct functionalities of the VFMs, enabling more accurate segmentation predictions. To fully exploit the potential of EICSeg, we further design a scalable self-prompt training strategy and an adaptive token-to-image prompt selection mechanism, facilitating both efficient training and inference. EICSeg is trained on 47 datasets covering diverse modalities and segmentation targets. Experiments on nine unseen datasets demonstrate its strong few-shot generalization ability, achieving an average Dice score of 74.0%, outperforming existing in-context and few-shot methods by 4.5%, and reducing the gap to task-specific models to 10.8%. Even with a single prompt, EICSeg achieves a competitive average Dice score of 60.1%. Notably, it performs automatic segmentation without manual prompt engineering, delivering results comparable to interactive models while requiring minimal labeled data. Source code will be available at https://github.com/zerone-fg/EICSeg</description><author>IEEE Transactions on Medical Imaging - new TOC</author><pubDate>Tue, 22 Jul 2025 13:16:21 GMT</pubDate><guid isPermaLink="true">http://ieeexplore.ieee.org/document/11090002</guid></item><item><title>[TMI] REDNet: Reliable Evidential Discounting Network for Multi-Modality Medical Image Segmentation</title><link>http://ieeexplore.ieee.org/document/11087642</link><description>In the field of computer-aided diagnosis, particularly for tumor diseases, segmentation is a prerequisite and primary step. Multi-modality images become essential for achieving accurate segmentation, which offer critical insights beyond the limitations of single-modality data. However, different modalities and images may suffer from different types of data imperfection, such as intensity non-uniformity, motion artifact, and low quality due to hardware limitations, which challenge image segmentation algorithms. To address this challenge, we propose a Reliable Evidential Discounting Network (REDNet), which is composed of three main modules: 1) the Intra-modality Consistency Evaluation Module (ICEM) measuring the data cohesion within the same modality; 2) the Cross-modality Difference Aggregation Module (CDAM) identifing data discrepancy across modalities; 3) the Discounting Fusion Module (DFM) processing the multi-modality evidence by applying discounting strategies to fuse the data. This approach maintains segmentation accuracy by effectively integrating multi-modality evidence, while discounting the influence of lower-quality data, ensuring reliable results despite the presence of image imperfections. We evaluated REDNet on two distinct datasets, BRATS2021 and an in-house pancreas dataset from Changhai Hospital. REDNet outperforms other methods, particularly in scenarios with imperfect image sources, and achieves reliable results in multi-modality tumor segmentation.</description><author>IEEE Transactions on Medical Imaging - new TOC</author><pubDate>Mon, 21 Jul 2025 13:19:10 GMT</pubDate><guid isPermaLink="true">http://ieeexplore.ieee.org/document/11087642</guid></item><item><title>[TMI] SegMamba-V2: Long-Range Sequential Modeling Mamba for General 3-D Medical Image Segmentation</title><link>http://ieeexplore.ieee.org/document/11084842</link><description>The Transformer architecture has demonstrated remarkable results in 3D medical image segmentation due to its capability of modeling global relationships. However, it poses a significant computational burden when processing high-dimensional medical images. Mamba, as a State Space Model (SSM), has recently emerged as a notable approach for modeling long-range dependencies in sequential data. Although a substantial amount of Mamba-based research has focused on natural language and 2D image processing, few studies explore the capability of Mamba on 3D medical images. In this paper, we propose SegMamba-V2, a novel 3D medical image segmentation model, to effectively capture long-range dependencies within whole-volume features at each scale. To achieve this goal, we first devise a hierarchical scale downsampling strategy to enhance the receptive field and mitigate information loss during downsampling. Furthermore, we design a novel tri-orientated spatial Mamba block that extends the global dependency modeling process from one plane to three orthogonal planes to improve feature representation capability. Moreover, we collect and annotate a large-scale dataset (named CRC-2000) with fine-grained categories to facilitate benchmarking evaluation in 3D colorectal cancer (CRC) segmentation. We evaluate the effectiveness of our SegMamba-V2 on CRC-2000 and three other large-scale 3D medical image segmentation datasets, covering various modalities, organs, and segmentation targets. Experimental results demonstrate that our Segmamba-V2 outperforms state-of-the-art methods by a significant margin, which indicates the universality and effectiveness of the proposed model on 3D medical image segmentation tasks. The code for SegMamba-V2 is publicly available at: https://github.com/ge-xing/SegMamba-V2</description><author>IEEE Transactions on Medical Imaging - new TOC</author><pubDate>Fri, 18 Jul 2025 13:16:20 GMT</pubDate><guid isPermaLink="true">http://ieeexplore.ieee.org/document/11084842</guid></item><item><title>[TBME] Assessment of Breast Composition With a Transmission-Based Microwave Imaging System</title><link>http://ieeexplore.ieee.org/document/11059861</link><description>Breast density is a key risk factor for breast cancer, but it is typically unknown before a first mammogram. Microwave imaging, proposed for cancer detection and monitoring, offers potential for measuring the composition of the breast. Objective: Assess the potential of microwave imaging as a method for estimating breast composition via correlation with mammogram metrics. Methods: Transmission based microwave imaging was applied to a cohort of 110 participants with prior mammograms. Several techniques were developed to estimate breast composition from microwave images, including average permittivity calculation, image thresholding and segmentation, and estimation of the fraction of glandular tissue in each pixel. These measures were compared to breast density category and percent density available from mammograms. Results: Average permittivity from microwave images correlated strongly with mammogram-based metrics. For the average permittivity, statistical analysis using one-way ANOVA revealed significant group differences across the various breast density categories. Thresholding and segmentation involved more detailed analysis of the images, and showed potential as alternative approaches to differentiating between breast composition categories. Conclusions: This study represents the largest cohort of healthy participants in which microwave breast images were compared with breast composition data available from clinical imaging. The cohort is well balanced across all categories. It highlights microwave imaging as a safe, portable, and affordable tool for non-invasive breast composition assessment and early cancer risk detection. Significance: The correlation between microwave imaging and mammogram-based breast density metrics highlights the potential for microwave imaging as a novel method for assessment of breast composition.</description><author>IEEE Transactions on Biomedical Engineering - new TOC</author><pubDate>Mon, 30 Jun 2025 13:19:40 GMT</pubDate><guid isPermaLink="true">http://ieeexplore.ieee.org/document/11059861</guid></item><item><title>[TBME] Prompt Learning With Bounding Box Constraints for Medical Image Segmentation</title><link>http://ieeexplore.ieee.org/document/11049003</link><description>Pixel-wise annotations are notoriously labourious and costly to obtain in the medical domain. To mitigate this burden, weakly supervised approaches based on bounding box annotations—much easier to acquire—offer a practical alternative. Vision foundation models have recently shown noteworthy segmentation performance when provided with prompts such as points or bounding boxes. Prompt learning exploits these models by adapting them to downstream tasks and automating segmentation, thereby reducing user intervention. However, existing prompt learning approaches depend on fully annotated segmentation masks. This paper proposes a novel framework that combines the representational power of foundation models with the annotation efficiency of weakly supervised segmentation. More specifically, our approach automates prompt generation for foundation models using only bounding box annotations. Our proposed optimization scheme integrates multiple constraints derived from box annotations with pseudo-labels generated by the prompted foundation model. Extensive experiments across multi-modal datasets reveal that, using the Segment Anything Model (SAM) as backbone, our weakly supervised method achieves an average Dice score of 84.90% in a limited data setting, outperforming existing fully-supervised and weakly-supervised approaches.</description><author>IEEE Transactions on Biomedical Engineering - new TOC</author><pubDate>Tue, 24 Jun 2025 13:17:26 GMT</pubDate><guid isPermaLink="true">http://ieeexplore.ieee.org/document/11049003</guid></item></channel></rss>