<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>My Customized Papers</title><link>https://github.com/your_username/your_repo</link><description>Aggregated research papers</description><language>en-US</language><lastBuildDate>Thu, 05 Feb 2026 07:05:32 GMT</lastBuildDate><generator>rfeed v1.1.1</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>[arXiv-CV] JSynFlow: Japanese Synthesised Flowchart Visual Question Answering Dataset built with Large Language Models</title><link>https://arxiv.org/abs/2602.04142</link><description>arXiv:2602.04142v1 Announce Type: new 
Abstract: Vision and language models (VLMs) are expected to analyse complex documents, such as those containing flowcharts, through a question-answering (QA) interface. The ability to recognise and interpret these flowcharts is in high demand, as they provide valuable insights unavailable in text-only explanations. However, developing VLMs with precise flowchart understanding requires large-scale datasets of flowchart images and corresponding text, the creation of which is highly time-consuming. To address this challenge, we introduce JSynFlow, a synthesised visual QA dataset for Japanese flowcharts, generated using large language models (LLMs). Our dataset comprises task descriptions for various business occupations, the corresponding flowchart images rendered from domain-specific language (DSL) code, and related QA pairs. This paper details the dataset's synthesis procedure and demonstrates that fine-tuning with JSynFlow significantly improves VLM performance on flowchart-based QA tasks. Our dataset is publicly available at https://huggingface.co/datasets/jri-advtechlab/jsynflow.</description><author>cs.CV updates on arXiv.org</author><pubDate>Thu, 05 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.04142v1</guid></item><item><title>[arXiv-CV] An Intuitionistic Fuzzy Logic Driven UNet architecture: Application to Brain Image segmentation</title><link>https://arxiv.org/abs/2602.04227</link><description>arXiv:2602.04227v1 Announce Type: new 
Abstract: Accurate segmentation of MRI brain images is essential for image analysis, diagnosis of neuro-logical disorders and medical image computing. In the deep learning approach, the convolutional neural networks (CNNs), especially UNet, are widely applied in medical image segmentation. However, it is difficult to deal with uncertainty due to the partial volume effect in brain images. To overcome this limitation, we propose an enhanced framework, named UNet with intuitionistic fuzzy logic (IF-UNet), which incorporates intuitionistic fuzzy logic into UNet. The model processes input data in terms of membership, nonmembership, and hesitation degrees, allowing it to better address tissue ambiguity resulting from partial volume effects and boundary uncertainties. The proposed architecture is evaluated on the Internet Brain Segmentation Repository (IBSR) dataset, and its performance is computed using accuracy, Dice coefficient, and intersection over union (IoU). Experimental results confirm that IF-UNet improves segmentation quality with handling uncertainty in brain images.</description><author>cs.CV updates on arXiv.org</author><pubDate>Thu, 05 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.04227v1</guid></item><item><title>[arXiv-CV] Enabling Real-Time Colonoscopic Polyp Segmentation on Commodity CPUs via Ultra-Lightweight Architecture</title><link>https://arxiv.org/abs/2602.04381</link><description>arXiv:2602.04381v1 Announce Type: new 
Abstract: Early detection of colorectal cancer hinges on real-time, accurate polyp identification and resection. Yet current high-precision segmentation models rely on GPUs, making them impractical to deploy in primary hospitals, mobile endoscopy units, or capsule robots. To bridge this gap, we present the UltraSeg family, operating in an extreme-compression regime (&lt;0.3 M parameters). UltraSeg-108K (0.108 M parameters) is optimized for single-center data, while UltraSeg-130K (0.13 M parameters) generalizes to multi-center, multi-modal images. By jointly optimizing encoder-decoder widths, incorporating constrained dilated convolutions to enlarge receptive fields, and integrating a cross-layer lightweight fusion module, the models achieve 90 FPS on a single CPU core without sacrificing accuracy. Evaluated on seven public datasets, UltraSeg retains &gt;94% of the Dice score of a 31 M-parameter U-Net while utilizing only 0.4% of its parameters, establishing a strong, clinically viable baseline for the extreme-compression domain and offering an immediately deployable solution for resource-constrained settings. This work provides not only a CPU-native solution for colonoscopy but also a reproducible blueprint for broader minimally invasive surgical vision applications. Source code is publicly available to ensure reproducibility and facilitate future benchmarking.</description><author>cs.CV updates on arXiv.org</author><pubDate>Thu, 05 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.04381v1</guid></item><item><title>[arXiv-CV] XtraLight-MedMamba for Classification of Neoplastic Tubular Adenomas</title><link>https://arxiv.org/abs/2602.04819</link><description>arXiv:2602.04819v1 Announce Type: new 
Abstract: Accurate risk stratification of precancerous polyps during routine colonoscopy screenings is essential for lowering the risk of developing colorectal cancer (CRC). However, assessment of low-grade dysplasia remains limited by subjective histopathologic interpretation. Advancements in digital pathology and deep learning provide new opportunities to identify subtle and fine morphologic patterns associated with malignant progression that may be imperceptible to the human eye. In this work, we propose XtraLight-MedMamba, an ultra-lightweight state-space-based deep learning framework for classifying neoplastic tubular adenomas from whole-slide images (WSIs). The architecture is a blend of ConvNext based shallow feature extractor with parallel vision mamba to efficiently model both long- and short-range dependencies and image generalization. An integration of Spatial and Channel Attention Bridge (SCAB) module enhances multiscale feature extraction, while Fixed Non-Negative Orthogonal Classifier (FNOClassifier) enables substantial parameter reduction and improved generalization. The model was evaluated on a curated dataset acquired from patients with low-grade tubular adenomas, stratified into case and control cohorts based on subsequent CRC development. XtraLight-MedMamba achieved an accuracy of 97.18% and an F1-score of 0.9767 using approximately 32,000 parameters, outperforming transformer-based and conventional Mamba architectures with significantly higher model complexity.</description><author>cs.CV updates on arXiv.org</author><pubDate>Thu, 05 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.04819v1</guid></item><item><title>[arXiv-CV] LitS: A novel Neighborhood Descriptor for Point Clouds</title><link>https://arxiv.org/abs/2602.04838</link><description>arXiv:2602.04838v1 Announce Type: new 
Abstract: With the advancement of 3D scanning technologies, point clouds have become fundamental for representing 3D spatial data, with applications that span across various scientific and technological fields. Practical analysis of this data depends crucially on available neighborhood descriptors to accurately characterize the local geometries of the point cloud. This paper introduces LitS, a novel neighborhood descriptor for 2D and 3D point clouds. LitS are piecewise constant functions on the unit circle that allow points to keep track of their surroundings. Each element in LitS' domain represents a direction with respect to a local reference system. Once constructed, evaluating LitS at any given direction gives us information about the number of neighbors in a cone-like region centered around that same direction. Thus, LitS conveys a lot of information about the local neighborhood of a point, which can be leveraged to gain global structural understanding by analyzing how LitS changes between close points. In addition, LitS comes in two versions ('regular' and 'cumulative') and has two parameters, allowing them to adapt to various contexts and types of point clouds. Overall, they are a versatile neighborhood descriptor, capable of capturing the nuances of local point arrangements and resilient to common point cloud data issues such as variable density and noise.</description><author>cs.CV updates on arXiv.org</author><pubDate>Thu, 05 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.04838v1</guid></item><item><title>[arXiv-CV] Adaptive Knowledge Transferring with Switching Dual-Student Framework for Semi-Supervised Medical Image Segmentation</title><link>https://arxiv.org/abs/2510.24366</link><description>arXiv:2510.24366v2 Announce Type: replace 
Abstract: Teacher-student frameworks have emerged as a leading approach in semi-supervised medical image segmentation, demonstrating strong performance across various tasks. However, the learning effects are still limited by the strong correlation and unreliable knowledge transfer process between teacher and student networks. To overcome this limitation, we introduce a novel switching Dual-Student architecture that strategically selects the most reliable student at each iteration to enhance dual-student collaboration and prevent error reinforcement. We also introduce a strategy of Loss-Aware Exponential Moving Average to dynamically ensure that the teacher absorbs meaningful information from students, improving the quality of pseudo-labels. Our plug-and-play framework is extensively evaluated on 3D medical image segmentation datasets, where it outperforms state-of-the-art semi-supervised methods, demonstrating its effectiveness in improving segmentation accuracy under limited supervision.</description><author>cs.CV updates on arXiv.org</author><pubDate>Thu, 05 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2510.24366v2</guid></item><item><title>[arXiv-CV] Quantization-Aware Neuromorphic Architecture for Skin Disease Classification on Resource-Constrained Devices</title><link>https://arxiv.org/abs/2507.15958</link><description>arXiv:2507.15958v4 Announce Type: replace-cross 
Abstract: On-device skin lesion analysis is constrained by the compute and energy cost of conventional CNN inference and by the need to update models as new patient data become available. Neuromorphic processors provide event-driven sparse computation and support on-chip incremental learning, yet deployment is often hindered by CNN-to-SNN conversion failures, including non-spike-compatible operators and accuracy degradation under class imbalance. We propose QANA, a quantization-aware CNN backbone embedded in an end-to-end pipeline engineered for conversion-stable neuromorphic execution. QANA replaces conversion-fragile components with spike-compatible transformations by bounding intermediate activations and aligning normalization with low-bit quantization, reducing conversion-induced distortion that disproportionately impacts rare classes. Efficiency is achieved through Ghost-based feature generation under tight FLOP budgets, while spatially-aware efficient channel attention and squeeze-and-excitation recalibrate channels without heavy global operators that are difficult to map to spiking cores. The resulting quantized projection head produces SNN-ready logits and enables incremental updates on edge hardware without full retraining or data offloading. On HAM10000, QANA achieves 91.6% Top-1 accuracy and 91.0% macro F1, improving the strongest converted SNN baseline by 3.5 percentage points in Top-1 accuracy (a 4.0% relative gain) and by 12.0 points in macro F1 (a 15.2% relative gain). On a clinical dataset, QANA achieves 90.8% Top-1 accuracy and 81.7% macro F1, improving the strongest converted SNN baseline by 3.2 points in Top-1 accuracy (a 3.7% relative gain) and by 3.6 points in macro F1 (a 4.6% relative gain). When deployed on BrainChip Akida, QANA runs in 1.5 ms per image with 1.7 mJ per image, corresponding to 94.6% lower latency and 99.0% lower energy than its GPU-based CNN implementation.</description><author>cs.CV updates on arXiv.org</author><pubDate>Thu, 05 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2507.15958v4</guid></item><item><title>[arXiv-IV] Rethinking domain generalization in medical image segmentation: One image as one domain</title><link>https://arxiv.org/abs/2501.04741</link><description>arXiv:2501.04741v5 Announce Type: replace 
Abstract: Domain shifts in medical image segmentation, particularly when data comes from different centers, pose significant challenges. Intra-center variability, such as differences in scanner models or imaging protocols, can cause domain shifts as large as, or even larger than, those between centers. To address this, we propose the "one image as one domain" (OIOD) hypothesis, which treats each image as a unique domain, enabling flexible and robust domain generalization. Based on this hypothesis, we develop a unified disentanglement-based domain generalization (UniDDG) framework, which simultaneously handles both multi-source and single-source domain generalization without requiring explicit domain labels. This approach simplifies training with a fixed architecture, independent of the number of source domains, reducing complexity and enhancing scalability. We decouple each input image into content representation and style code, then exchange and combine these within the batch for segmentation, reconstruction, and further disentanglement. By maintaining distinct style codes for each image, our model ensures thorough decoupling of content representations and style codes, improving domain invariance of the content representations. Additionally, we enhance generalization with expansion mask attention (EMA) for boundary preservation and style augmentation (SA) to simulate diverse image styles, improving robustness to domain shifts. Extensive experiments show that our method achieves Dice scores of 84.43% and 88.91% for multi-source to single-center and single-center generalization in optic disc and optic cup segmentation, respectively, and 86.96% and 88.56% for prostate segmentation, outperforming current state-of-the-art domain generalization methods, offering superior performance and adaptability across clinical settings.</description><author>eess.IV updates on arXiv.org</author><pubDate>Thu, 05 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2501.04741v5</guid></item><item><title>[arXiv-IV] Physics-Based Learning of the Wave Speed Landscape in Complex Media</title><link>https://arxiv.org/abs/2602.03281</link><description>arXiv:2602.03281v2 Announce Type: replace-cross 
Abstract: Wave velocity is a key parameter for imaging complex media, but in vivo measurements are typically limited to reflection geometries, where only backscattered waves from short-scale heterogeneities are accessible. As a result, conventional reflection imaging fails to recover large-scale variations of the wave velocity landscape. Here we show that matrix imaging overcomes this limitation by exploiting the quality of wave focusing as an intrinsic guide star. We model wave propagation as a trainable multi-layer network that leverages optimization and deep learning tools to infer the wave velocity distribution. We validate this approach through ultrasound experiments on tissue-mimicking phantoms and human breast tissues, demonstrating its potential for tumour detection and characterization. Our method is broadly applicable to any kind of waves and media for which a reflection matrix can be measured.</description><author>eess.IV updates on arXiv.org</author><pubDate>Thu, 05 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.03281v2</guid></item><item><title>[arXiv-ML] Finding Structure in Continual Learning</title><link>https://arxiv.org/abs/2602.04555</link><description>arXiv:2602.04555v1 Announce Type: new 
Abstract: Learning from a stream of tasks usually pits plasticity against stability: acquiring new knowledge often causes catastrophic forgetting of past information. Most methods address this by summing competing loss terms, creating gradient conflicts that are managed with complex and often inefficient strategies such as external memory replay or parameter regularization. We propose a reformulation of the continual learning objective using Douglas-Rachford Splitting (DRS). This reframes the learning process not as a direct trade-off, but as a negotiation between two decoupled objectives: one promoting plasticity for new tasks and the other enforcing stability of old knowledge. By iteratively finding a consensus through their proximal operators, DRS provides a more principled and stable learning dynamic. Our approach achieves an efficient balance between stability and plasticity without the need for auxiliary modules or complex add-ons, providing a simpler yet more powerful paradigm for continual learning systems.</description><author>cs.LG updates on arXiv.org</author><pubDate>Thu, 05 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.04555v1</guid></item><item><title>[arXiv-ML] Online unsupervised Hebbian learning in deep photonic neuromorphic networks</title><link>https://arxiv.org/abs/2601.22300</link><description>arXiv:2601.22300v1 Announce Type: cross 
Abstract: While software implementations of neural networks have driven significant advances in computation, the von Neumann architecture imposes fundamental limitations on speed and energy efficiency. Neuromorphic networks, with structures inspired by the brain's architecture, offer a compelling solution with the potential to approach the extreme energy efficiency of neurobiological systems. Photonic neuromorphic networks (PNNs) are particularly attractive because they leverage the inherent advantages of light, namely high parallelism, low latency, and exceptional energy efficiency. Previous PNN demonstrations have largely focused on device-level functionalities or system-level implementations reliant on supervised learning and inefficient optical-electrical-optical (OEO) conversions. Here, we introduce a purely photonic deep PNN architecture that enables online, unsupervised learning. We propose a local feedback mechanism operating entirely in the optical domain that implements a Hebbian learning rule using non-volatile phase-change material synapses. We experimentally demonstrate this approach on a non-trivial letter recognition task using a commercially available fiber-optic platform and achieve a 100 percent recognition rate, showcasing an all-optical solution for efficient, real-time information processing. This work unlocks the potential of photonic computing for complex artificial intelligence applications by enabling direct, high-throughput processing of optical information without intermediate OEO signal conversions.</description><author>cs.LG updates on arXiv.org</author><pubDate>Thu, 05 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2601.22300v1</guid></item><item><title>[arXiv-ML] Efficient Subgroup Analysis via Optimal Trees with Global Parameter Fusion</title><link>https://arxiv.org/abs/2602.04077</link><description>arXiv:2602.04077v1 Announce Type: cross 
Abstract: Identifying and making statistical inferences on differential treatment effects (commonly known as subgroup analysis in clinical research) is central to precision health. Subgroup analysis allows practitioners to pinpoint populations for whom a treatment is especially beneficial or protective, thereby advancing targeted interventions. Tree based recursive partitioning methods are widely used for subgroup analysis due to their interpretability. Nevertheless, these approaches encounter significant limitations, including suboptimal partitions induced by greedy heuristics and overfitting from locally estimated splits, especially under limited sample sizes. To address these limitations, we propose a fused optimal causal tree method that leverages mixed integer optimization (MIO) to facilitate precise subgroup identification. Our approach ensures globally optimal partitions and introduces a parameter fusion constraint to facilitate information sharing across related subgroups. This design substantially improves subgroup discovery accuracy and enhances statistical efficiency. We provide theoretical guarantees by rigorously establishing out of sample risk bounds and comparing them with those of classical tree based methods. Empirically, our method consistently outperforms popular baselines in simulations. Finally, we demonstrate its practical utility through a case study on the Health and Aging Brain Study Health Disparities (HABS-HD) dataset, where our approach yields clinically meaningful insights.</description><author>cs.LG updates on arXiv.org</author><pubDate>Thu, 05 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.04077v1</guid></item><item><title>[arXiv-ML] Robust inverse material design with physical guarantees using the Voigt-Reuss Net</title><link>https://arxiv.org/abs/2511.11388</link><description>arXiv:2511.11388v2 Announce Type: replace 
Abstract: We propose a spectrally normalized surrogate for forward and inverse mechanical homogenization with hard physical guarantees. Leveraging the Voigt-Reuss bounds, we factor their difference via a Cholesky-like operator and learn a dimensionless, symmetric positive semi-definite representation with eigenvalues in $[0,1]$; the inverse map returns symmetric positive-definite predictions that lie between the bounds in the L\"owner sense. In 3D linear elasticity on an open dataset of stochastic biphasic microstructures, a fully connected Voigt-Reuss net trained on $&gt;\!7.5\times 10^{5}$ FFT-based labels with 236 isotropy-invariant descriptors and three contrast parameters recovers the isotropic projection with near-perfect fidelity (isotropy-related entries: $R^2 \ge 0.998$), while anisotropy-revealing couplings are unidentifiable from $SO(3)$-invariant inputs. Tensor-level relative Frobenius errors have median $\approx 1.7\%$ and mean $\approx 3.4\%$ across splits. For 2D plane strain on thresholded trigonometric microstructures, coupling spectral normalization with a differentiable renderer and a CNN yields $R^2&gt;0.99$ on all components, subpercent normalized losses, accurate tracking of percolation-induced eigenvalue jumps, and robust generalization to out-of-distribution images. Treating the parametric microstructure as design variables, batched first-order optimization with a single surrogate matches target tensors within a few percent and returns diverse near-optimal designs. Overall, the Voigt-Reuss net unifies accurate, physically admissible forward prediction with large-batch, constraint-consistent inverse design, and is generic to elliptic operators and coupled-physics settings.</description><author>cs.LG updates on arXiv.org</author><pubDate>Thu, 05 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2511.11388v2</guid></item><item><title>[arXiv-ML] Guardrailed Uplift Targeting: A Causal Optimization Playbook for Marketing Strategy</title><link>https://arxiv.org/abs/2512.19805</link><description>arXiv:2512.19805v2 Announce Type: replace 
Abstract: This paper introduces a marketing decision framework that optimizes customer targeting by integrating heterogeneous treatment effect estimation with explicit business guardrails. The objective is to maximize revenue and retention while adhering to constraints such as budget, revenue protection, and customer experience. The framework first estimates Conditional Average Treatment Effects (CATE) using uplift learners, then solves a constrained allocation problem to decide whom to target and which offer to deploy. It supports decisions in retention messaging, event rewards, and spend-threshold assignment. Validated through offline simulations and online A/B tests, the approach consistently outperforms propensity and static baselines, offering a reusable playbook for causal targeting at scale.</description><author>cs.LG updates on arXiv.org</author><pubDate>Thu, 05 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2512.19805v2</guid></item><item><title>[BSPC] LGPS: A lightweight GAN-based approach for polyp segmentation in colonoscopy images</title><link>https://www.sciencedirect.com/science/article/pii/S1746809426003319?dgcid=rss_sd_all</link><description>&lt;p&gt;Publication date: 1 June 2026&lt;/p&gt;&lt;p&gt;&lt;b&gt;Source:&lt;/b&gt; Biomedical Signal Processing and Control, Volume 118&lt;/p&gt;&lt;p&gt;Author(s): Fiseha Berhanu Tesema, Alejandro Guerra-Manzanares, Tianxiang Cui, Qian Zhang, Moses M. Solomon, Xiangjian He&lt;/p&gt;</description><author>ScienceDirect Publication: Biomedical Signal Processing and Control</author><pubDate>Thu, 05 Feb 2026 02:32:38 GMT</pubDate><guid isPermaLink="true">https://www.sciencedirect.com/science/article/pii/S1746809426003319</guid></item><item><title>[ESWA] Assessment of critical success factors for Lean Six Sigma in food businesses using Pythagorean fuzzy information</title><link>https://www.sciencedirect.com/science/article/pii/S0957417426002290?dgcid=rss_sd_all</link><description>&lt;p&gt;Publication date: 15 May 2026&lt;/p&gt;&lt;p&gt;&lt;b&gt;Source:&lt;/b&gt; Expert Systems with Applications, Volume 311&lt;/p&gt;&lt;p&gt;Author(s): Naif Almakayeel, Arunodaya Raj Mishra, Pratibha Rani, Sayyedeh Parisa Saeidi, Erfan Babaee Tirkolaee&lt;/p&gt;</description><author>ScienceDirect Publication: Expert Systems with Applications</author><pubDate>Wed, 04 Feb 2026 18:57:42 GMT</pubDate><guid isPermaLink="true">https://www.sciencedirect.com/science/article/pii/S0957417426002290</guid></item><item><title>[MedIA] SAM-Driven Cross Prompting with Adaptive Sampling Consistency for Semi-supervised Medical Image Segmentation</title><link>https://www.sciencedirect.com/science/article/pii/S1361841526000423?dgcid=rss_sd_all</link><description>&lt;p&gt;Publication date: Available online 3 February 2026&lt;/p&gt;&lt;p&gt;&lt;b&gt;Source:&lt;/b&gt; Medical Image Analysis&lt;/p&gt;&lt;p&gt;Author(s): Juzheng Miao, Cheng Chen, Yuchen Yuan, Quanzheng Li, Pheng-Ann Heng&lt;/p&gt;</description><author>ScienceDirect Publication: Medical Image Analysis</author><pubDate>Wed, 04 Feb 2026 18:57:35 GMT</pubDate><guid isPermaLink="true">https://www.sciencedirect.com/science/article/pii/S1361841526000423</guid></item><item><title>[CBM] A semi-automated modelling pipeline to predict the mechanics of multiple sclerosis lesion afflicted brains from magnetic resonance images</title><link>https://www.sciencedirect.com/science/article/pii/S0010482526000806?dgcid=rss_sd_all</link><description>&lt;p&gt;Publication date: March 2026&lt;/p&gt;&lt;p&gt;&lt;b&gt;Source:&lt;/b&gt; Computers in Biology and Medicine, Volume 204&lt;/p&gt;&lt;p&gt;Author(s): Adam C Szekely-Kohn, Diana Cruz De Oliveira, Marco Castellani, Michael Douglas, Zubair Ahmed, Daniel M Espino&lt;/p&gt;</description><author>ScienceDirect Publication: Computers in Biology and Medicine</author><pubDate>Wed, 04 Feb 2026 13:11:14 GMT</pubDate><guid isPermaLink="true">https://www.sciencedirect.com/science/article/pii/S0010482526000806</guid></item><item><title>[NC] ADML: Asymmetric deformation-guided mutual learning for semi-supervised medical image segmentation</title><link>https://www.sciencedirect.com/science/article/pii/S0925231226003371?dgcid=rss_sd_all</link><description>&lt;p&gt;Publication date: Available online 3 February 2026&lt;/p&gt;&lt;p&gt;&lt;b&gt;Source:&lt;/b&gt; Neurocomputing&lt;/p&gt;&lt;p&gt;Author(s): Yu Peng, Haoyu Zou, Kehu Yang, Qingqun Kong&lt;/p&gt;</description><author>ScienceDirect Publication: Neurocomputing</author><pubDate>Wed, 04 Feb 2026 13:11:14 GMT</pubDate><guid isPermaLink="true">https://www.sciencedirect.com/science/article/pii/S0925231226003371</guid></item><item><title>[arXiv-CV] SRA-Seg: Synthetic to Real Alignment for Semi-Supervised Medical Image Segmentation</title><link>https://arxiv.org/abs/2602.02944</link><description>arXiv:2602.02944v1 Announce Type: new 
Abstract: Synthetic data, an appealing alternative to extensive expert-annotated data for medical image segmentation, consistently fails to improve segmentation performance despite its visual realism. The reason being that synthetic and real medical images exist in different semantic feature spaces, creating a domain gap that current semi-supervised learning methods cannot bridge. We propose SRA-Seg, a framework explicitly designed to align synthetic and real feature distributions for medical image segmentation. SRA-Seg introduces a similarity-alignment (SA) loss using frozen DINOv2 embeddings to pull synthetic representations toward their nearest real counterparts in semantic space. We employ soft edge blending to create smooth anatomical transitions and continuous labels, eliminating the hard boundaries from traditional copy-paste augmentation. The framework generates pseudo-labels for synthetic images via an EMA teacher model and applies soft-segmentation losses that respect uncertainty in mixed regions. Our experiments demonstrate strong results: using only 10% labeled real data and 90% synthetic unlabeled data, SRA-Seg achieves 89.34% Dice on ACDC and 84.42% on FIVES, significantly outperforming existing semi-supervised methods and matching the performance of methods using real unlabeled data.</description><author>cs.CV updates on arXiv.org</author><pubDate>Wed, 04 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.02944v1</guid></item><item><title>[arXiv-CV] Fully Kolmogorov-Arnold Deep Model in Medical Image Segmentation</title><link>https://arxiv.org/abs/2602.03156</link><description>arXiv:2602.03156v1 Announce Type: new 
Abstract: Deeply stacked KANs are practically impossible due to high training difficulties and substantial memory requirements. Consequently, existing studies can only incorporate few KAN layers, hindering the comprehensive exploration of KANs. This study overcomes these limitations and introduces the first fully KA-based deep model, demonstrating that KA-based layers can entirely replace traditional architectures in deep learning and achieve superior learning capacity. Specifically, (1) the proposed Share-activation KAN (SaKAN) reformulates Sprecher's variant of Kolmogorov-Arnold representation theorem, which achieves better optimization due to its simplified parameterization and denser training samples, to ease training difficulty, (2) this paper indicates that spline gradients contribute negligibly to training while consuming huge GPU memory, thus proposes the Grad-Free Spline to significantly reduce memory usage and computational overhead. (3) Building on these two innovations, our ALL U-KAN is the first representative implementation of fully KA-based deep model, where the proposed KA and KAonv layers completely replace FC and Conv layers. Extensive evaluations on three medical image segmentation tasks confirm the superiority of the full KA-based architecture compared to partial KA-based and traditional architectures, achieving all higher segmentation accuracy. Compared to directly deeply stacked KAN, ALL U-KAN achieves 10 times reduction in parameter count and reduces memory consumption by more than 20 times, unlocking the new explorations into deep KAN architectures.</description><author>cs.CV updates on arXiv.org</author><pubDate>Wed, 04 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.03156v1</guid></item><item><title>[arXiv-CV] LEVIO: Lightweight Embedded Visual Inertial Odometry for Resource-Constrained Devices</title><link>https://arxiv.org/abs/2602.03294</link><description>arXiv:2602.03294v1 Announce Type: new 
Abstract: Accurate, infrastructure-less sensor systems for motion tracking are essential for mobile robotics and augmented reality (AR) applications. The most popular state-of-the-art visual-inertial odometry (VIO) systems, however, are too computationally demanding for resource-constrained hardware, such as micro-drones and smart glasses. This work presents LEVIO, a fully featured VIO pipeline optimized for ultra-low-power compute platforms, allowing six-degrees-of-freedom (DoF) real-time sensing. LEVIO incorporates established VIO components such as Oriented FAST and Rotated BRIEF (ORB) feature tracking and bundle adjustment, while emphasizing a computationally efficient architecture with parallelization and low memory usage to suit embedded microcontrollers and low-power systems-on-chip (SoCs). The paper proposes and details the algorithmic design choices and the hardware-software co-optimization approach, and presents real-time performance on resource-constrained hardware. LEVIO is validated on a parallel-processing ultra-low-power RISC-V SoC, achieving 20 FPS while consuming less than 100 mW, and benchmarked against public VIO datasets, offering a compelling balance between efficiency and accuracy. To facilitate reproducibility and adoption, the complete implementation is released as open-source.</description><author>cs.CV updates on arXiv.org</author><pubDate>Wed, 04 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.03294v1</guid></item><item><title>[arXiv-CV] MedSAM-Agent: Empowering Interactive Medical Image Segmentation with Multi-turn Agentic Reinforcement Learning</title><link>https://arxiv.org/abs/2602.03320</link><description>arXiv:2602.03320v1 Announce Type: new 
Abstract: Medical image segmentation is evolving from task-specific models toward generalizable frameworks. Recent research leverages Multi-modal Large Language Models (MLLMs) as autonomous agents, employing reinforcement learning with verifiable reward (RLVR) to orchestrate specialized tools like the Segment Anything Model (SAM). However, these approaches often rely on single-turn, rigid interaction strategies and lack process-level supervision during training, which hinders their ability to fully exploit the dynamic potential of interactive tools and leads to redundant actions. To bridge this gap, we propose MedSAM-Agent, a framework that reformulates interactive segmentation as a multi-step autonomous decision-making process. First, we introduce a hybrid prompting strategy for expert-curated trajectory generation, enabling the model to internalize human-like decision heuristics and adaptive refinement strategies. Furthermore, we develop a two-stage training pipeline that integrates multi-turn, end-to-end outcome verification with a clinical-fidelity process reward design to promote interaction parsimony and decision efficiency. Extensive experiments across 6 medical modalities and 21 datasets demonstrate that MedSAM-Agent achieves state-of-the-art performance, effectively unifying autonomous medical reasoning with robust, iterative optimization. Code is available \href{https://github.com/CUHK-AIM-Group/MedSAM-Agent}{here}.</description><author>cs.CV updates on arXiv.org</author><pubDate>Wed, 04 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.03320v1</guid></item><item><title>[arXiv-CV] EEO-TFV: Escape-Explore Optimizer for Web-Scale Time-Series Forecasting and Vision Analysis</title><link>https://arxiv.org/abs/2602.02551</link><description>arXiv:2602.02551v1 Announce Type: cross 
Abstract: Transformer-based foundation models have achieved remarkable progress in tasks such as time-series forecasting and image segmentation. However, they frequently suffer from error accumulation in multivariate long-sequence prediction and exhibit vulnerability to out-of-distribution samples in image-related tasks. Furthermore, these challenges become particularly pronounced in large-scale Web data analysis tasks, which typically involve complex temporal patterns and multimodal features. This complexity substantially increases optimization difficulty, rendering models prone to stagnation at saddle points within high-dimensional parameter spaces. To address these issues, we propose a lightweight Transformer architecture in conjunction with a novel Escape-Explore Optimizer (EEO). The optimizer enhances both exploration and generalization while effectively avoiding sharp minima and saddle-point traps. Experimental results show that, in representative Web data scenarios, our method achieves performance on par with state-of-the-art models across 11 time-series benchmark datasets and the Synapse medical image segmentation task. Moreover, it demonstrates superior generalization and stability, thereby validating its potential as a versatile cross-task foundation model for Web-scale data mining and analysis.</description><author>cs.CV updates on arXiv.org</author><pubDate>Wed, 04 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.02551v1</guid></item><item><title>[arXiv-CV] A Random Matrix Theory Perspective on the Consistency of Diffusion Models</title><link>https://arxiv.org/abs/2602.02908</link><description>arXiv:2602.02908v1 Announce Type: cross 
Abstract: Diffusion models trained on different, non-overlapping subsets of a dataset often produce strikingly similar outputs when given the same noise seed. We trace this consistency to a simple linear effect: the shared Gaussian statistics across splits already predict much of the generated images. To formalize this, we develop a random matrix theory (RMT) framework that quantifies how finite datasets shape the expectation and variance of the learned denoiser and sampling map in the linear setting. For expectations, sampling variability acts as a renormalization of the noise level through a self-consistent relation $\sigma^2 \mapsto \kappa(\sigma^2)$, explaining why limited data overshrink low-variance directions and pull samples toward the dataset mean. For fluctuations, our variance formulas reveal three key factors behind cross-split disagreement: \textit{anisotropy} across eigenmodes, \textit{inhomogeneity} across inputs, and overall scaling with dataset size. Extending deterministic-equivalence tools to fractional matrix powers further allows us to analyze entire sampling trajectories. The theory sharply predicts the behavior of linear diffusion models, and we validate its predictions on UNet and DiT architectures in their non-memorization regime, identifying where and how samples deviates across training data split. This provides a principled baseline for reproducibility in diffusion training, linking spectral properties of data to the stability of generative outputs.</description><author>cs.CV updates on arXiv.org</author><pubDate>Wed, 04 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.02908v1</guid></item><item><title>[arXiv-CV] Rethinking Bottlenecks in Safety Fine-Tuning of Vision Language Models</title><link>https://arxiv.org/abs/2501.18533</link><description>arXiv:2501.18533v3 Announce Type: replace 
Abstract: Large Vision-Language Models (VLMs) have achieved remarkable performance across a wide range of tasks. However, their deployment in safety-critical domains poses significant challenges. Existing safety fine-tuning methods, which focus on textual or multimodal content, fall short in addressing challenging cases or disrupt the balance between helpfulness and harmlessness. Our evaluation highlights a safety reasoning gap: these methods lack safety visual reasoning ability, leading to such bottlenecks. To address this limitation and enhance both visual perception and reasoning in safety-critical contexts, we propose a novel dataset that integrates multi-image inputs with safety Chain-of-Thought (CoT) labels as fine-grained reasoning logic to improve model performance. Specifically, we introduce the Multi-Image Safety (MIS) dataset, an instruction-following dataset tailored for multi-image safety scenarios, consisting of training and test splits. Our experiments demonstrate that fine-tuning InternVL2.5-8B with MIS significantly outperforms both powerful open-source models and API-based models in challenging multi-image tasks requiring safety-related visual reasoning. This approach not only delivers exceptional safety performance but also preserves general capabilities without any trade-offs. Specifically, fine-tuning with MIS increases average accuracy by 0.83% across five general benchmarks and reduces the Attack Success Rate (ASR) on multiple safety benchmarks by a large margin.</description><author>cs.CV updates on arXiv.org</author><pubDate>Wed, 04 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2501.18533v3</guid></item><item><title>[arXiv-CV] Material-informed Gaussian Splatting for 3D World Reconstruction in a Digital Twin</title><link>https://arxiv.org/abs/2511.20348</link><description>arXiv:2511.20348v3 Announce Type: replace 
Abstract: 3D reconstruction for Digital Twins often relies on LiDAR-based methods, which provide accurate geometry but lack the semantics and textures naturally captured by cameras. Traditional LiDAR-camera fusion approaches require complex calibration and still struggle with certain materials like glass, which are visible in images but poorly represented in point clouds. We propose a camera-only pipeline that reconstructs scenes using 3D Gaussian Splatting from multi-view images, extracts semantic material masks via vision models, converts Gaussian representations to mesh surfaces with projected material labels, and assigns physics-based material properties for accurate sensor simulation in modern graphics engines and simulators. This approach combines photorealistic reconstruction with physics-based material assignment, providing sensor simulation fidelity comparable to LiDAR-camera fusion while eliminating hardware complexity and calibration requirements. We validate our camera-only method using an internal dataset from an instrumented test vehicle, leveraging LiDAR as ground truth for reflectivity validation alongside image similarity metrics.</description><author>cs.CV updates on arXiv.org</author><pubDate>Wed, 04 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2511.20348v3</guid></item><item><title>[arXiv-CV] Data Augmentation for High-Fidelity Generation of CAR-T/NK Immunological Synapse Images</title><link>https://arxiv.org/abs/2602.00949</link><description>arXiv:2602.00949v2 Announce Type: replace 
Abstract: Chimeric antigen receptor (CAR)-T and NK cell immunotherapies have transformed cancer treatment, and recent studies suggest that the quality of the CAR-T/NK cell immunological synapse (IS) may serve as a functional biomarker for predicting therapeutic efficacy. Accurate detection and segmentation of CAR-T/NK IS structures using artificial neural networks (ANNs) can greatly increase the speed and reliability of IS quantification. However, a persistent challenge is the limited size of annotated microscopy datasets, which restricts the ability of ANNs to generalize. To address this challenge, we integrate two complementary data-augmentation frameworks. First, we employ Instance Aware Automatic Augmentation (IAAA), an automated, instance-preserving augmentation method that generates synthetic CAR-T/NK IS images and corresponding segmentation masks by applying optimized augmentation policies to original IS data. IAAA supports multiple imaging modalities (e.g., fluorescence and brightfield) and can be applied directly to CAR-T/NK IS images derived from patient samples. In parallel, we introduce a Semantic-Aware AI Augmentation (SAAA) pipeline that combines a diffusion-based mask generator with a Pix2Pix conditional image synthesizer. This second method enables the creation of diverse, anatomically realistic segmentation masks and produces high-fidelity CAR-T/NK IS images aligned with those masks, further expanding the training corpus beyond what IAAA alone can provide. Together, these augmentation strategies generate synthetic images whose visual and structural properties closely match real IS data, significantly improving CAR-T/NK IS detection and segmentation performance. By enhancing the robustness and accuracy of IS quantification, this work supports the development of more reliable imaging-based biomarkers for predicting patient response to CAR-T/NK immunotherapy.</description><author>cs.CV updates on arXiv.org</author><pubDate>Wed, 04 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.00949v2</guid></item><item><title>[arXiv-CV] Diff4MMLiTS: Advanced Multimodal Liver Tumor Segmentation via Diffusion-Based Image Synthesis and Alignment</title><link>https://arxiv.org/abs/2412.20418</link><description>arXiv:2412.20418v2 Announce Type: replace-cross 
Abstract: Multimodal learning has been demonstrated to enhance performance across various clinical tasks, owing to the diverse perspectives offered by different modalities of data. However, existing multimodal segmentation methods rely on well-registered multimodal data, which is unrealistic for real-world clinical images, particularly for indistinct and diffuse regions such as liver tumors. In this paper, we introduce Diff4MMLiTS, a four-stage multimodal liver tumor segmentation pipeline: pre-registration of the target organs in multimodal CTs; dilation of the annotated modality's mask and followed by its use in inpainting to obtain multimodal normal CTs without tumors; synthesis of strictly aligned multimodal CTs with tumors using the latent diffusion model based on multimodal CT features and randomly generated tumor masks; and finally, training the segmentation model, thus eliminating the need for strictly aligned multimodal data. Extensive experiments on public and internal datasets demonstrate the superiority of Diff4MMLiTS over other state-of-the-art multimodal segmentation methods.</description><author>cs.CV updates on arXiv.org</author><pubDate>Wed, 04 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2412.20418v2</guid></item><item><title>[arXiv-CV] Understanding-informed Bias Mitigation for Fair CMR Segmentation</title><link>https://arxiv.org/abs/2503.17089</link><description>arXiv:2503.17089v3 Announce Type: replace-cross 
Abstract: Artificial intelligence (AI) is increasingly being used for medical imaging tasks. However, there can be biases in AI models, particularly when they are trained using imbalanced training datasets. One such example has been the strong ethnicity bias effect in cardiac magnetic resonance (CMR) image segmentation models. Although this phenomenon has been reported in a number of publications, little is known about the effectiveness of bias mitigation algorithms in this domain. We aim to investigate the impact of common bias mitigation methods to address bias between Black and White subjects in AI-based CMR segmentation models. Specifically, we use oversampling, importance reweighing and Group DRO as well as combinations of these techniques to mitigate the ethnicity bias. Second, motivated by recent findings on the root causes of AI-based CMR segmentation bias, we evaluate the same methods using models trained and evaluated on cropped CMR images. We find that bias can be mitigated using oversampling, significantly improving performance for the underrepresented Black subjects whilst not significantly reducing the majority White subjects' performance. Using cropped images increases performance for both ethnicities and reduces the bias, whilst adding oversampling as a bias mitigation technique with cropped images reduces the bias further. When testing the models on an external clinical validation set, we find high segmentation performance and no statistically significant bias.</description><author>cs.CV updates on arXiv.org</author><pubDate>Wed, 04 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2503.17089v3</guid></item><item><title>[arXiv-IV] Physics-Based Learning of the Wave Speed Landscape in Complex Media</title><link>https://arxiv.org/abs/2602.03281</link><description>arXiv:2602.03281v1 Announce Type: cross 
Abstract: Wave velocity is a key parameter for imaging complex media, but in vivo measurements are typically limited to reflection geometries, where only backscattered waves from short-scale heterogeneities are accessible. As a result, conventional reflection imaging fails to recover large-scale variations of the wave velocity landscape. Here we show that matrix imaging overcomes this limitation by exploiting the quality of wave focusing as an intrinsic guide star. We model wave propagation as a trainable multi-layer network that leverages optimization and deep learning tools to infer the wave velocity distribution. We validate this approach through ultrasound experiments on tissue-mimicking phantoms and human breast tissues, demonstrating its potential for tumour detection and characterization. Our method is broadly applicable to any kind of waves and media for which a reflection matrix can be measured.</description><author>eess.IV updates on arXiv.org</author><pubDate>Wed, 04 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.03281v1</guid></item><item><title>[arXiv-ML] Automated Dysphagia Screening Using Noninvasive Neck Acoustic Sensing</title><link>https://arxiv.org/abs/2602.02725</link><description>arXiv:2602.02725v1 Announce Type: new 
Abstract: Pharyngeal health plays a vital role in essential human functions such as breathing, swallowing, and vocalization. Early detection of swallowing abnormalities, also known as dysphagia, is crucial for timely intervention. However, current diagnostic methods often rely on radiographic imaging or invasive procedures. In this study, we propose an automated framework for detecting dysphagia using portable and noninvasive acoustic sensing coupled with applied machine learning. By capturing subtle acoustic signals from the neck during swallowing tasks, we aim to identify patterns associated with abnormal physiological conditions. Our approach achieves promising test-time abnormality detection performance, with an AUC-ROC of 0.904 under 5 independent train-test splits. This work demonstrates the feasibility of using noninvasive acoustic sensing as a practical and scalable tool for pharyngeal health monitoring.</description><author>cs.LG updates on arXiv.org</author><pubDate>Wed, 04 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.02725v1</guid></item><item><title>[arXiv-ML] A Positive Case for Faithfulness: LLM Self-Explanations Help Predict Model Behavior</title><link>https://arxiv.org/abs/2602.02639</link><description>arXiv:2602.02639v1 Announce Type: cross 
Abstract: LLM self-explanations are often presented as a promising tool for AI oversight, yet their faithfulness to the model's true reasoning process is poorly understood. Existing faithfulness metrics have critical limitations, typically relying on identifying unfaithfulness via adversarial prompting or detecting reasoning errors. These methods overlook the predictive value of explanations. We introduce Normalized Simulatability Gain (NSG), a general and scalable metric based on the idea that a faithful explanation should allow an observer to learn a model's decision-making criteria, and thus better predict its behavior on related inputs. We evaluate 18 frontier proprietary and open-weight models, e.g., Gemini 3, GPT-5.2, and Claude 4.5, on 7,000 counterfactuals from popular datasets covering health, business, and ethics. We find self-explanations substantially improve prediction of model behavior (11-37% NSG). Self-explanations also provide more predictive information than explanations generated by external models, even when those models are stronger. This implies an advantage from self-knowledge that external explanation methods cannot replicate. Our approach also reveals that, across models, 5-15% of self-explanations are egregiously misleading. Despite their imperfections, we show a positive case for self-explanations: they encode information that helps predict model behavior.</description><author>cs.LG updates on arXiv.org</author><pubDate>Wed, 04 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.02639v1</guid></item><item><title>[arXiv-ML] Principled Federated Random Forests for Heterogeneous Data</title><link>https://arxiv.org/abs/2602.03258</link><description>arXiv:2602.03258v1 Announce Type: cross 
Abstract: Random Forests (RF) are among the most powerful and widely used predictive models for centralized tabular data, yet few methods exist to adapt them to the federated learning setting. Unlike most federated learning approaches, the piecewise-constant nature of RF prevents exact gradient-based optimization. As a result, existing federated RF implementations rely on unprincipled heuristics: for instance, aggregating decision trees trained independently on clients fails to optimize the global impurity criterion, even under simple distribution shifts. We propose FedForest, a new federated RF algorithm for horizontally partitioned data that naturally accommodates diverse forms of client data heterogeneity, from covariate shift to more complex outcome shift mechanisms. We prove that our splitting procedure, based on aggregating carefully chosen client statistics, closely approximates the split selected by a centralized algorithm. Moreover, FedForest allows splits on client indicators, enabling a non-parametric form of personalization that is absent from prior federated random forest methods. Empirically, we demonstrate that the resulting federated forests closely match centralized performance across heterogeneous benchmarks while remaining communication-efficient.</description><author>cs.LG updates on arXiv.org</author><pubDate>Wed, 04 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.03258v1</guid></item><item><title>[arXiv-ML] Sparse maximal update parameterization: A holistic approach to sparse training dynamics</title><link>https://arxiv.org/abs/2405.15743</link><description>arXiv:2405.15743v3 Announce Type: replace 
Abstract: Several challenges make it difficult for sparse neural networks to compete with dense models. First, setting a large fraction of weights to zero impairs forward and gradient signal propagation. Second, sparse studies often need to test multiple sparsity levels, while also introducing new hyperparameters (HPs), leading to prohibitive tuning costs. Indeed, the standard practice is to re-use the learning HPs originally crafted for dense models. Unfortunately, we show sparse and dense networks do not share the same optimal HPs. Without stable dynamics and effective training recipes, it is costly to test sparsity at scale, which is key to surpassing dense networks and making the business case for sparsity acceleration in hardware.
  A holistic approach is needed to tackle these challenges and we propose S$\mu$Par as one such approach. For random unstructured static sparsity, S$\mu$Par ensures activations, gradients, and weight updates all scale independently of sparsity level. Further, by reparameterizing the HPs, S$\mu$Par enables the same HP values to be optimal as we vary both sparsity level and model width. HPs can be tuned on small dense networks and transferred to large sparse models, greatly reducing tuning costs. On large-scale language modeling, S$\mu$Par shows increasing improvements over standard parameterization as sparsity increases, leading up to 11.9% relative loss improvement at 99.2% sparsity. A minimal implementation of S$\mu$Par is available at https://github.com/EleutherAI/nanoGPT-mup/tree/supar.</description><author>cs.LG updates on arXiv.org</author><pubDate>Wed, 04 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2405.15743v3</guid></item><item><title>[arXiv-ML] Sharpness-Aware Machine Unlearning</title><link>https://arxiv.org/abs/2506.13715</link><description>arXiv:2506.13715v2 Announce Type: replace 
Abstract: We characterize the effectiveness of Sharpness-aware minimization (SAM) under machine unlearning scheme, where unlearning forget signals interferes with learning retain signals. While previous work prove that SAM improves generalization with noise memorization prevention, we show that SAM abandons such denoising property when fitting the forget set, leading to altered generalization depending on signal strength. We further characterize the signal surplus of SAM in the order of signal strength, which enables learning from less retain signals to maintain model performance and putting more weight on unlearning the forget set. Empirical studies show that SAM outperforms SGD with relaxed requirement for retain signals and can enhance various unlearning methods either as pretrain or unlearn algorithm. Motivated by our refined characterization of SAM unlearning and observing that overfitting can benefit more stringent sample-specific unlearning, we propose Sharp MinMax, which splits the model into two to learn retain signals with SAM and unlearn forget signals with sharpness maximization, achieving best performance. Extensive experiments show that SAM enhances unlearning across varying difficulties measured by memorization, yielding decreased feature entanglement between retain and forget sets, stronger resistance to membership inference attacks, and a flatter loss landscape. Our observations generalize to more noised data, different optimizers, and different architectures.</description><author>cs.LG updates on arXiv.org</author><pubDate>Wed, 04 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2506.13715v2</guid></item><item><title>[arXiv-ML] NEZHA: A Zero-sacrifice and Hyperspeed Decoding Architecture for Generative Recommendations</title><link>https://arxiv.org/abs/2511.18793</link><description>arXiv:2511.18793v2 Announce Type: replace-cross 
Abstract: Generative Recommendation (GR), powered by Large Language Models (LLMs), represents a promising new paradigm for industrial recommender systems. However, their practical application is severely hindered by high inference latency, which makes them infeasible for high-throughput, real-time services and limits their overall business impact. While Speculative Decoding (SD) has been proposed to accelerate the autoregressive generation process, existing implementations introduce new bottlenecks: they typically require separate draft models and model-based verifiers, requiring additional training and increasing the latency overhead. In this paper, we address these challenges with NEZHA, a novel architecture that achieves hyperspeed decoding for GR systems without sacrificing recommendation quality. Specifically, NEZHA integrates a nimble autoregressive draft head directly into the primary model, enabling efficient self-drafting. This design, combined with a specialized input prompt structure, preserves the integrity of sequence-to-sequence generation. Furthermore, to tackle the critical problem of hallucination, a major source of performance degradation, we introduce an efficient, model-free verifier based on a hash set. We demonstrate the effectiveness of NEZHA through extensive experiments on public datasets and have successfully deployed the system on Taobao since October 2025, driving the billion-level advertising revenue and serving hundreds of millions of daily active users.</description><author>cs.LG updates on arXiv.org</author><pubDate>Wed, 04 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2511.18793v2</guid></item><item><title>[arXiv-ML] LLMs as Orchestrators: Constraint-Compliant Multi-Agent Optimization for Recommendation Systems</title><link>https://arxiv.org/abs/2601.19121</link><description>arXiv:2601.19121v3 Announce Type: replace-cross 
Abstract: Recommendation systems must optimize multiple objectives while satisfying hard business constraints such as fairness and coverage. For example, an e-commerce platform may require every recommendation list to include items from multiple sellers and at least one newly listed product; violating such constraints--even once--is unacceptable in production. Prior work on multi-objective recommendation and recent LLM-based recommender agents largely treat constraints as soft penalties or focus on item scoring and interaction, leading to frequent violations in real-world deployments. How to leverage LLMs for coordinating constrained optimization in recommendation systems remains underexplored. We propose DualAgent-Rec, an LLM-coordinated dual-agent framework for constrained multi-objective e-commerce recommendation. The framework separates optimization into an Exploitation Agent that prioritizes accuracy under hard constraints and an Exploration Agent that promotes diversity through unconstrained Pareto search. An LLM-based coordinator adaptively allocates resources between agents based on optimization progress and constraint satisfaction, while an adaptive epsilon-relaxation mechanism guarantees feasibility of final solutions. Experiments on the Amazon Reviews 2023 dataset demonstrate that DualAgent-Rec achieves 100% constraint satisfaction and improves Pareto hypervolume by 4-6% over strong baselines, while maintaining competitive accuracy-diversity trade-offs. These results indicate that LLMs can act as effective orchestration agents for deployable and constraint-compliant recommendation systems.</description><author>cs.LG updates on arXiv.org</author><pubDate>Wed, 04 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2601.19121v3</guid></item><item><title>[ESWA] FedCA: Federated domain generalization for medical image segmentation via cross-client feature style transfer and adaptive style alignment</title><link>https://www.sciencedirect.com/science/article/pii/S0957417426003076?dgcid=rss_sd_all</link><description>&lt;p&gt;Publication date: 15 May 2026&lt;/p&gt;&lt;p&gt;&lt;b&gt;Source:&lt;/b&gt; Expert Systems with Applications, Volume 311&lt;/p&gt;&lt;p&gt;Author(s): Yihan Ren, Yanfeng Li, Jia Sun, Houjin Chen, Xiaochen Ma, Bijuan Ren&lt;/p&gt;</description><author>ScienceDirect Publication: Expert Systems with Applications</author><pubDate>Tue, 03 Feb 2026 19:02:07 GMT</pubDate><guid isPermaLink="true">https://www.sciencedirect.com/science/article/pii/S0957417426003076</guid></item><item><title>[arXiv-CV] DensiThAI, A Multi-View Deep Learning Framework for Breast Density Estimation using Infrared Images</title><link>https://arxiv.org/abs/2602.00145</link><description>arXiv:2602.00145v1 Announce Type: new 
Abstract: Breast tissue density is a key biomarker of breast cancer risk and a major factor affecting mammographic sensitivity. However, density assessment currently relies almost exclusively on X-ray mammography, an ionizing imaging modality. This study investigates the feasibility of estimating breast density using artificial intelligence over infrared thermal images, offering a non-ionizing imaging approach. The underlying hypothesis is that fibroglandular and adipose tissues exhibit distinct thermophysical and physiological properties, leading to subtle but spatially coherent temperature variations on the breast surface. In this paper, we propose DensiThAI, a multi-view deep learning framework for breast density classification from thermal images. The framework was evaluated on a multi-center dataset of 3,500 women using mammography-derived density labels as reference. Using five standard thermal views, DensiThAI achieved a mean AUROC of 0.73 across 10 random splits, with statistically significant separation between density classes across all splits (p &lt;&lt; 0.05). Consistent performance across age cohorts supports the potential of thermal imaging as a non-ionizing approach for breast density assessment with implications for improved patient experience and workflow optimization.</description><author>cs.CV updates on arXiv.org</author><pubDate>Tue, 03 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.00145v1</guid></item><item><title>[arXiv-CV] A Hybrid Mamba-SAM Architecture for Efficient 3D Medical Image Segmentation</title><link>https://arxiv.org/abs/2602.00650</link><description>arXiv:2602.00650v1 Announce Type: new 
Abstract: Accurate segmentation of 3D medical images such as MRI and CT is essential for clinical diagnosis and treatment planning. Foundation models like the Segment Anything Model (SAM) provide powerful general-purpose representations but struggle in medical imaging due to domain shift, their inherently 2D design, and the high computational cost of fine-tuning. To address these challenges, we propose Mamba-SAM, a novel and efficient hybrid architecture that combines a frozen SAM encoder with the linear-time efficiency and long-range modeling capabilities of Mamba-based State Space Models (SSMs). We investigate two parameter-efficient adaptation strategies. The first is a dual-branch architecture that explicitly fuses general features from a frozen SAM encoder with domain-specific representations learned by a trainable VMamba encoder using cross-attention. The second is an adapter-based approach that injects lightweight, 3D-aware Tri-Plane Mamba (TPMamba) modules into the frozen SAM ViT encoder to implicitly model volumetric context. Within this framework, we introduce Multi-Frequency Gated Convolution (MFGC), which enhances feature representation by jointly analyzing spatial and frequency-domain information via 3D discrete cosine transforms and adaptive gating. Extensive experiments on the ACDC cardiac MRI dataset demonstrate the effectiveness of the proposed methods. The dual-branch Mamba-SAM-Base model achieves a mean Dice score of 0.906, comparable to UNet++ (0.907), while outperforming all baselines on Myocardium (0.910) and Left Ventricle (0.971) segmentation. The adapter-based TP MFGC variant offers superior inference speed (4.77 FPS) with strong accuracy (0.880 Dice). These results show that hybridizing foundation models with efficient SSM-based architectures provides a practical and effective solution for 3D medical image segmentation.</description><author>cs.CV updates on arXiv.org</author><pubDate>Tue, 03 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.00650v1</guid></item><item><title>[arXiv-CV] Data Augmentation for High-Fidelity Generation of CAR-T/NK Immunological Synapse Images</title><link>https://arxiv.org/abs/2602.00949</link><description>arXiv:2602.00949v1 Announce Type: new 
Abstract: Chimeric antigen receptor (CAR)-T and NK cell immunotherapies have transformed cancer treatment, and recent studies suggest that the quality of the CAR-T/NK cell immunological synapse (IS) may serve as a functional biomarker for predicting therapeutic efficacy. Accurate detection and segmentation of CAR-T/NK IS structures using artificial neural networks (ANNs) can greatly increase the speed and reliability of IS quantification. However, a persistent challenge is the limited size of annotated microscopy datasets, which restricts the ability of ANNs to generalize. To address this challenge, we integrate two complementary data-augmentation frameworks. First, we employ Instance Aware Automatic Augmentation (IAAA), an automated, instance-preserving augmentation method that generates synthetic CAR-T/NK IS images and corresponding segmentation masks by applying optimized augmentation policies to original IS data. IAAA supports multiple imaging modalities (e.g., fluorescence and brightfield) and can be applied directly to CAR-T/NK IS images derived from patient samples. In parallel, we introduce a Semantic-Aware AI Augmentation (SAAA) pipeline that combines a diffusion-based mask generator with a Pix2Pix conditional image synthesizer. This second method enables the creation of diverse, anatomically realistic segmentation masks and produces high-fidelity CAR-T/NK IS images aligned with those masks, further expanding the training corpus beyond what IAAA alone can provide. Together, these augmentation strategies generate synthetic images whose visual and structural properties closely match real IS data, significantly improving CAR-T/NK IS detection and segmentation performance. By enhancing the robustness and accuracy of IS quantification, this work supports the development of more reliable imaging-based biomarkers for predicting patient response to CAR-T/NK immunotherapy.</description><author>cs.CV updates on arXiv.org</author><pubDate>Tue, 03 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.00949v1</guid></item><item><title>[arXiv-CV] Stronger Semantic Encoders Can Harm Relighting Performance: Probing Visual Priors via Augmented Latent Intrinsics</title><link>https://arxiv.org/abs/2602.01391</link><description>arXiv:2602.01391v1 Announce Type: new 
Abstract: Image-to-image relighting requires representations that disentangle scene properties from illumination. Recent methods rely on latent intrinsic representations but remain under-constrained and often fail on challenging materials such as metal and glass. A natural hypothesis is that stronger pretrained visual priors should resolve these failures. We find the opposite: features from top-performing semantic encoders often degrade relighting quality, revealing a fundamental trade-off between semantic abstraction and photometric fidelity. We study this trade-off and introduce Augmented Latent Intrinsics (ALI), which balances semantic context and dense photometric structure by fusing features from a pixel-aligned visual encoder into a latent-intrinsic framework, together with a self-supervised refinement strategy to mitigate the scarcity of paired real-world data. Trained only on unlabeled real-world image pairs and paired with a dense, pixel-aligned visual prior, ALI achieves strong improvements in relighting, with the largest gains on complex, specular materials. Project page: https:\\augmented-latent-intrinsics.github.io</description><author>cs.CV updates on arXiv.org</author><pubDate>Tue, 03 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.01391v1</guid></item><item><title>[arXiv-CV] Federated Vision Transformer with Adaptive Focal Loss for Medical Image Classification</title><link>https://arxiv.org/abs/2602.01633</link><description>arXiv:2602.01633v1 Announce Type: new 
Abstract: While deep learning models like Vision Transformer (ViT) have achieved significant advances, they typically require large datasets. With data privacy regulations, access to many original datasets is restricted, especially medical images. Federated learning (FL) addresses this challenge by enabling global model aggregation without data exchange. However, the heterogeneity of the data and the class imbalance that exist in local clients pose challenges for the generalization of the model. This study proposes a FL framework leveraging a dynamic adaptive focal loss (DAFL) and a client-aware aggregation strategy for local training. Specifically, we design a dynamic class imbalance coefficient that adjusts based on each client's sample distribution and class data distribution, ensuring minority classes receive sufficient attention and preventing sparse data from being ignored. To address client heterogeneity, a weighted aggregation strategy is adopted, which adapts to data size and characteristics to better capture inter-client variations. The classification results on three public datasets (ISIC, Ocular Disease and RSNA-ICH) show that the proposed framework outperforms DenseNet121, ResNet50, ViT-S/16, ViT-L/32, FedCLIP, Swin Transformer, CoAtNet, and MixNet in most cases, with accuracy improvements ranging from 0.98\% to 41.69\%. Ablation studies on the imbalanced ISIC dataset validate the effectiveness of the proposed loss function and aggregation strategy compared to traditional loss functions and other FL approaches. The codes can be found at: https://github.com/AIPMLab/ViT-FLDAF.</description><author>cs.CV updates on arXiv.org</author><pubDate>Tue, 03 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.01633v1</guid></item><item><title>[arXiv-CV] Evaluating OCR Performance for Assistive Technology: Effects of Walking Speed, Camera Placement, and Camera Type</title><link>https://arxiv.org/abs/2602.02223</link><description>arXiv:2602.02223v1 Announce Type: new 
Abstract: Optical character recognition (OCR), which converts printed or handwritten text into machine-readable form, is widely used in assistive technology for people with blindness and low vision. Yet, most evaluations rely on static datasets that do not reflect the challenges of mobile use. In this study, we systematically evaluated OCR performance under both static and dynamic conditions. Static tests measured detection range across distances of 1-7 meters and viewing angles of 0-75 degrees horizontally. Dynamic tests examined the impact of motion by varying walking speed from slow (0.8 m/s) to very fast (1.8 m/s) and comparing three camera mounting positions: head-mounted, shoulder-mounted, and hand-held. We evaluated both a smartphone and smart glasses, using the phone's main and ultra-wide cameras. Four OCR engines were benchmarked to assess accuracy at different distances and viewing angles: Google Vision, PaddleOCR 3.0, EasyOCR, and Tesseract. PaddleOCR 3.0 was then used to evaluate accuracy at different walking speeds. Accuracy was computed at the character level using the Levenshtein ratio against manually defined ground truth. Results showed that recognition accuracy declined with increased walking speed and wider viewing angles. Google Vision achieved the highest overall accuracy, with PaddleOCR close behind as the strongest open-source alternative. Across devices, the phone's main camera achieved the highest accuracy, and a shoulder-mounted placement yielded the highest average among body positions; however, differences among shoulder, head, and hand were not statistically significant.</description><author>cs.CV updates on arXiv.org</author><pubDate>Tue, 03 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.02223v1</guid></item><item><title>[arXiv-CV] Reading Recognition in the Wild</title><link>https://arxiv.org/abs/2505.24848</link><description>arXiv:2505.24848v3 Announce Type: replace 
Abstract: To enable egocentric contextual AI in always-on smart glasses, it is crucial to be able to keep a record of the user's interactions with the world, including during reading. In this paper, we introduce a new task of reading recognition to determine when the user is reading. We first introduce the first-of-its-kind large-scale multimodal Reading in the Wild dataset, containing 100 hours of reading and non-reading videos in diverse and realistic scenarios. We then identify three modalities (egocentric RGB, eye gaze, head pose) that can be used to solve the task, and present a flexible transformer model that performs the task using these modalities, either individually or combined. We show that these modalities are relevant and complementary to the task, and investigate how to efficiently and effectively encode each modality. Additionally, we show the usefulness of this dataset towards classifying types of reading, extending current reading understanding studies conducted in constrained settings to larger scale, diversity and realism.</description><author>cs.CV updates on arXiv.org</author><pubDate>Tue, 03 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2505.24848v3</guid></item><item><title>[arXiv-CV] Prior-Guided Residual Diffusion: Calibrated and Efficient Medical Image Segmentation</title><link>https://arxiv.org/abs/2509.01330</link><description>arXiv:2509.01330v2 Announce Type: replace 
Abstract: Ambiguity in medical image segmentation calls for models that capture full conditional distributions rather than a single point estimate. We present Prior-Guided Residual Diffusion (PGRD), a diffusion-based framework that learns voxel-wise distributions while maintaining strong calibration and practical sampling efficiency. PGRD embeds discrete labels as one-hot targets in a continuous space to align segmentation with diffusion modeling. A coarse prior predictor provides step-wise guidance; the diffusion network then learns the residual to the prior, accelerating convergence and improving calibration. A deep diffusion supervision scheme further stabilizes training by supervising intermediate time steps. Evaluated on representative MRI and CT datasets, PGRD achieves higher Dice scores and lower NLL/ECE values than Bayesian, ensemble, Probabilistic U-Net, and vanilla diffusion baselines, while requiring fewer sampling steps to reach strong performance.</description><author>cs.CV updates on arXiv.org</author><pubDate>Tue, 03 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2509.01330v2</guid></item><item><title>[arXiv-CV] Learning to Look Closer: A New Instance-Wise Loss for Small Cerebral Lesion Segmentation</title><link>https://arxiv.org/abs/2511.17146</link><description>arXiv:2511.17146v2 Announce Type: replace 
Abstract: Traditional loss functions in medical image segmentation, such as Dice, often under-segment small lesions because their small relative volume contributes negligibly to the overall loss. To address this, instance-wise loss functions and metrics have been proposed to evaluate segmentation quality on a per-lesion basis. We introduce CC-DiceCE, a loss function based on the CC-Metrics framework, and compare it with the existing blob loss. Both are benchmarked against a DiceCE baseline within the nnU-Net framework, which provides a robust and standardized setup. We find that CC-DiceCE loss increases detection (recall) with minimal to no degradation in segmentation performance, though with dataset-dependent trade-offs in precision. Furthermore, our multi-dataset study shows that CC-DiceCE generally outperforms blob loss.</description><author>cs.CV updates on arXiv.org</author><pubDate>Tue, 03 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2511.17146v2</guid></item><item><title>[arXiv-CV] VibrantSR: Sub-Meter Canopy Height Models from Sentinel-2 Using Generative Flow Matching</title><link>https://arxiv.org/abs/2601.09866</link><description>arXiv:2601.09866v2 Announce Type: replace 
Abstract: We present VibrantSR (Vibrant Super-Resolution), a generative super-resolution framework for estimating 0.5 meter canopy height models (CHMs) from 10 meter Sentinel-2 imagery. Unlike approaches based on aerial imagery that are constrained by infrequent and irregular acquisition schedules, VibrantSR leverages globally available Sentinel-2 seasonal composites, enabling consistent monitoring at a seasonal-to-annual cadence. Evaluated across 22 EPA Level 3 eco-regions in the western United States using spatially disjoint validation splits, VibrantSR achieves a Mean Absolute Error of 4.39 meters for canopy heights &gt;= 2 m, outperforming Meta (4.83 m), LANDFIRE (5.96 m), and ETH (7.05 m) satellite-based benchmarks. While aerial-based VibrantVS (2.71 m MAE) retains an accuracy advantage, VibrantSR enables operational forest monitoring and carbon accounting at continental scales without reliance on costly and temporally infrequent aerial acquisitions.</description><author>cs.CV updates on arXiv.org</author><pubDate>Tue, 03 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2601.09866v2</guid></item><item><title>[arXiv-CV] Helios 2.0: A Robust, Ultra-Low Power Gesture Recognition System Optimised for Event-Sensor based Wearables</title><link>https://arxiv.org/abs/2503.07825</link><description>arXiv:2503.07825v2 Announce Type: replace-cross 
Abstract: We present an advance in wearable technology: a mobile-optimized, real-time, ultra-low-power event camera system that enables natural hand gesture control for smart glasses, dramatically improving user experience. While hand gesture recognition in computer vision has advanced significantly, critical challenges remain in creating systems that are intuitive, adaptable across diverse users and environments, and energy-efficient enough for practical wearable applications. Our approach tackles these challenges through carefully selected microgestures: lateral thumb swipes across the index finger (in both directions) and a double pinch between thumb and index fingertips. These human-centered interactions leverage natural hand movements, ensuring intuitive usability without requiring users to learn complex command sequences. To overcome variability in users and environments, we developed a novel simulation methodology that enables comprehensive domain sampling without extensive real-world data collection. Our power-optimised architecture maintains exceptional performance, achieving F1 scores above 80\% on benchmark datasets featuring diverse users and environments. The resulting models operate at just 6-8 mW when exploiting the Qualcomm Snapdragon Hexagon DSP, with our 2-channel implementation exceeding 70\% F1 accuracy and our 6-channel model surpassing 80\% F1 accuracy across all gesture classes in user studies. These results were achieved using only synthetic training data. This improves on the state-of-the-art for F1 accuracy by 20\% with a power reduction 25x when using DSP. This advancement brings deploying ultra-low-power vision systems in wearable devices closer and opens new possibilities for seamless human-computer interaction.</description><author>cs.CV updates on arXiv.org</author><pubDate>Tue, 03 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2503.07825v2</guid></item><item><title>[arXiv-CV] CostNav: A Navigation Benchmark for Real-World Economic-Cost Evaluation of Physical AI Agents</title><link>https://arxiv.org/abs/2511.20216</link><description>arXiv:2511.20216v2 Announce Type: replace-cross 
Abstract: While current navigation benchmarks prioritize task success in simplified settings, they neglect the multidimensional economic constraints essential for the real-world commercialization of autonomous delivery systems. We introduce CostNav, an Economic Navigation Benchmark that evaluates physical AI agents through comprehensive economic cost-revenue analysis aligned with real-world business operations. By integrating industry-standard data - such as SEC filings and AIS injury reports - with Isaac Sim's detailed collision and cargo dynamics, CostNav transcends simple task completion to accurately evaluate business value in complex, real-world scenarios. To our knowledge, CostNav is the first work to quantitatively expose the gap between navigation research metrics and commercial viability, revealing that optimizing for task success on a simplified task fundamentally differs from optimizing for real-world economic deployment. Our evaluation of rule-based Nav2 navigation shows that current approaches are not economically viable: the contribution margin is -22.81/run (AMCL) and -12.87/run (GPS), resulting in no break-even point. We challenge the community to develop navigation policies that achieve economic viability on CostNav. We remain method-agnostic, evaluating success solely on the metric of cost rather than the underlying architecture. All resources are available at https://github.com/worv-ai/CostNav.</description><author>cs.CV updates on arXiv.org</author><pubDate>Tue, 03 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2511.20216v2</guid></item><item><title>[arXiv-ML] BLOCK-EM: Preventing Emergent Misalignment by Blocking Causal Features</title><link>https://arxiv.org/abs/2602.00767</link><description>arXiv:2602.00767v1 Announce Type: new 
Abstract: Emergent misalignment can arise when a language model is fine-tuned on a narrowly scoped supervised objective: the model learns the target behavior, yet also develops undesirable out-of-domain behaviors. We investigate a mechanistic approach to preventing emergent misalignment by identifying a small set of internal features that reliably control the misaligned behavior and then discouraging the model from strengthening these features during fine-tuning. Across six fine-tuning domains, blocking (i.e., constraining) a fixed set of features achieves up to 95\% relative reduction in emergent misalignment with no degradation in model quality or target-task performance. We strengthen validity with disjoint selection/evaluation splits, multiple independent judges, multiple random seeds for key settings, quality metrics, and extensive ablations demonstrating that the reduction in misalignment is specific to the identified mechanism. We also characterize a limiting regime in which misalignment re-emerges under prolonged fine-tuning, present evidence consistent with rerouting through alternative features or layers, and evaluate modifications that partially restore the misalignment-blocking effect. Overall, our results show that targeted training-time constraints on internal mechanisms can mitigate emergent misalignment without degrading target-task performance.</description><author>cs.LG updates on arXiv.org</author><pubDate>Tue, 03 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.00767v1</guid></item><item><title>[arXiv-ML] The GT-Score: A Robust Objective Function for Reducing Overfitting in Data-Driven Trading Strategies</title><link>https://arxiv.org/abs/2602.00080</link><description>arXiv:2602.00080v1 Announce Type: cross 
Abstract: Overfitting remains a critical challenge in data-driven financial modeling, where machine learning (ML) systems learn spurious patterns in historical prices and fail out of sample and in deployment. This paper introduces the GT-Score, a composite objective function that integrates performance, statistical significance, consistency, and downside risk to guide optimization toward more robust trading strategies. This approach directly addresses critical pitfalls in quantitative strategy development, specifically data snooping during optimization and the unreliability of statistical inference under non-normal return distributions. Using historical stock data for 50 S&amp;amp;P 500 companies spanning 2010-2024, we conduct an empirical evaluation that includes walk-forward validation with nine sequential time splits and a Monte Carlo study with 15 random seeds across three trading strategies. In walk-forward validation, GT-Score improves the generalization ratio (validation return divided by training return) by 98% relative to baseline objective functions. Paired statistical tests on Monte Carlo out-of-sample returns indicate statistically detectable differences between objective functions (p &lt; 0.01 for comparisons with Sortino and Simple), with small effect sizes. These results suggest that embedding an anti-overfitting structure into the objective can improve the reliability of backtests in quantitative research. Reproducible code and processed result files are provided as supplementary materials.</description><author>cs.LG updates on arXiv.org</author><pubDate>Tue, 03 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.00080v1</guid></item><item><title>[arXiv-ML] Neuron Block Dynamics for XOR Classification with Zero-Margin</title><link>https://arxiv.org/abs/2602.00172</link><description>arXiv:2602.00172v1 Announce Type: cross 
Abstract: The ability of neural networks to learn useful features through stochastic gradient descent (SGD) is a cornerstone of their success. Most theoretical analyses focus on regression or on classification tasks with a positive margin, where worst-case gradient bounds suffice. In contrast, we study zero-margin nonlinear classification by analyzing the Gaussian XOR problem, where inputs are Gaussian and the XOR decision boundary determines labels. In this setting, a non-negligible fraction of data lies arbitrarily close to the boundary, breaking standard margin-based arguments. Building on Glasgow's (2024) analysis, we extend the study of training dynamics from discrete to Gaussian inputs and develop a framework for the dynamics of neuron blocks. We show that neurons cluster into four directions and that block-level signals evolve coherently, a phenomenon essential in the Gaussian setting where individual neuron signals vary significantly. Leveraging this block perspective, we analyze generalization without relying on margin assumptions, adopting an average-case view that distinguishes regions of reliable prediction from regions of persistent error. Numerical experiments confirm the predicted two-phase block dynamics and demonstrate their robustness beyond the Gaussian setting.</description><author>cs.LG updates on arXiv.org</author><pubDate>Tue, 03 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.00172v1</guid></item><item><title>[arXiv-ML] TRIP-Bench: A Benchmark for Long-Horizon Interactive Agents in Real-World Scenarios</title><link>https://arxiv.org/abs/2602.01675</link><description>arXiv:2602.01675v1 Announce Type: cross 
Abstract: As LLM-based agents are deployed in increasingly complex real-world settings, existing benchmarks underrepresent key challenges such as enforcing global constraints, coordinating multi-tool reasoning, and adapting to evolving user behavior over long, multi-turn interactions. To bridge this gap, we introduce \textbf{TRIP-Bench}, a long-horizon benchmark grounded in realistic travel-planning scenarios. TRIP-Bench leverages real-world data, offers 18 curated tools and 40+ travel requirements, and supports automated evaluation. It includes splits of varying difficulty; the hard split emphasizes long and ambiguous interactions, style shifts, feasibility changes, and iterative version revision. Dialogues span up to 15 user turns, can involve 150+ tool calls, and may exceed 200k tokens of context. Experiments show that even advanced models achieve at most 50\% success on the easy split, with performance dropping below 10\% on hard subsets. We further propose \textbf{GTPO}, an online multi-turn reinforcement learning method with specialized reward normalization and reward differencing. Applied to Qwen2.5-32B-Instruct, GTPO improves constraint satisfaction and interaction robustness, outperforming Gemini-3-Pro in our evaluation. We expect TRIP-Bench to advance practical long-horizon interactive agents, and GTPO to provide an effective online RL recipe for robust long-horizon training.</description><author>cs.LG updates on arXiv.org</author><pubDate>Tue, 03 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.01675v1</guid></item><item><title>[arXiv-ML] Hierarchical Federated Learning with SignSGD: A Highly Communication-Efficient Approach</title><link>https://arxiv.org/abs/2602.02355</link><description>arXiv:2602.02355v1 Announce Type: cross 
Abstract: Hierarchical federated learning (HFL) has emerged as a key architecture for large-scale wireless and Internet of Things systems, where devices communicate with nearby edge servers before reaching the cloud. In these environments, uplink bandwidth and latency impose strict communication limits, thereby making aggressive gradient compression essential. One-bit methods such as sign-based stochastic gradient descent (SignSGD) offer an attractive solution in flat federated settings, but existing theory and algorithms do not naturally extend to hierarchical settings. In particular, the interaction between majority-vote aggregation at the edge layer and model aggregation at the cloud layer, and its impact on end-to-end performance, remains unknown. To bridge this gap, we propose a highly communication-efficient sign-based HFL framework and develop its corresponding formulation for nonconvex learning, where devices send only signed stochastic gradients, edge servers combine them through majority-vote, and the cloud periodically averages the obtained edge models, while utilizing downlink quantization to broadcast the global model. We introduce the resulting scalable HFL algorithm, HierSignSGD, and provide the convergence analysis for SignSGD in a hierarchical setting. Our core technical contribution is a characterization of how biased sign compression, two-level aggregation intervals, and inter-cluster heterogeneity collectively affect convergence. Numerical experiments under homogeneous and heterogeneous data splits show that HierSignSGD, despite employing extreme compression, achieves accuracy comparable to or better than full-precision stochastic gradient descent while reducing communication cost in the process, and remains robust under aggressive downlink sparsification.</description><author>cs.LG updates on arXiv.org</author><pubDate>Tue, 03 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.02355v1</guid></item><item><title>[arXiv-ML] Live-Evo: Online Evolution of Agentic Memory from Continuous Feedback</title><link>https://arxiv.org/abs/2602.02369</link><description>arXiv:2602.02369v1 Announce Type: cross 
Abstract: Large language model (LLM) agents are increasingly equipped with memory, which are stored experience and reusable guidance that can improve task-solving performance. Recent \emph{self-evolving} systems update memory based on interaction outcomes, but most existing evolution pipelines are developed for static train/test splits and only approximate online learning by folding static benchmarks, making them brittle under true distribution shift and continuous feedback. We introduce \textsc{Live-Evo}, an online self-evolving memory system that learns from a stream of incoming data over time. \textsc{Live-Evo} decouples \emph{what happened} from \emph{how to use it} via an Experience Bank and a Meta-Guideline Bank, compiling task-adaptive guidelines from retrieved experiences for each task. To manage memory online, \textsc{Live-Evo} maintains experience weights and updates them from feedback: experiences that consistently help are reinforced and retrieved more often, while misleading or stale experiences are down-weighted and gradually forgotten, analogous to reinforcement and decay in human memory. On the live \textit{Prophet Arena} benchmark over a 10-week horizon, \textsc{Live-Evo} improves Brier score by 20.8\% and increases market returns by 12.9\%, while also transferring to deep-research benchmarks with consistent gains over strong baselines. Our code is available at https://github.com/ag2ai/Live-Evo.</description><author>cs.LG updates on arXiv.org</author><pubDate>Tue, 03 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2602.02369v1</guid></item><item><title>[arXiv-ML] TSCAN: Context-Aware Uplift Modeling via Two-Stage Training for Online Merchant Business Diagnosis</title><link>https://arxiv.org/abs/2504.18881</link><description>arXiv:2504.18881v3 Announce Type: replace 
Abstract: A primary challenge in ITE estimation is sample selection bias. Traditional approaches utilize treatment regularization techniques such as the Integral Probability Metrics (IPM), re-weighting, and propensity score modeling to mitigate this bias. However, these regularizations may introduce undesirable information loss and limit the performance of the model. Furthermore, treatment effects vary across different external contexts, and the existing methods are insufficient in fully interacting with and utilizing these contextual features. To address these issues, we propose a Context-Aware uplift model based on the Two-Stage training approach (TSCAN), comprising CAN-U and CAN-D sub-models. In the first stage, we train an uplift model, called CAN-U, which includes the treatment regularizations of IPM and propensity score prediction, to generate a complete dataset with counterfactual uplift labels. In the second stage, we train a model named CAN-D, which utilizes an isotonic output layer to directly model uplift effects, thereby eliminating the reliance on the regularization components. CAN-D adaptively corrects the errors estimated by CAN-U through reinforcing the factual samples, while avoiding the negative impacts associated with the aforementioned regularizations. Additionally, we introduce a Context-Aware Attention Layer throughout the two-stage process to manage the interactions between treatment, merchant, and contextual features, thereby modeling the varying treatment effect in different contexts. We conduct extensive experiments on two real-world datasets to validate the effectiveness of TSCAN. Ultimately, the deployment of our model for real-world merchant diagnosis on one of China's largest online food ordering platforms validates its practical utility and impact.</description><author>cs.LG updates on arXiv.org</author><pubDate>Tue, 03 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2504.18881v3</guid></item><item><title>[arXiv-ML] Less is More: Clustered Cross-Covariance Control for Offline RL</title><link>https://arxiv.org/abs/2601.20765</link><description>arXiv:2601.20765v2 Announce Type: replace 
Abstract: A fundamental challenge in offline reinforcement learning is distributional shift. Scarce data or datasets dominated by out-of-distribution (OOD) areas exacerbate this issue. Our theoretical analysis and experiments show that the standard squared error objective induces a harmful TD cross covariance. This effect amplifies in OOD areas, biasing optimization and degrading policy learning. To counteract this mechanism, we develop two complementary strategies: partitioned buffer sampling that restricts updates to localized replay partitions, attenuates irregular covariance effects, and aligns update directions, yielding a scheme that is easy to integrate with existing implementations, namely Clustered Cross-Covariance Control for TD (C^4). We also introduce an explicit gradient-based corrective penalty that cancels the covariance induced bias within each update. We prove that buffer partitioning preserves the lower bound property of the maximization objective, and that these constraints mitigate excessive conservatism in extreme OOD areas without altering the core behavior of policy constrained offline reinforcement learning. Empirically, our method showcases higher stability and up to 30% improvement in returns over prior methods, especially with small datasets and splits that emphasize OOD areas.</description><author>cs.LG updates on arXiv.org</author><pubDate>Tue, 03 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2601.20765v2</guid></item><item><title>[arXiv-ML] The Nuclear Route: Sharp Asymptotics of ERM in Overparameterized Quadratic Networks</title><link>https://arxiv.org/abs/2505.17958</link><description>arXiv:2505.17958v3 Announce Type: replace-cross 
Abstract: We study the high-dimensional asymptotics of empirical risk minimization (ERM) in over-parametrized two-layer neural networks with quadratic activations trained on synthetic data. We derive sharp asymptotics for both training and test errors by mapping the $\ell_2$-regularized learning problem to a convex matrix sensing task with nuclear norm penalization. This reveals that capacity control in such networks emerges from a low-rank structure in the learned feature maps. Our results characterize the global minima of the loss and yield precise generalization thresholds, showing how the width of the target function governs learnability. This analysis bridges and extends ideas from spin-glass methods, matrix factorization, and convex optimization and emphasizes the deep link between low-rank matrix sensing and learning in quadratic neural networks.</description><author>cs.LG updates on arXiv.org</author><pubDate>Tue, 03 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2505.17958v3</guid></item><item><title>[arXiv-ML] Single-Head Attention in High Dimensions: A Theory of Generalization, Weights Spectra, and Scaling Laws</title><link>https://arxiv.org/abs/2509.24914</link><description>arXiv:2509.24914v2 Announce Type: replace-cross 
Abstract: Trained attention layers exhibit striking and reproducible spectral structure of the weights, including low-rank collapse, bulk deformation, and isolated spectral outliers, yet the origin of these phenomena and their implications for generalization remain poorly understood. We study empirical risk minimization in a single-head tied-attention layer trained on synthetic high-dimensional sequence tasks generated from the attention-indexed model. Using tools from random matrix theory, spin-glass theory, and approximate message passing, we obtain an exact high-dimensional characterization of training and test error, interpolation and recovery thresholds, and the spectrum of the key and query matrices. Our theory predicts the full singular-value distribution of the trained query-key map, including low-rank structure and isolated spectral outliers, in qualitative agreement with observations in more realistic transformers. Finally, for targets with power-law spectra, we show that learning proceeds through sequential spectral recovery, leading to the emergence of power-law scaling laws.</description><author>cs.LG updates on arXiv.org</author><pubDate>Tue, 03 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2509.24914v2</guid></item><item><title>[arXiv-ML] A Backpropagation-Free Feedback-Hebbian Network for Continual Learning Dynamics</title><link>https://arxiv.org/abs/2601.06758</link><description>arXiv:2601.06758v3 Announce Type: replace-cross 
Abstract: Feedback-rich neural architectures can regenerate earlier representations and inject temporal context, making them a natural setting for strictly local synaptic plasticity. Existing literature raises doubt about whether a minimal, backpropagation-free feedback-Hebbian system can already express interpretable continual-learning-relevant behaviors under controlled training schedules. In this work, we introduce a compact prediction-reconstruction architecture with a dedicated feedback pathway providing lightweight, locally trainable temporal context for continual adaptation. All synapses are updated by a unified local rule combining centered Hebbian covariance, Oja-style stabilization, and a local supervised drive where targets are available. With a simple two-pair association task, learning is characterized through layer-wise activity snapshots, connectivity trajectories (row and column means of learned weights), and a normalized retention index across phases. Under sequential A to B training, forward output connectivity exhibits a long-term depression (LTD)-like suppression of the earlier association, while feedback connectivity preserves an A-related trace during acquisition of B. Under an alternating sequence, both associations are concurrently maintained rather than sequentially suppressed. Architectural controls and rule-term ablations isolate the role of dedicated feedback in regeneration and co-maintenance, alongside the role of the local supervised term in output selectivity and unlearning. Together, the results show that a compact feedback pathway trained with local plasticity can support regeneration and continual-learning-relevant dynamics in a minimal, mechanistically transparent setting.</description><author>cs.LG updates on arXiv.org</author><pubDate>Tue, 03 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2601.06758v3</guid></item><item><title>[arXiv-ML] Large-Scale Optimization Model Auto-Formulation: Harnessing LLM Flexibility via Structured Workflow</title><link>https://arxiv.org/abs/2601.09635</link><description>arXiv:2601.09635v3 Announce Type: replace-cross 
Abstract: Large-scale optimization is a key backbone of modern business decision-making. However, building these models is often labor-intensive and time-consuming. We address this by proposing LEAN-LLM-OPT, a LightwEight AgeNtic workflow construction framework for LLM-assisted large-scale OPTimization auto-formulation. LEAN-LLM-OPT takes as input a problem description together with associated datasets and orchestrates a team of LLM agents to produce an optimization formulation. Specifically, upon receiving a query, two upstream LLM agents dynamically construct a workflow that specifies, step-by-step, how optimization models for similar problems can be formulated. A downstream LLM agent then follows this workflow to generate the final output. The agentic workflow leverages common modeling practices to structure the modeling process into a sequence of sub-tasks, offloading mechanical data-handling operations to auxiliary tools. This reduces the LLM's burden in planning and data handling, allowing us to exploit its flexibility to address unstructured components. Extensive simulations show that LEAN-LLM-OPT, instantiated with GPT-4.1 and the open source gpt-oss-20B, achieves strong performance on large-scale optimization modeling tasks and is competitive with state-of-the-art approaches. In addition, in a Singapore Airlines choice-based revenue management use case, LEAN-LLM-OPT demonstrates practical value by achieving leading performance across a range of scenarios. Along the way, we introduce Large-Scale-OR and Air-NRM, the first comprehensive benchmarks for large-scale optimization auto-formulation. The code and data of this work is available at https://github.com/CoraLiang01/lean-llm-opt.</description><author>cs.LG updates on arXiv.org</author><pubDate>Tue, 03 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2601.09635v3</guid></item><item><title>[NC] A Dual Consistency Training (DCT) strategy for polyphonic sound event detection</title><link>https://www.sciencedirect.com/science/article/pii/S0925231226002778?dgcid=rss_sd_all</link><description>&lt;p&gt;Publication date: 7 April 2026&lt;/p&gt;&lt;p&gt;&lt;b&gt;Source:&lt;/b&gt; Neurocomputing, Volume 673&lt;/p&gt;&lt;p&gt;Author(s): Qiong Wu, Mingyu Wang, Ying Hu, Xinchun Ma, Zhijian Ou&lt;/p&gt;</description><author>ScienceDirect Publication: Neurocomputing</author><pubDate>Mon, 02 Feb 2026 13:11:02 GMT</pubDate><guid isPermaLink="true">https://www.sciencedirect.com/science/article/pii/S0925231226002778</guid></item><item><title>[arXiv-CV] Region-Normalized DPO for Medical Image Segmentation under Noisy Judges</title><link>https://arxiv.org/abs/2601.23222</link><description>arXiv:2601.23222v1 Announce Type: new 
Abstract: While dense pixel-wise annotations remain the gold standard for medical image segmentation, they are costly to obtain and limit scalability. In contrast, many deployed systems already produce inexpensive automatic quality-control (QC) signals like model agreement, uncertainty measures, or learned mask-quality scores which can be used for further model training without additional ground-truth annotation. However, these signals can be noisy and biased, making preference-based fine-tuning susceptible to harmful updates. We study Direct Preference Optimization (DPO) for segmentation from such noisy judges using proposals generated by a supervised base segmenter trained on a small labeled set. We find that outcomes depend strongly on how preference pairs are mined: selecting the judge's top-ranked proposal can improve peak performance when the judge is reliable, but can amplify harmful errors under weaker judges. We propose Region-Normalized DPO (RN-DPO), a segmentation-aware objective which normalizes preference updates by the size of the disagreement region between masks, reducing the leverage of harmful comparisons and improving optimization stability. Across two medical datasets and multiple regimes, RN-DPO improves sustained performance and stabilizes preference-based fine-tuning, outperforming standard DPO and strong baselines without requiring additional pixel annotations.</description><author>cs.CV updates on arXiv.org</author><pubDate>Mon, 02 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2601.23222v1</guid></item><item><title>[arXiv-CV] EndoCaver: Handling Fog, Blur and Glare in Endoscopic Images via Joint Deblurring-Segmentation</title><link>https://arxiv.org/abs/2601.22537</link><description>arXiv:2601.22537v1 Announce Type: cross 
Abstract: Endoscopic image analysis is vital for colorectal cancer screening, yet real-world conditions often suffer from lens fogging, motion blur, and specular highlights, which severely compromise automated polyp detection. We propose EndoCaver, a lightweight transformer with a unidirectional-guided dual-decoder architecture, enabling joint multi-task capability for image deblurring and segmentation while significantly reducing computational complexity and model parameters. Specifically, it integrates a Global Attention Module (GAM) for cross-scale aggregation, a Deblurring-Segmentation Aligner (DSA) to transfer restoration cues, and a cosine-based scheduler (LoCoS) for stable multi-task optimisation. Experiments on the Kvasir-SEG dataset show that EndoCaver achieves 0.922 Dice on clean data and 0.889 under severe image degradation, surpassing state-of-the-art methods while reducing model parameters by 90%. These results demonstrate its efficiency and robustness, making it well-suited for on-device clinical deployment. Code is available at https://github.com/ReaganWu/EndoCaver.</description><author>cs.CV updates on arXiv.org</author><pubDate>Mon, 02 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2601.22537v1</guid></item><item><title>[arXiv-CV] Training Beyond Convergence: Grokking nnU-Net for Glioma Segmentation in Sub-Saharan MRI</title><link>https://arxiv.org/abs/2601.22637</link><description>arXiv:2601.22637v1 Announce Type: cross 
Abstract: Gliomas are placing an increasingly clinical burden on Sub-Saharan Africa (SSA). In the region, the median survival for patients remains under two years, and access to diagnostic imaging is extremely limited. These constraints highlight an urgent need for automated tools that can extract the maximum possible information from each available scan, tools that are specifically trained on local data, rather than adapted from high-income settings where conditions are vastly different. We utilize the Brain Tumor Segmentation (BraTS) Africa 2025 Challenge dataset, an expert annotated collection of glioma MRIs. Our objectives are: (i) establish a strong baseline with nnUNet on this dataset, and (ii) explore whether the celebrated "grokking" phenomenon an abrupt, late training jump from memorization to superior generalization can be triggered to push performance without extra labels. We evaluate two training regimes. The first is a fast, budget-conscious approach that limits optimization to just a few epochs, reflecting the constrained GPU resources typically available in African institutions. Despite this limitation, nnUNet achieves strong Dice scores: 92.3% for whole tumor (WH), 86.6% for tumor core (TC), and 86.3% for enhancing tumor (ET). The second regime extends training well beyond the point of convergence, aiming to trigger a grokking-driven performance leap. With this approach, we were able to achieve grokking and enhanced our results to higher Dice scores: 92.2% for whole tumor (WH), 90.1% for tumor core (TC), and 90.2% for enhancing tumor (ET).</description><author>cs.CV updates on arXiv.org</author><pubDate>Mon, 02 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2601.22637v1</guid></item><item><title>[arXiv-CV] From Cold Start to Active Learning: Embedding-Based Scan Selection for Medical Image Segmentation</title><link>https://arxiv.org/abs/2601.18532</link><description>arXiv:2601.18532v2 Announce Type: replace 
Abstract: Accurate segmentation annotations are critical for disease monitoring, yet manual labeling remains a major bottleneck due to the time and expertise required. Active learning (AL) alleviates this burden by prioritizing informative samples for annotation, typically through a diversity-based cold-start phase followed by uncertainty-driven selection. We propose a novel cold-start sampling strategy that combines foundation-model embeddings with clustering, including automatic selection of the number of clusters and proportional sampling across clusters, to construct a diverse and representative initial training. This is followed by an uncertainty-based AL framework that integrates spatial diversity to guide sample selection. The proposed method is intuitive and interpretable, enabling visualization of the feature-space distribution of candidate samples. We evaluate our approach on three datasets spanning X-ray and MRI modalities. On the CheXmask dataset, the cold-start strategy outperforms random selection, improving Dice from 0.918 to 0.929 and reducing the Hausdorff distance from 32.41 to 27.66 mm. In the AL setting, combined entropy and diversity selection improves Dice from 0.919 to 0.939 and reduces the Hausdorff distance from 30.10 to 19.16 mm. On the Montgomery dataset, cold-start gains are substantial, with Dice improving from 0.928 to 0.950 and Hausdorff distance decreasing from 14.22 to 9.38 mm. On the SynthStrip dataset, cold-start selection slightly affects Dice but reduces the Hausdorff distance from 9.43 to 8.69 mm, while active learning improves Dice from 0.816 to 0.826 and reduces the Hausdorff distance from 7.76 to 6.38 mm. Overall, the proposed framework consistently outperforms baseline methods in low-data regimes, improving segmentation accuracy.</description><author>cs.CV updates on arXiv.org</author><pubDate>Mon, 02 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2601.18532v2</guid></item><item><title>[arXiv-ML] Beyond Fixed Rounds: Data-Free Early Stopping for Practical Federated Learning</title><link>https://arxiv.org/abs/2601.22669</link><description>arXiv:2601.22669v1 Announce Type: new 
Abstract: Federated Learning (FL) facilitates decentralized collaborative learning without transmitting raw data. However, reliance on fixed global rounds or validation data for hyperparameter tuning hinders practical deployment by incurring high computational costs and privacy risks. To address this, we propose a data-free early stopping framework that determines the optimal stopping point by monitoring the task vector's growth rate using solely server-side parameters. The numerical results on skin lesion/blood cell classification demonstrate that our approach is comparable to validation-based early stopping across various state-of-the-art FL methods. In particular, the proposed framework spends an average of 47/20 (skin lesion/blood cell) rounds to achieve over 12.5%/10.3% higher performance than early stopping based on validation data. To the best of our knowledge, this is the first work to propose an early stopping framework for FL methods without using any validation data.</description><author>cs.LG updates on arXiv.org</author><pubDate>Mon, 02 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2601.22669v1</guid></item><item><title>[arXiv-ML] Matterhorn: Efficient Analog Sparse Spiking Transformer Architecture with Masked Time-To-First-Spike Encoding</title><link>https://arxiv.org/abs/2601.22876</link><description>arXiv:2601.22876v1 Announce Type: new 
Abstract: Spiking neural networks (SNNs) have emerged as a promising candidate for energy-efficient LLM inference. However, current energy evaluations for SNNs primarily focus on counting accumulate operations, and fail to account for real-world hardware costs such as data movement, which can consume nearly 80% of the total energy. In this paper, we propose Matterhorn, a spiking transformer that integrates a novel masked time-to-first-spike (M-TTFS) encoding method to reduce spike movement and a memristive synapse unit (MSU) to eliminate weight access overhead. M-TTFS employs a masking strategy that reassigns the zero-energy silent state (a spike train of all 0s) to the most frequent membrane potential rather than the lowest. This aligns the coding scheme with the data distribution, minimizing spike movement energy without information loss. We further propose a `dead zone' strategy that maximizes sparsity by mapping all values within a given range to the silent state. At the hardware level, the MSU utilizes compute-in-memory (CIM) technology to perform analog integration directly within memory, effectively removing weight access costs. On the GLUE benchmark, Matterhorn establishes a new state-of-the-art, surpassing existing SNNs by 1.42% in average accuracy while delivering a 2.31 times improvement in energy efficiency.</description><author>cs.LG updates on arXiv.org</author><pubDate>Mon, 02 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2601.22876v1</guid></item><item><title>[arXiv-ML] Uncertainty-Aware Extrapolation in Bayesian Oblique Trees</title><link>https://arxiv.org/abs/2601.22899</link><description>arXiv:2601.22899v1 Announce Type: new 
Abstract: Decision trees are widely used due to their interpretability and efficiency, but they struggle in regression tasks that require reliable extrapolation and well-calibrated uncertainty. Piecewise-constant leaf predictions are bounded by the training targets and often become overconfident under distribution shift. We propose a single-tree Bayesian model that extends VSPYCT by equipping each leaf with a GP predictor. Bayesian oblique splits provide uncertainty-aware partitioning of the input space, while GP leaves model local functional behaviour and enable principled extrapolation beyond the observed target range. We present an efficient inference and prediction scheme that combines posterior sampling of split parameters with \gls{gp} posterior predictions, and a gating mechanism that activates GP-based extrapolation when inputs fall outside the training support of a leaf. Experiments on benchmark regression tasks show improvements in the predictive performance compared to standard variational oblique trees, and substantial performance gains in extrapolation scenarios.</description><author>cs.LG updates on arXiv.org</author><pubDate>Mon, 02 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2601.22899v1</guid></item><item><title>[arXiv-ML] Trackly: A Unified SaaS Platform for User Behavior Analytics and Real Time Rule Based Anomaly Detection</title><link>https://arxiv.org/abs/2601.22800</link><description>arXiv:2601.22800v1 Announce Type: cross 
Abstract: Understanding user behavior is essential for improving digital experiences, optimizing business conversions, and mitigating threats like account takeovers, fraud, and bot attacks. Most platforms separate product analytics and security, creating fragmented visibility and delayed threat detection. Trackly, a scalable SaaS platform, unifies comprehensive user behavior analytics with real time, rule based anomaly detection. It tracks sessions, IP based geo location, device browser fingerprints, and granular events such as page views, add to cart, and checkouts. Suspicious activities logins from new devices or locations, impossible travel (Haversine formula), rapid bot like actions, VPN proxy usage, or multiple accounts per IP are flagged via configurable rules with weighted risk scoring, enabling transparent, explainable decisions. A real time dashboard provides global session maps, DAU MAU, bounce rates, and session durations. Integration is simplified with a lightweight JavaScript SDK and secure REST APIs. Implemented on a multi tenant microservices stack (ASP.NET Core, MongoDB, RabbitMQ, Next.js), Trackly achieved 98.1% accuracy, 97.7% precision, and 2.25% false positives on synthetic datasets, proving its efficiency for SMEs and ecommerce.</description><author>cs.LG updates on arXiv.org</author><pubDate>Mon, 02 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2601.22800v1</guid></item><item><title>[arXiv-ML] PIDSMaker: Building and Evaluating Provenance-based Intrusion Detection Systems</title><link>https://arxiv.org/abs/2601.22983</link><description>arXiv:2601.22983v1 Announce Type: cross 
Abstract: Recent provenance-based intrusion detection systems (PIDSs) have demonstrated strong potential for detecting advanced persistent threats (APTs) by applying machine learning to system provenance graphs. However, evaluating and comparing PIDSs remains difficult: prior work uses inconsistent preprocessing pipelines, non-standard dataset splits, and incompatible ground-truth labeling and metrics. These discrepancies undermine reproducibility, impede fair comparison, and impose substantial re-implementation overhead on researchers. We present PIDSMaker, an open-source framework for developing and evaluating PIDSs under consistent protocols. PIDSMaker consolidates eight state-of-the-art systems into a modular, extensible architecture with standardized preprocessing and ground-truth labels, enabling consistent experiments and apples-to-apples comparisons. A YAML-based configuration interface supports rapid prototyping by composing components across systems without code changes. PIDSMaker also includes utilities for ablation studies, hyperparameter tuning, multi-run instability measurement, and visualization, addressing methodological gaps identified in prior work. We demonstrate PIDSMaker through concrete use cases and release it with preprocessed datasets and labels to support shared evaluation for the PIDS community.</description><author>cs.LG updates on arXiv.org</author><pubDate>Mon, 02 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2601.22983v1</guid></item><item><title>[arXiv-ML] Towards a more realistic evaluation of machine learning models for bearing fault diagnosis</title><link>https://arxiv.org/abs/2509.22267</link><description>arXiv:2509.22267v2 Announce Type: replace 
Abstract: Reliable detection of bearing faults is essential for maintaining the safety and operational efficiency of rotating machinery. While recent advances in machine learning (ML), particularly deep learning, have shown strong performance in controlled settings, many studies fail to generalize to real-world applications due to methodological flaws, most notably data leakage. This paper investigates the issue of data leakage in vibration-based bearing fault diagnosis and its impact on model evaluation. We demonstrate that common dataset partitioning strategies, such as segment-wise and condition-wise splits, introduce spurious correlations that inflate performance metrics. To address this, we propose a rigorous, leakage-free evaluation methodology centered on bearing-wise data partitioning, ensuring no overlap between the physical components used for training and testing. Additionally, we reformulate the classification task as a multi-label problem, enabling the detection of co-occurring fault types and the use of prevalence-independent metrics such as Macro AUROC. Beyond preventing leakage, we also examine the effect of dataset diversity on generalization, showing that the number of unique training bearings is a decisive factor for achieving robust performance. We evaluate our methodology on three widely adopted datasets: CWRU, Paderborn University (PU), and University of Ottawa (UORED-VAFCLS). This study highlights the importance of leakage-aware evaluation protocols and provides practical guidelines for dataset partitioning, model selection, and validation, fostering the development of more trustworthy ML systems for industrial fault diagnosis applications.</description><author>cs.LG updates on arXiv.org</author><pubDate>Mon, 02 Feb 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2509.22267v2</guid></item><item><title>[NC] Cross-Domain Semantic Compensation via SpatialFrequency Decoupling for Robust Medical Image Segmentation</title><link>https://www.sciencedirect.com/science/article/pii/S092523122600295X?dgcid=rss_sd_all</link><description>&lt;p&gt;Publication date: Available online 29 January 2026&lt;/p&gt;&lt;p&gt;&lt;b&gt;Source:&lt;/b&gt; Neurocomputing&lt;/p&gt;&lt;p&gt;Author(s): Bangcheng Zhan, Ping Qi, Ruiyao Zhang, Enmin Song, Zhangbo Chu&lt;/p&gt;</description><author>ScienceDirect Publication: Neurocomputing</author><pubDate>Fri, 30 Jan 2026 18:51:39 GMT</pubDate><guid isPermaLink="true">https://www.sciencedirect.com/science/article/pii/S092523122600295X</guid></item><item><title>[NC] Few-shot medical image segmentation via CLIP-driven dual contrastive learning</title><link>https://www.sciencedirect.com/science/article/pii/S0925231226002122?dgcid=rss_sd_all</link><description>&lt;p&gt;Publication date: 7 April 2026&lt;/p&gt;&lt;p&gt;&lt;b&gt;Source:&lt;/b&gt; Neurocomputing, Volume 673&lt;/p&gt;&lt;p&gt;Author(s): Meiqi Zou, Wubang Yuan, Yiqing Liu, Dan Zeng, Gang Zhao&lt;/p&gt;</description><author>ScienceDirect Publication: Neurocomputing</author><pubDate>Fri, 30 Jan 2026 18:51:39 GMT</pubDate><guid isPermaLink="true">https://www.sciencedirect.com/science/article/pii/S0925231226002122</guid></item><item><title>[arXiv-CV] BLO-Inst: Bi-Level Optimization Based Alignment of YOLO and SAM for Robust Instance Segmentation</title><link>https://arxiv.org/abs/2601.22061</link><description>arXiv:2601.22061v1 Announce Type: new 
Abstract: The Segment Anything Model has revolutionized image segmentation with its zero-shot capabilities, yet its reliance on manual prompts hinders fully automated deployment. While integrating object detectors as prompt generators offers a pathway to automation, existing pipelines suffer from two fundamental limitations: objective mismatch, where detectors optimized for geometric localization do not correspond to the optimal prompting context required by SAM, and alignment overfitting in standard joint training, where the detector simply memorizes specific prompt adjustments for training samples rather than learning a generalizable policy. To bridge this gap, we introduce BLO-Inst, a unified framework that aligns detection and segmentation objectives by bi-level optimization. We formulate the alignment as a nested optimization problem over disjoint data splits. In the lower level, the SAM is fine-tuned to maximize segmentation fidelity given the current detection proposals on a subset ($D_1$). In the upper level, the detector is updated to generate bounding boxes that explicitly minimize the validation loss of the fine-tuned SAM on a separate subset ($D_2$). This effectively transforms the detector into a segmentation-aware prompt generator, optimizing the bounding boxes not just for localization accuracy, but for downstream mask quality. Extensive experiments demonstrate that BLO-Inst achieves superior performance, outperforming standard baselines on tasks in general and biomedical domains.</description><author>cs.CV updates on arXiv.org</author><pubDate>Fri, 30 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2601.22061v1</guid></item><item><title>[arXiv-IV] Integrating Color Histogram Analysis and Convolutional Neural Network for Skin Lesion Classification</title><link>https://arxiv.org/abs/2601.20869</link><description>arXiv:2601.20869v1 Announce Type: cross 
Abstract: The color of skin lesions is an important diagnostic feature for identifying malignant melanoma and other skin diseases. Typical colors associated with melanocytic lesions include tan, brown, black, red, white, and blue gray. This study introduces a novel feature: the number of colors present in a lesion, which can indicate the severity of disease and help distinguish melanomas from benign lesions. We propose a color histogram analysis method to examine lesion pixel values from three publicly available datasets: PH2, ISIC2016, and Med Node. The PH2 dataset contains ground truth annotations of lesion colors, while ISIC2016 and Med Node do not; our algorithm estimates the ground truth using color histogram analysis based on PH2. We then design and train a 19 layer Convolutional Neural Network (CNN) with residual skip connections to classify lesions into three categories based on the number of colors present. DeepDream visualization is used to interpret features learned by the network, and multiple CNN configurations are tested. The best model achieves a weighted F1 score of 75 percent. LIME is applied to identify important regions influencing model decisions. The results show that the number of colors in a lesion is a significant feature for describing skin conditions, and the proposed CNN with three skip connections demonstrates strong potential for clinical diagnostic support.</description><author>eess.IV updates on arXiv.org</author><pubDate>Fri, 30 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2601.20869v1</guid></item><item><title>[arXiv-ML] A block-coordinate descent framework for non-convex composite optimization. Application to sparse precision matrix estimation</title><link>https://arxiv.org/abs/2601.21467</link><description>arXiv:2601.21467v1 Announce Type: new 
Abstract: Block-coordinate descent (BCD) is the method of choice to solve numerous large scale optimization problems, however their theoretical study for non-convex optimization, has received less attention. In this paper, we present a new block-coordinate descent (BCD) framework to tackle non-convex composite optimization problems, ensuring decrease of the objective function and convergence to a solution. This framework is general enough to include variable metric proximal gradient updates, proximal Newton updates, and alternated minimization updates. This generality allows to encompass three versions of the most used solvers in the sparse precision matrix estimation problem, deemed Graphical Lasso: graphical ISTA, Primal GLasso, and QUIC. We demonstrate the value of this new framework on non-convex sparse precision matrix estimation problems, providing convergence guarantees and up to a $100$-fold reduction in the number of iterations required to reach state-of-the-art estimation quality.</description><author>cs.LG updates on arXiv.org</author><pubDate>Fri, 30 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2601.21467v1</guid></item><item><title>[arXiv-ML] Modeling Cascaded Delay Feedback for Online Net Conversion Rate Prediction: Benchmark, Insights and Solutions</title><link>https://arxiv.org/abs/2601.19965</link><description>arXiv:2601.19965v2 Announce Type: replace 
Abstract: In industrial recommender systems, conversion rate (CVR) is widely used for traffic allocation, but it fails to fully reflect recommendation effectiveness because it ignores refund behavior. To better capture true user satisfaction and business value, net conversion rate (NetCVR), defined as the probability that a clicked item is purchased and not refunded, has been proposed.Unlike CVR, NetCVR prediction involves a more complex multi-stage cascaded delayed feedback process. The two cascaded delays from click to conversion and from conversion to refund have opposite effects, making traditional CVR modeling methods inapplicable. Moreover, the lack of open-source datasets and online continuous training schemes further hinders progress in this area.To address these challenges, we introduce CASCADE (Cascaded Sequences of Conversion and Delayed Refund), the first large-scale open dataset derived from the Taobao app for online continuous NetCVR prediction. Through an in-depth analysis of CASCADE, we identify three key insights: (1) NetCVR exhibits strong temporal dynamics, necessitating online continuous modeling; (2) cascaded modeling of CVR and refund rate outperforms direct NetCVR modeling; and (3) delay time, which correlates with both CVR and refund rate, is an important feature for NetCVR prediction.Based on these insights, we propose TESLA, a continuous NetCVR modeling framework featuring a CVR-refund-rate cascaded architecture, stage-wise debiasing, and a delay-time-aware ranking loss. Extensive experiments demonstrate that TESLA consistently outperforms state-of-the-art methods on CASCADE, achieving absolute improvements of 12.41 percent in RI-AUC and 14.94 percent in RI-PRAUC on NetCVR prediction. The code and dataset are publicly available at https://github.com/alimama-tech/NetCVR.</description><author>cs.LG updates on arXiv.org</author><pubDate>Fri, 30 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2601.19965v2</guid></item><item><title>[arXiv-ML] A Backpropagation-Free Feedback-Hebbian Network for Continual Learning Dynamics</title><link>https://arxiv.org/abs/2601.06758</link><description>arXiv:2601.06758v2 Announce Type: replace-cross 
Abstract: Feedback-rich neural architectures can regenerate earlier representations and inject temporal context, making them a natural setting for strictly local synaptic plasticity. Existing literature raises doubt about whether a minimal, backpropagation-free feedback-Hebbian system can already express interpretable continual-learning-relevant behaviors under controlled training schedules. In this work, we introduce a compact prediction-reconstruction architecture with a dedicated feedback pathway providing lightweight, locally trainable temporal context for continual adaptation. All synapses are updated by a unified local rule combining centered Hebbian covariance, Oja-style stabilization, and a local supervised drive where targets are available. With a simple two-pair association task, learning is characterized through layer-wise activity snapshots, connectivity trajectories (row and column means of learned weights), and a normalized retention index across phases. Under sequential A to B training, forward output connectivity exhibits a long-term depression (LTD)-like suppression of the earlier association, while feedback connectivity preserves an A-related trace during acquisition of B. Under an alternating sequence, both associations are concurrently maintained rather than sequentially suppressed. Architectural controls and rule-term ablations isolate the role of dedicated feedback in regeneration and co-maintenance, alongside the role of the local supervised term in output selectivity and unlearning. Together, the results show that a compact feedback pathway trained with local plasticity can support regeneration and continual-learning-relevant dynamics in a minimal, mechanistically transparent setting.</description><author>cs.LG updates on arXiv.org</author><pubDate>Fri, 30 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2601.06758v2</guid></item><item><title>[arXiv-ML] LLMs as Orchestrators: Constraint-Compliant Multi-Agent Optimization for Recommendation Systems</title><link>https://arxiv.org/abs/2601.19121</link><description>arXiv:2601.19121v2 Announce Type: replace-cross 
Abstract: Recommendation systems must optimize multiple objectives while satisfying hard business constraints such as fairness and coverage. For example, an e-commerce platform may require every recommendation list to include items from multiple sellers and at least one newly listed product; violating such constraints--even once--is unacceptable in production. Prior work on multi-objective recommendation and recent LLM-based recommender agents largely treat constraints as soft penalties or focus on item scoring and interaction, leading to frequent violations in real-world deployments. How to leverage LLMs for coordinating constrained optimization in recommendation systems remains underexplored. We propose DualAgent-Rec, an LLM-coordinated dual-agent framework for constrained multi-objective e-commerce recommendation. The framework separates optimization into an Exploitation Agent that prioritizes accuracy under hard constraints and an Exploration Agent that promotes diversity through unconstrained Pareto search. An LLM-based coordinator adaptively allocates resources between agents based on optimization progress and constraint satisfaction, while an adaptive epsilon-relaxation mechanism guarantees feasibility of final solutions. Experiments on the Amazon Reviews 2023 dataset demonstrate that DualAgent-Rec achieves 100% constraint satisfaction and improves Pareto hypervolume by 4-6% over strong baselines, while maintaining competitive accuracy-diversity trade-offs. These results indicate that LLMs can act as effective orchestration agents for deployable and constraint-compliant recommendation systems.</description><author>cs.LG updates on arXiv.org</author><pubDate>Fri, 30 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2601.19121v2</guid></item><item><title>[arXiv-CV] Bridging the Applicator Gap with Data-Doping:Dual-Domain Learning for Precise Bladder Segmentation in CT-Guided Brachytherapy</title><link>https://arxiv.org/abs/2601.20302</link><description>arXiv:2601.20302v1 Announce Type: new 
Abstract: Performance degradation due to covariate shift remains a major challenge for deep learning models in medical image segmentation. An open question is whether samples from a shifted distribution can effectively support learning when combined with limited target domain data. We investigate this problem in the context of bladder segmentation in CT guided gynecological brachytherapy, a critical task for accurate dose optimization and organ at risk sparing. While CT scans without brachytherapy applicators (no applicator: NA) are widely available, scans with applicators inserted (with applicator: WA) are scarce and exhibit substantial anatomical deformation and imaging artifacts, making automated segmentation particularly difficult.
  We propose a dual domain learning strategy that integrates NA and WA CT data to improve robustness and generalizability under covariate shift. Using a curated assorted dataset, we show that NA data alone fail to capture the anatomical and artifact related characteristics of WA images. However, introducing a modest proportion of WA data into a predominantly NA training set leads to significant performance improvements. Through systematic experiments across axial, coronal, and sagittal planes using multiple deep learning architectures, we demonstrate that doping only 10 to 30 percent WA data achieves segmentation performance comparable to models trained exclusively on WA data.
  The proposed approach attains Dice similarity coefficients of up to 0.94 and Intersection over Union scores of up to 0.92, indicating effective domain adaptation and improved clinical reliability. This study highlights the value of integrating anatomically similar but distribution shifted datasets to overcome data scarcity and enhance deep learning based segmentation for brachytherapy treatment planning.</description><author>cs.CV updates on arXiv.org</author><pubDate>Thu, 29 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2601.20302v1</guid></item><item><title>[arXiv-CV] EgoLife: Towards Egocentric Life Assistant</title><link>https://arxiv.org/abs/2503.03803</link><description>arXiv:2503.03803v2 Announce Type: replace 
Abstract: We introduce EgoLife, a project to develop an egocentric life assistant that accompanies and enhances personal efficiency through AI-powered wearable glasses. To lay the foundation for this assistant, we conducted a comprehensive data collection study where six participants lived together for one week, continuously recording their daily activities - including discussions, shopping, cooking, socializing, and entertainment - using AI glasses for multimodal egocentric video capture, along with synchronized third-person-view video references. This effort resulted in the EgoLife Dataset, a comprehensive 300-hour egocentric, interpersonal, multiview, and multimodal daily life dataset with intensive annotation. Leveraging this dataset, we introduce EgoLifeQA, a suite of long-context, life-oriented question-answering tasks designed to provide meaningful assistance in daily life by addressing practical questions such as recalling past relevant events, monitoring health habits, and offering personalized recommendations. To address the key technical challenges of (1) developing robust visual-audio models for egocentric data, (2) enabling identity recognition, and (3) facilitating long-context question answering over extensive temporal information, we introduce EgoButler, an integrated system comprising EgoGPT and EgoRAG. EgoGPT is an omni-modal model trained on egocentric datasets, achieving state-of-the-art performance on egocentric video understanding. EgoRAG is a retrieval-based component that supports answering ultra-long-context questions. Our experimental studies verify their working mechanisms and reveal critical factors and bottlenecks, guiding future improvements. By releasing our datasets, models, and benchmarks, we aim to stimulate further research in egocentric AI assistants.</description><author>cs.CV updates on arXiv.org</author><pubDate>Thu, 29 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2503.03803v2</guid></item><item><title>[arXiv-CV] DAUNet: A Lightweight UNet Variant with Deformable Convolutions and Parameter-Free Attention for Medical Image Segmentation</title><link>https://arxiv.org/abs/2512.07051</link><description>arXiv:2512.07051v2 Announce Type: replace 
Abstract: Medical image segmentation plays a pivotal role in automated diagnostic and treatment planning systems. In this work, we present DAUNet, a novel lightweight UNet variant that integrates Deformable V2 Convolutions and Parameter-Free Attention (SimAM) to improve spatial adaptability and context-aware feature fusion without increasing model complexity. DAUNet's bottleneck employs dynamic deformable kernels to handle geometric variations, while the decoder and skip pathways are enhanced using SimAM attention modules for saliency-aware refinement. Extensive evaluations on two challenging datasets, FH-PS-AoP (fetal head and pubic symphysis ultrasound) and FUMPE (CT-based pulmonary embolism detection), demonstrate that DAUNet outperforms state-of-the-art models in Dice score, HD95, and ASD, while maintaining superior parameter efficiency. Ablation studies highlight the individual contributions of deformable convolutions and SimAM attention. DAUNet's robustness to missing context and low-contrast regions establishes its suitability for deployment in real-time and resource-constrained clinical environments.</description><author>cs.CV updates on arXiv.org</author><pubDate>Thu, 29 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2512.07051v2</guid></item><item><title>[arXiv-CV] From Specialist to Generalist: Unlocking SAM's Learning Potential on Unlabeled Medical Images</title><link>https://arxiv.org/abs/2601.17934</link><description>arXiv:2601.17934v2 Announce Type: replace 
Abstract: Foundation models like the Segment Anything Model (SAM) show strong generalization, yet adapting them to medical images remains difficult due to domain shift, scarce labels, and the inability of Parameter-Efficient Fine-Tuning (PEFT) to exploit unlabeled data. While conventional models like U-Net excel in semi-supervised medical learning, their potential to assist a PEFT SAM has been largely overlooked. We introduce SC-SAM, a specialist-generalist framework where U-Net provides point-based prompts and pseudo-labels to guide SAM's adaptation, while SAM serves as a powerful generalist supervisor to regularize U-Net. This reciprocal guidance forms a bidirectional co-training loop that allows both models to effectively exploit the unlabeled data. Across prostate MRI and polyp segmentation benchmarks, our method achieves state-of-the-art results, outperforming other existing semi-supervised SAM variants and even medical foundation models like MedSAM, highlighting the value of specialist-generalist cooperation for label-efficient medical image segmentation. Our code is available at https://github.com/vnlvi2k3/SC-SAM.</description><author>cs.CV updates on arXiv.org</author><pubDate>Thu, 29 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2601.17934v2</guid></item><item><title>[arXiv-ML] Modeling Cascaded Delay Feedback for Online Net Conversion Rate Prediction: Benchmark, Insights and Solutions</title><link>https://arxiv.org/abs/2601.19965</link><description>arXiv:2601.19965v1 Announce Type: new 
Abstract: In industrial recommender systems, conversion rate (CVR) is widely used for traffic allocation, but it fails to fully reflect recommendation effectiveness because it ignores refund behavior. To better capture true user satisfaction and business value, net conversion rate (NetCVR), defined as the probability that a clicked item is purchased and not refunded, has been proposed.Unlike CVR, NetCVR prediction involves a more complex multi-stage cascaded delayed feedback process. The two cascaded delays from click to conversion and from conversion to refund have opposite effects, making traditional CVR modeling methods inapplicable. Moreover, the lack of open-source datasets and online continuous training schemes further hinders progress in this area.To address these challenges, we introduce CASCADE (Cascaded Sequences of Conversion and Delayed Refund), the first large-scale open dataset derived from the Taobao app for online continuous NetCVR prediction. Through an in-depth analysis of CASCADE, we identify three key insights: (1) NetCVR exhibits strong temporal dynamics, necessitating online continuous modeling; (2) cascaded modeling of CVR and refund rate outperforms direct NetCVR modeling; and (3) delay time, which correlates with both CVR and refund rate, is an important feature for NetCVR prediction.Based on these insights, we propose TESLA, a continuous NetCVR modeling framework featuring a CVR-refund-rate cascaded architecture, stage-wise debiasing, and a delay-time-aware ranking loss. Extensive experiments demonstrate that TESLA consistently outperforms state-of-the-art methods on CASCADE, achieving absolute improvements of 12.41 percent in RI-AUC and 14.94 percent in RI-PRAUC on NetCVR prediction. The code and dataset are publicly available at https://github.com/alimama-tech/NetCVR.</description><author>cs.LG updates on arXiv.org</author><pubDate>Thu, 29 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2601.19965v1</guid></item><item><title>[arXiv-ML] A Learning-based Framework for Spatial Impulse Response Compensation in 3D Photoacoustic Computed Tomography</title><link>https://arxiv.org/abs/2601.20291</link><description>arXiv:2601.20291v1 Announce Type: new 
Abstract: Photoacoustic computed tomography (PACT) is a promising imaging modality that combines the advantages of optical contrast with ultrasound detection. Utilizing ultrasound transducers with larger surface areas can improve detection sensitivity. However, when computationally efficient analytic reconstruction methods that neglect the spatial impulse responses (SIRs) of the transducer are employed, the spatial resolution of the reconstructed images will be compromised. Although optimization-based reconstruction methods can explicitly account for SIR effects, their computational cost is generally high, particularly in three-dimensional (3D) applications. To address the need for accurate but rapid 3D PACT image reconstruction, this study presents a framework for establishing a learned SIR compensation method that operates in the data domain. The learned compensation method maps SIR-corrupted PACT measurement data to compensated data that would have been recorded by idealized point-like transducers. Subsequently, the compensated data can be used with a computationally efficient reconstruction method that neglects SIR effects. Two variants of the learned compensation model are investigated that employ a U-Net model and a specifically designed, physics-inspired model, referred to as Deconv-Net. A fast and analytical training data generation procedure is also a component of the presented framework. The framework is rigorously validated in virtual imaging studies, demonstrating resolution improvement and robustness to noise variations, object complexity, and sound speed heterogeneity. When applied to in-vivo breast imaging data, the learned compensation models revealed fine structures that had been obscured by SIR-induced artifacts. To our knowledge, this is the first demonstration of learned SIR compensation in 3D PACT imaging.</description><author>cs.LG updates on arXiv.org</author><pubDate>Thu, 29 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2601.20291v1</guid></item><item><title>[arXiv-ML] Delayed Feedback Modeling for Post-Click Gross Merchandise Volume Prediction: Benchmark, Insights and Approaches</title><link>https://arxiv.org/abs/2601.20307</link><description>arXiv:2601.20307v1 Announce Type: new 
Abstract: The prediction objectives of online advertisement ranking models are evolving from probabilistic metrics like conversion rate (CVR) to numerical business metrics like post-click gross merchandise volume (GMV). Unlike the well-studied delayed feedback problem in CVR prediction, delayed feedback modeling for GMV prediction remains unexplored and poses greater challenges, as GMV is a continuous target, and a single click can lead to multiple purchases that cumulatively form the label. To bridge the research gap, we establish TRACE, a GMV prediction benchmark containing complete transaction sequences rising from each user click, which supports delayed feedback modeling in an online streaming manner. Our analysis and exploratory experiments on TRACE reveal two key insights: (1) the rapid evolution of the GMV label distribution necessitates modeling delayed feedback under online streaming training; (2) the label distribution of repurchase samples substantially differs from that of single-purchase samples, highlighting the need for separate modeling. Motivated by these findings, we propose RepurchasE-Aware Dual-branch prEdictoR (READER), a novel GMV modeling paradigm that selectively activates expert parameters according to repurchase predictions produced by a router. Moreover, READER dynamically calibrates the regression target to mitigate under-estimation caused by incomplete labels. Experimental results show that READER yields superior performance on TRACE over baselines, achieving a 2.19% improvement in terms of accuracy. We believe that our study will open up a new avenue for studying online delayed feedback modeling for GMV prediction, and our TRACE benchmark with the gathered insights will facilitate future research and application in this promising direction. Our code and dataset are available at https://github.com/alimama-tech/OnlineGMV .</description><author>cs.LG updates on arXiv.org</author><pubDate>Thu, 29 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2601.20307v1</guid></item><item><title>[arXiv-ML] Less is More: Clustered Cross-Covariance Control for Offline RL</title><link>https://arxiv.org/abs/2601.20765</link><description>arXiv:2601.20765v1 Announce Type: new 
Abstract: A fundamental challenge in offline reinforcement learning is distributional shift. Scarce data or datasets dominated by out-of-distribution (OOD) areas exacerbate this issue. Our theoretical analysis and experiments show that the standard squared error objective induces a harmful TD cross covariance. This effect amplifies in OOD areas, biasing optimization and degrading policy learning. To counteract this mechanism, we develop two complementary strategies: partitioned buffer sampling that restricts updates to localized replay partitions, attenuates irregular covariance effects, and aligns update directions, yielding a scheme that is easy to integrate with existing implementations, namely Clustered Cross-Covariance Control for TD (C^4). We also introduce an explicit gradient-based corrective penalty that cancels the covariance induced bias within each update. We prove that buffer partitioning preserves the lower bound property of the maximization objective, and that these constraints mitigate excessive conservatism in extreme OOD areas without altering the core behavior of policy constrained offline reinforcement learning. Empirically, our method showcases higher stability and up to 30% improvement in returns over prior methods, especially with small datasets and splits that emphasize OOD areas.</description><author>cs.LG updates on arXiv.org</author><pubDate>Thu, 29 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2601.20765v1</guid></item><item><title>[arXiv-ML] Machine learning surrogate models of many-body dispersion interactions in polymer melts</title><link>https://arxiv.org/abs/2503.15149</link><description>arXiv:2503.15149v2 Announce Type: replace 
Abstract: Accurate prediction of many-body dispersion (MBD) interactions is essential for understanding the van der Waals forces that govern the behavior of many complex molecular systems. However, the high computational cost of MBD calculations limits their direct application in large-scale simulations. In this work, we introduce a machine learning surrogate model specifically designed to predict MBD forces in polymer melts, a system that demands accurate MBD description and offers structural advantages for machine learning approaches. Our model is based on a trimmed SchNet architecture that selectively retains the most relevant atomic connections and incorporates trainable radial basis functions for geometric encoding. We validate our surrogate model on datasets from polyethylene, polypropylene, and polyvinyl chloride melts, demonstrating high predictive accuracy and robust generalization across diverse polymer systems. In addition, the model captures key physical features, such as the characteristic decay behavior of MBD interactions, providing valuable insights for optimizing cutoff strategies. Characterized by high computational efficiency, our surrogate model enables practical incorporation of MBD effects into large-scale molecular simulations.</description><author>cs.LG updates on arXiv.org</author><pubDate>Thu, 29 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2503.15149v2</guid></item><item><title>[MedIA] DPFR: Semi-Supervised Gland Segmentation via Density Perturbation and Feature Recalibration</title><link>https://www.sciencedirect.com/science/article/pii/S1361841526000319?dgcid=rss_sd_all</link><description>&lt;p&gt;Publication date: Available online 27 January 2026&lt;/p&gt;&lt;p&gt;&lt;b&gt;Source:&lt;/b&gt; Medical Image Analysis&lt;/p&gt;&lt;p&gt;Author(s): Jiejiang Yu, Yu Liu&lt;/p&gt;</description><author>ScienceDirect Publication: Medical Image Analysis</author><pubDate>Thu, 29 Jan 2026 02:29:26 GMT</pubDate><guid isPermaLink="true">https://www.sciencedirect.com/science/article/pii/S1361841526000319</guid></item><item><title>[MedIA] A navigation-guided 3D breast ultrasound scanning and reconstruction system for automated multi-lesion spatial localization and diagnosis</title><link>https://www.sciencedirect.com/science/article/pii/S1361841526000344?dgcid=rss_sd_all</link><description>&lt;p&gt;Publication date: Available online 28 January 2026&lt;/p&gt;&lt;p&gt;&lt;b&gt;Source:&lt;/b&gt; Medical Image Analysis&lt;/p&gt;&lt;p&gt;Author(s): Yi Zhang, Yulin Yan, Kun Wang, Muyu Cai, Yifei Xiang, Yan Guo, Puxun Tu, Tao Ying, Xiaojun Chen&lt;/p&gt;</description><author>ScienceDirect Publication: Medical Image Analysis</author><pubDate>Thu, 29 Jan 2026 02:29:26 GMT</pubDate><guid isPermaLink="true">https://www.sciencedirect.com/science/article/pii/S1361841526000344</guid></item><item><title>[BSPC] DCAO: enhanced AO for breast cancer histopathological segmentation</title><link>https://www.sciencedirect.com/science/article/pii/S1746809426002119?dgcid=rss_sd_all</link><description>&lt;p&gt;Publication date: 1 June 2026&lt;/p&gt;&lt;p&gt;&lt;b&gt;Source:&lt;/b&gt; Biomedical Signal Processing and Control, Volume 118&lt;/p&gt;&lt;p&gt;Author(s): Hongliang Guo, Xinyue Zhang, Ruihang Zhang, Yufei Liu, Helong Yu, Huiling Chen&lt;/p&gt;</description><author>ScienceDirect Publication: Biomedical Signal Processing and Control</author><pubDate>Wed, 28 Jan 2026 18:43:31 GMT</pubDate><guid isPermaLink="true">https://www.sciencedirect.com/science/article/pii/S1746809426002119</guid></item><item><title>[BSPC] Bridge-connected dual encoders network for multi-scale targets in 2D medical image segmentation</title><link>https://www.sciencedirect.com/science/article/pii/S1746809426001928?dgcid=rss_sd_all</link><description>&lt;p&gt;Publication date: 1 June 2026&lt;/p&gt;&lt;p&gt;&lt;b&gt;Source:&lt;/b&gt; Biomedical Signal Processing and Control, Volume 118&lt;/p&gt;&lt;p&gt;Author(s): Shiyu Zhu, Weixuan Wan, Junyang Han, Jiacheng Nie, Hongzhi Liu, Huazhong Shu, Yang Chen, Shipeng Xie, Hui Tang&lt;/p&gt;</description><author>ScienceDirect Publication: Biomedical Signal Processing and Control</author><pubDate>Wed, 28 Jan 2026 18:43:31 GMT</pubDate><guid isPermaLink="true">https://www.sciencedirect.com/science/article/pii/S1746809426001928</guid></item><item><title>[PR] Adaptive knowledge transferring with switching dual-student framework for semi-supervised medical image segmentation</title><link>https://www.sciencedirect.com/science/article/pii/S0031320326000786?dgcid=rss_sd_all</link><description>&lt;p&gt;Publication date: July 2026&lt;/p&gt;&lt;p&gt;&lt;b&gt;Source:&lt;/b&gt; Pattern Recognition, Volume 175&lt;/p&gt;&lt;p&gt;Author(s): Hoang-Thien Nguyen, Thanh-Huy Nguyen, Ba-Thinh Lam, Vi Vu, Bach X. Nguyen, Jianhua Xing, Tianyang Wang, Xingjian Li, Min Xu&lt;/p&gt;</description><author>ScienceDirect Publication: Pattern Recognition</author><pubDate>Wed, 28 Jan 2026 18:43:29 GMT</pubDate><guid isPermaLink="true">https://www.sciencedirect.com/science/article/pii/S0031320326000786</guid></item><item><title>[arXiv-CV] Anatomically-aware conformal prediction for medical image segmentation with random walks</title><link>https://arxiv.org/abs/2601.18997</link><description>arXiv:2601.18997v1 Announce Type: new 
Abstract: The reliable deployment of deep learning in medical imaging requires uncertainty quantification that provides rigorous error guarantees while remaining anatomically meaningful. Conformal prediction (CP) is a powerful distribution-free framework for constructing statistically valid prediction intervals. However, standard applications in segmentation often ignore anatomical context, resulting in fragmented, spatially incoherent, and over-segmented prediction sets that limit clinical utility. To bridge this gap, this paper proposes Random-Walk Conformal Prediction (RW-CP), a model-agnostic framework which can be added on top of any segmentation method. RW-CP enforces spatial coherence to generate anatomically valid sets. Our method constructs a k-nearest neighbour graph from pre-trained vision foundation model features and applies a random walk to diffuse uncertainty. The random walk diffusion regularizes the non-conformity scores, making the prediction sets less sensitive to the conformal calibration parameter $\lambda$, ensuring more stable and continuous anatomical boundaries. RW-CP maintains rigorous marginal coverage while significantly improving segmentation quality. Evaluations on multi-modal public datasets show improvements of up to $35.4\%$ compared to standard CP baselines, given an allowable error rate of $\alpha=0.1$.</description><author>cs.CV updates on arXiv.org</author><pubDate>Wed, 28 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2601.18997v1</guid></item><item><title>[arXiv-CV] Pareto-Guided Optimization for Uncertainty-Aware Medical Image Segmentation</title><link>https://arxiv.org/abs/2601.19365</link><description>arXiv:2601.19365v1 Announce Type: new 
Abstract: Uncertainty in medical image segmentation is inherently non-uniform, with boundary regions exhibiting substantially higher ambiguity than interior areas. Conventional training treats all pixels equally, leading to unstable optimization during early epochs when predictions are unreliable. We argue that this instability hinders convergence toward Pareto-optimal solutions and propose a region-wise curriculum strategy that prioritizes learning from certain regions and gradually incorporates uncertain ones, reducing gradient variance. Methodologically, we introduce a Pareto-consistent loss that balances trade-offs between regional uncertainties by adaptively reshaping the loss landscape and constraining convergence dynamics between interior and boundary regions; this guides the model toward Pareto-approximate solutions. To address boundary ambiguity, we further develop a fuzzy labeling mechanism that maintains binary confidence in non-boundary areas while enabling smooth transitions near boundaries, stabilizing gradients, and expanding flat regions in the loss surface. Experiments on brain metastasis and non-metastatic tumor segmentation show consistent improvements across multiple configurations, with our method outperforming traditional crisp-set approaches in all tumor subregions.</description><author>cs.CV updates on arXiv.org</author><pubDate>Wed, 28 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2601.19365v1</guid></item><item><title>[arXiv-CV] DSVM-UNet : Enhancing VM-UNet with Dual Self-distillation for Medical Image Segmentation</title><link>https://arxiv.org/abs/2601.19690</link><description>arXiv:2601.19690v1 Announce Type: new 
Abstract: Vision Mamba models have been extensively researched in various fields, which address the limitations of previous models by effectively managing long-range dependencies with a linear-time overhead. Several prospective studies have further designed Vision Mamba based on UNet(VM-UNet) for medical image segmentation. These approaches primarily focus on optimizing architectural designs by creating more complex structures to enhance the model's ability to perceive semantic features. In this paper, we propose a simple yet effective approach to improve the model by Dual Self-distillation for VM-UNet (DSVM-UNet) without any complex architectural designs. To achieve this goal, we develop double self-distillation methods to align the features at both the global and local levels. Extensive experiments conducted on the ISIC2017, ISIC2018, and Synapse benchmarks demonstrate that our approach achieves state-of-the-art performance while maintaining computational efficiency. Code is available at https://github.com/RoryShao/DSVM-UNet.git.</description><author>cs.CV updates on arXiv.org</author><pubDate>Wed, 28 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2601.19690v1</guid></item><item><title>[arXiv-CV] AMGFormer: Adaptive Multi-Granular Transformer for Brain Tumor Segmentation with Missing Modalities</title><link>https://arxiv.org/abs/2601.19349</link><description>arXiv:2601.19349v1 Announce Type: cross 
Abstract: Multimodal MRI is essential for brain tumor segmentation, yet missing modalities in clinical practice cause existing methods to exhibit &gt;40% performance variance across modality combinations, rendering them clinically unreliable. We propose AMGFormer, achieving significantly improved stability through three synergistic modules: (1) QuadIntegrator Bridge (QIB) enabling spatially adaptive fusion maintaining consistent predictions regardless of available modalities, (2) Multi-Granular Attention Orchestrator (MGAO) focusing on pathological regions to reduce background sensitivity, and (3) Modality Quality-Aware Enhancement (MQAE) preventing error propagation from corrupted sequences. On BraTS 2018, our method achieves 89.33% WT, 82.70% TC, 67.23% ET Dice scores with &lt;0.5% variance across 15 modality combinations, solving the stability crisis. Single-modality ET segmentation shows 40-81% relative improvements over state-of-the-art methods. The method generalizes to BraTS 2020/2021, achieving up to 92.44% WT, 89.91% TC, 84.57% ET. The model demonstrates potential for clinical deployment with 1.2s inference. Code: https://github.com/guochengxiangives/AMGFormer.</description><author>cs.CV updates on arXiv.org</author><pubDate>Wed, 28 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2601.19349v1</guid></item><item><title>[arXiv-CV] Semantic-aware Random Convolution and Source Matching for Domain Generalization in Medical Image Segmentation</title><link>https://arxiv.org/abs/2512.01510</link><description>arXiv:2512.01510v2 Announce Type: replace 
Abstract: We tackle the challenging problem of single-source domain generalization (DG) for medical image segmentation, where we train a network on one domain (e.g., CT) and directly apply it to a different domain (e.g., MR) without adapting the model and without requiring images or annotations from the new domain during training. Our method diversifies the source domain through semantic-aware random convolution, where different regions of a source image are augmented differently at training-time, based on their annotation labels. At test-time, we complement the randomization of the training domain via mapping the intensity of target domain images, making them similar to source domain data. We perform a comprehensive evaluation on a variety of cross-modality and cross-center generalization settings for abdominal, whole-heart and prostate segmentation, where we outperform previous DG techniques in a vast majority of experiments. Additionally, we also investigate our method when training on whole-heart CT or MR data and testing on the diastolic and systolic phase of cine MR data captured with different scanner hardware. Overall, our evaluation shows that our method achieves new state-of-the-art performance in DG for medical image segmentation, even matching the performance of the in-domain baseline in several settings. We will release our source code upon acceptance of this manuscript.</description><author>cs.CV updates on arXiv.org</author><pubDate>Wed, 28 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2512.01510v2</guid></item><item><title>[arXiv-ML] LLM-Assisted Logic Rule Learning: Scaling Human Expertise for Time Series Anomaly Detection</title><link>https://arxiv.org/abs/2601.19255</link><description>arXiv:2601.19255v1 Announce Type: new 
Abstract: Time series anomaly detection is critical for supply chain management to take proactive operations, but faces challenges: classical unsupervised anomaly detection based on exploiting data patterns often yields results misaligned with business requirements and domain knowledge, while manual expert analysis cannot scale to millions of products in the supply chain. We propose a framework that leverages large language models (LLMs) to systematically encode human expertise into interpretable, logic-based rules for detecting anomaly patterns in supply chain time series data. Our approach operates in three stages: 1) LLM-based labeling of training data instructed by domain knowledge, 2) automated generation and iterative improvements of symbolic rules through LLM-driven optimization, and 3) rule augmentation with business-relevant anomaly categories supported by LLMs to enhance interpretability. The experiment results showcase that our approach outperforms the unsupervised learning methods in both detection accuracy and interpretability. Furthermore, compared to direct LLM deployment for time series anomaly detection, our approach provides consistent, deterministic results with low computational latency and cost, making it ideal for production deployment. The proposed framework thus demonstrates how LLMs can bridge the gap between scalable automation and expert-driven decision-making in operational settings.</description><author>cs.LG updates on arXiv.org</author><pubDate>Wed, 28 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2601.19255v1</guid></item><item><title>[arXiv-ML] Process-Aware Procurement Lead Time Prediction for Shipyard Delay Mitigation</title><link>https://arxiv.org/abs/2601.19296</link><description>arXiv:2601.19296v1 Announce Type: new 
Abstract: Accurately predicting procurement lead time (PLT) remains a challenge in engineered-to-order industries such as shipbuilding and plant construction, where delays in a single key component can disrupt project timelines. In shipyards, pipe spools are critical components; installed deep within hull blocks soon after steel erection, any delay in their procurement can halt all downstream tasks. Recognizing their importance, existing studies predict PLT using the static physical attributes of pipe spools. However, procurement is inherently a dynamic, multi-stakeholder business process involving a continuous sequence of internal and external events at the shipyard, factors often overlooked in traditional approaches. To address this issue, this paper proposes a novel framework that combines event logs, dataset records of the procurement events, with static attributes to predict PLT. The temporal attributes of each event are extracted to reflect the continuity and temporal context of the process. Subsequently, a deep sequential neural network combined with a multi-layered perceptron is employed to integrate these static and dynamic features, enabling the model to capture both structural and contextual information in procurement. Comparative experiments are conducted using real-world pipe spool procurement data from a globally renowned South Korean shipbuilding corporation. Three tasks are evaluated, which are production, post-processing, and procurement lead time prediction. The results show a 22.6% to 50.4% improvement in prediction performance in terms of mean absolute error over the best-performing existing approaches across the three tasks. These findings indicate the value of considering procurement process information for more accurate PLT prediction.</description><author>cs.LG updates on arXiv.org</author><pubDate>Wed, 28 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2601.19296v1</guid></item><item><title>[arXiv-ML] Statistical Inference for Explainable Boosting Machines</title><link>https://arxiv.org/abs/2601.18857</link><description>arXiv:2601.18857v1 Announce Type: cross 
Abstract: Explainable boosting machines (EBMs) are popular "glass-box" models that learn a set of univariate functions using boosting trees. These achieve explainability through visualizations of each feature's effect. However, unlike linear model coefficients, uncertainty quantification for the learned univariate functions requires computationally intensive bootstrapping, making it hard to know which features truly matter. We provide an alternative using recent advances in statistical inference for gradient boosting, deriving methods for statistical inference as well as end-to-end theoretical guarantees. Using a moving average instead of a sum of trees (Boulevard regularization) allows the boosting process to converge to a feature-wise kernel ridge regression. This produces asymptotically normal predictions that achieve the minimax-optimal mean squared error for fitting Lipschitz GAMs with $p$ features at rate $O(pn^{-2/3})$, successfully avoiding the curse of dimensionality. We then construct prediction intervals for the response and confidence intervals for each learned univariate function with a runtime independent of the number of datapoints, enabling further explainability within EBMs.</description><author>cs.LG updates on arXiv.org</author><pubDate>Wed, 28 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2601.18857v1</guid></item><item><title>[arXiv-ML] C2NP: A Benchmark for Learning Scale-Dependent Geometric Invariances in 3D Materials Generation</title><link>https://arxiv.org/abs/2601.19076</link><description>arXiv:2601.19076v1 Announce Type: cross 
Abstract: Generative models for materials have achieved strong performance on periodic bulk crystals, yet their ability to generalize across scale transitions to finite nanostructures remains largely untested. We introduce Crystal-to-Nanoparticle (C2NP), a systematic benchmark for evaluating generative models when moving between infinite crystalline unit cells and finite nanoparticles, where surface effects and size-dependent distortions dominate. C2NP defines two complementary tasks: (i) generating nanoparticles of specified radii from periodic unit cells, testing whether models capture surface truncation and geometric constraints; and (ii) recovering bulk lattice parameters and space-group symmetry from finite particle configurations, assessing whether models can infer underlying crystallographic order despite surface perturbations. Using diverse materials as a structurally consistent testbed, we construct over 170,000 nanoparticle configurations by carving particles from supercells derived from DFT-relaxed crystal unit cells, and introduce size-based splits that separate interpolation from extrapolation regimes. Experiments with state-of-the-art approaches, including diffusion, flow-matching, and variational models, show that even when losses are low, models often fail geometrically under distribution shift, yielding large lattice-recovery errors and near-zero joint accuracy on structure and symmetry. Overall, our results suggest that current methods rely on template memorization rather than scalable physical generalization. C2NP offers a controlled, reproducible framework for diagnosing these failures, with immediate applications to nanoparticle catalyst design, nanostructured hydrides for hydrogen storage, and materials discovery. Dataset and code are available at https://github.com/KurbanIntelligenceLab/C2NP.</description><author>cs.LG updates on arXiv.org</author><pubDate>Wed, 28 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2601.19076v1</guid></item><item><title>[arXiv-ML] LLMs as Orchestrators: Constraint-Compliant Multi-Agent Optimization for Recommendation Systems</title><link>https://arxiv.org/abs/2601.19121</link><description>arXiv:2601.19121v1 Announce Type: cross 
Abstract: Recommendation systems must optimize multiple objectives while satisfying hard business constraints such as fairness and coverage. For example, an e-commerce platform may require every recommendation list to include items from multiple sellers and at least one newly listed product; violating such constraints--even once--is unacceptable in production. Prior work on multi-objective recommendation and recent LLM-based recommender agents largely treat constraints as soft penalties or focus on item scoring and interaction, leading to frequent violations in real-world deployments. How to leverage LLMs for coordinating constrained optimization in recommendation systems remains underexplored. We propose DualAgent-Rec, an LLM-coordinated dual-agent framework for constrained multi-objective e-commerce recommendation. The framework separates optimization into an Exploitation Agent that prioritizes accuracy under hard constraints and an Exploration Agent that promotes diversity through unconstrained Pareto search. An LLM-based coordinator adaptively allocates resources between agents based on optimization progress and constraint satisfaction, while an adaptive epsilon-relaxation mechanism guarantees feasibility of final solutions. Experiments on the Amazon Reviews 2023 dataset demonstrate that DualAgent-Rec achieves 100% constraint satisfaction and improves Pareto hypervolume by 4-6% over strong baselines, while maintaining competitive accuracy-diversity trade-offs. These results indicate that LLMs can act as effective orchestration agents for deployable and constraint-compliant recommendation systems.</description><author>cs.LG updates on arXiv.org</author><pubDate>Wed, 28 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2601.19121v1</guid></item><item><title>[arXiv-CV] Data-Efficient Meningioma Segmentation via Implicit Spatiotemporal Mixing and Sim2Real Semantic Injection</title><link>https://arxiv.org/abs/2601.17031</link><description>arXiv:2601.17031v1 Announce Type: new 
Abstract: The performance of medical image segmentation is increasingly defined by the efficiency of data utilization rather than merely the volume of raw data. Accurate segmentation, particularly for complex pathologies like meningiomas, demands that models fully exploit the latent information within limited high-quality annotations. To maximize the value of existing datasets, we propose a novel dual-augmentation framework that synergistically integrates spatial manifold expansion and semantic object injection. Specifically, we leverage Implicit Neural Representations (INR) to model continuous velocity fields. Unlike previous methods, we perform linear mixing on the integrated deformation fields, enabling the efficient generation of anatomically plausible variations by interpolating within the deformation space. This approach allows for the extensive exploration of structural diversity from a small set of anchors. Furthermore, we introduce a Sim2Real lesion injection module. This module constructs a high-fidelity simulation domain by transplanting lesion textures into healthy anatomical backgrounds, effectively bridging the gap between synthetic augmentation and real-world pathology. Comprehensive experiments on a hybrid dataset demonstrate that our framework significantly enhances the data efficiency and robustness of state-of-the-art models, including nnU-Net and U-Mamba, offering a potent strategy for high-performance medical image analysis with limited annotation budgets.</description><author>cs.CV updates on arXiv.org</author><pubDate>Tue, 27 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2601.17031v1</guid></item><item><title>[arXiv-CV] GlassesGB: Controllable 2D GAN-Based Eyewear Personalization for 3D Gaussian Blendshapes Head Avatars</title><link>https://arxiv.org/abs/2601.17088</link><description>arXiv:2601.17088v1 Announce Type: new 
Abstract: Virtual try-on systems allow users to interactively try different products within VR scenarios. However, most existing VTON methods operate only on predefined eyewear templates and lack support for fine-grained, user-driven customization. While GlassesGAN enables personalized 2D eyewear design, its capability remains limited to 2D image generation. Motivated by the success of 3D Gaussian Blendshapes in head reconstruction, we integrate these two techniques and propose GlassesGB, a framework that supports customizable eyewear generation for 3D head avatars. GlassesGB effectively bridges 2D generative customization with 3D head avatar rendering, addressing the challenge in achieving personalized eyewear design for VR applications. The implementation code is available at https://ruiyangju.github.io/GlassesGB.</description><author>cs.CV updates on arXiv.org</author><pubDate>Tue, 27 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2601.17088v1</guid></item><item><title>[arXiv-CV] Performance uncertainty in medical image analysis: a large-scale investigation of confidence intervals</title><link>https://arxiv.org/abs/2601.17103</link><description>arXiv:2601.17103v1 Announce Type: new 
Abstract: Performance uncertainty quantification is essential for reliable validation and eventual clinical translation of medical imaging artificial intelligence (AI). Confidence intervals (CIs) play a central role in this process by indicating how precise a reported performance estimate is. Yet, due to the limited amount of work examining CI behavior in medical imaging, the community remains largely unaware of how many diverse CI methods exist and how they behave in specific settings. The purpose of this study is to close this gap. To this end, we conducted a large-scale empirical analysis across a total of 24 segmentation and classification tasks, using 19 trained models per task group, a broad spectrum of commonly used performance metrics, multiple aggregation strategies, and several widely adopted CI methods. Reliability (coverage) and precision (width) of each CI method were estimated across all settings to characterize their dependence on study characteristics. Our analysis revealed five principal findings: 1) the sample size required for reliable CIs varies from a few dozens to several thousands of cases depending on study parameters; 2) CI behavior is strongly affected by the choice of performance metric; 3) aggregation strategy substantially influences the reliability of CIs, e.g. they require more observations for macro than for micro; 4) the machine learning problem (segmentation versus classification) modulates these effects; 5) different CI methods are not equally reliable and precise depending on the use case. These results form key components for the development of future guidelines on reporting performance uncertainty in medical imaging AI.</description><author>cs.CV updates on arXiv.org</author><pubDate>Tue, 27 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2601.17103v1</guid></item><item><title>[arXiv-CV] StealthMark: Harmless and Stealthy Ownership Verification for Medical Segmentation via Uncertainty-Guided Backdoors</title><link>https://arxiv.org/abs/2601.17107</link><description>arXiv:2601.17107v1 Announce Type: new 
Abstract: Annotating medical data for training AI models is often costly and limited due to the shortage of specialists with relevant clinical expertise. This challenge is further compounded by privacy and ethical concerns associated with sensitive patient information. As a result, well-trained medical segmentation models on private datasets constitute valuable intellectual property requiring robust protection mechanisms. Existing model protection techniques primarily focus on classification and generative tasks, while segmentation models-crucial to medical image analysis-remain largely underexplored. In this paper, we propose a novel, stealthy, and harmless method, StealthMark, for verifying the ownership of medical segmentation models under black-box conditions. Our approach subtly modulates model uncertainty without altering the final segmentation outputs, thereby preserving the model's performance. To enable ownership verification, we incorporate model-agnostic explanation methods, e.g. LIME, to extract feature attributions from the model outputs. Under specific triggering conditions, these explanations reveal a distinct and verifiable watermark. We further design the watermark as a QR code to facilitate robust and recognizable ownership claims. We conducted extensive experiments across four medical imaging datasets and five mainstream segmentation models. The results demonstrate the effectiveness, stealthiness, and harmlessness of our method on the original model's segmentation performance. For example, when applied to the SAM model, StealthMark consistently achieved ASR above 95% across various datasets while maintaining less than a 1% drop in Dice and AUC scores, significantly outperforming backdoor-based watermarking methods and highlighting its strong potential for practical deployment. Our implementation code is made available at: https://github.com/Qinkaiyu/StealthMark.</description><author>cs.CV updates on arXiv.org</author><pubDate>Tue, 27 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2601.17107v1</guid></item><item><title>[arXiv-CV] Learning with Geometric Priors in U-Net Variants for Polyp Segmentation</title><link>https://arxiv.org/abs/2601.17331</link><description>arXiv:2601.17331v1 Announce Type: new 
Abstract: Accurate and robust polyp segmentation is essential for early colorectal cancer detection and for computer-aided diagnosis. While convolutional neural network-, Transformer-, and Mamba-based U-Net variants have achieved strong performance, they still struggle to capture geometric and structural cues, especially in low-contrast or cluttered colonoscopy scenes. To address this challenge, we propose a novel Geometric Prior-guided Module (GPM) that injects explicit geometric priors into U-Net-based architectures for polyp segmentation. Specifically, we fine-tune the Visual Geometry Grounded Transformer (VGGT) on a simulated ColonDepth dataset to estimate depth maps of polyp images tailored to the endoscopic domain. These depth maps are then processed by GPM to encode geometric priors into the encoder's feature maps, where they are further refined using spatial and channel attention mechanisms that emphasize both local spatial and global channel information. GPM is plug-and-play and can be seamlessly integrated into diverse U-Net variants. Extensive experiments on five public polyp segmentation datasets demonstrate consistent gains over three strong baselines. Code and the generated depth maps are available at: https://github.com/fvazqu/GPM-PolypSeg</description><author>cs.CV updates on arXiv.org</author><pubDate>Tue, 27 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2601.17331v1</guid></item><item><title>[arXiv-CV] UCAD: Uncertainty-guided Contour-aware Displacement for semi-supervised medical image segmentation</title><link>https://arxiv.org/abs/2601.17366</link><description>arXiv:2601.17366v1 Announce Type: new 
Abstract: Existing displacement strategies in semi-supervised segmentation only operate on rectangular regions, ignoring anatomical structures and resulting in boundary distortions and semantic inconsistency. To address these issues, we propose UCAD, an Uncertainty-Guided Contour-Aware Displacement framework for semi-supervised medical image segmentation that preserves contour-aware semantics while enhancing consistency learning. Our UCAD leverages superpixels to generate anatomically coherent regions aligned with anatomy boundaries, and an uncertainty-guided selection mechanism to selectively displace challenging regions for better consistency learning. We further propose a dynamic uncertainty-weighted consistency loss, which adaptively stabilizes training and effectively regularizes the model on unlabeled regions. Extensive experiments demonstrate that UCAD consistently outperforms state-of-the-art semi-supervised segmentation methods, achieving superior segmentation accuracy under limited annotation. The code is available at:https://github.com/dcb937/UCAD.</description><author>cs.CV updates on arXiv.org</author><pubDate>Tue, 27 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2601.17366v1</guid></item><item><title>[arXiv-CV] BMDS-Net: A Bayesian Multi-Modal Deep Supervision Network for Robust Brain Tumor Segmentation</title><link>https://arxiv.org/abs/2601.17504</link><description>arXiv:2601.17504v1 Announce Type: new 
Abstract: Accurate brain tumor segmentation from multi-modal magnetic resonance imaging (MRI) is a prerequisite for precise radiotherapy planning and surgical navigation. While recent Transformer-based models such as Swin UNETR have achieved impressive benchmark performance, their clinical utility is often compromised by two critical issues: sensitivity to missing modalities (common in clinical practice) and a lack of confidence calibration. Merely chasing higher Dice scores on idealized data fails to meet the safety requirements of real-world medical deployment. In this work, we propose BMDS-Net, a unified framework that prioritizes clinical robustness and trustworthiness over simple metric maximization. Our contribution is three-fold. First, we construct a robust deterministic backbone by integrating a Zero-Init Multimodal Contextual Fusion (MMCF) module and a Residual-Gated Deep Decoder Supervision (DDS) mechanism, enabling stable feature learning and precise boundary delineation with significantly reduced Hausdorff Distance, even under modality corruption. Second, and most importantly, we introduce a memory-efficient Bayesian fine-tuning strategy that transforms the network into a probabilistic predictor, providing voxel-wise uncertainty maps to highlight potential errors for clinicians. Third, comprehensive experiments on the BraTS 2021 dataset demonstrate that BMDS-Net not only maintains competitive accuracy but, more importantly, exhibits superior stability in missing-modality scenarios where baseline models fail. The source code is publicly available at https://github.com/RyanZhou168/BMDS-Net.</description><author>cs.CV updates on arXiv.org</author><pubDate>Tue, 27 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2601.17504v1</guid></item><item><title>[arXiv-CV] From Specialist to Generalist: Unlocking SAM's Learning Potential on Unlabeled Medical Images</title><link>https://arxiv.org/abs/2601.17934</link><description>arXiv:2601.17934v1 Announce Type: new 
Abstract: Foundation models like the Segment Anything Model (SAM) show strong generalization, yet adapting them to medical images remains difficult due to domain shift, scarce labels, and the inability of Parameter-Efficient Fine-Tuning (PEFT) to exploit unlabeled data. While conventional models like U-Net excel in semi-supervised medical learning, their potential to assist a PEFT SAM has been largely overlooked. We introduce SC-SAM, a specialist-generalist framework where U-Net provides point-based prompts and pseudo-labels to guide SAM's adaptation, while SAM serves as a powerful generalist supervisor to regularize U-Net. This reciprocal guidance forms a bidirectional co-training loop that allows both models to effectively exploit the unlabeled data. Across prostate MRI and polyp segmentation benchmarks, our method achieves state-of-the-art results, outperforming other existing semi-supervised SAM variants and even medical foundation models like MedSAM, highlighting the value of specialist-generalist cooperation for label-efficient medical image segmentation. Our code is available at https://github.com/vnlvi2k3/SC-SAM.</description><author>cs.CV updates on arXiv.org</author><pubDate>Tue, 27 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2601.17934v1</guid></item><item><title>[arXiv-CV] DTC: A Deformable Transposed Convolution Module for Medical Image Segmentation</title><link>https://arxiv.org/abs/2601.17939</link><description>arXiv:2601.17939v1 Announce Type: new 
Abstract: In medical image segmentation, particularly in UNet-like architectures, upsampling is primarily used to transform smaller feature maps into larger ones, enabling feature fusion between encoder and decoder features and supporting multi-scale prediction. Conventional upsampling methods, such as transposed convolution and linear interpolation, operate on fixed positions: transposed convolution applies kernel elements to predetermined pixel or voxel locations, while linear interpolation assigns values based on fixed coordinates in the original feature map. These fixed-position approaches may fail to capture structural information beyond predefined sampling positions and can lead to artifacts or loss of detail. Inspired by deformable convolutions, we propose a novel upsampling method, Deformable Transposed Convolution (DTC), which learns dynamic coordinates (i.e., sampling positions) to generate high-resolution feature maps for both 2D and 3D medical image segmentation tasks. Experiments on 3D (e.g., BTCV15) and 2D datasets (e.g., ISIC18, BUSI) demonstrate that DTC can be effectively integrated into existing medical image segmentation models, consistently improving the decoder's feature reconstruction and detail recovery capability.</description><author>cs.CV updates on arXiv.org</author><pubDate>Tue, 27 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2601.17939v1</guid></item><item><title>[arXiv-CV] Leveraging Persistence Image to Enhance Robustness and Performance in Curvilinear Structure Segmentation</title><link>https://arxiv.org/abs/2601.18045</link><description>arXiv:2601.18045v1 Announce Type: new 
Abstract: Segmenting curvilinear structures in medical images is essential for analyzing morphological patterns in clinical applications. Integrating topological properties, such as connectivity, improves segmentation accuracy and consistency. However, extracting and embedding such properties - especially from Persistence Diagrams (PD) - is challenging due to their non-differentiability and computational cost. Existing approaches mostly encode topology through handcrafted loss functions, which generalize poorly across tasks. In this paper, we propose PIs-Regressor, a simple yet effective module that learns persistence image (PI) - finite, differentiable representations of topological features - directly from data. Together with Topology SegNet, which fuses these features in both downsampling and upsampling stages, our framework integrates topology into the network architecture itself rather than auxiliary losses. Unlike existing methods that depend heavily on handcrafted loss functions, our approach directly incorporates topological information into the network structure, leading to more robust segmentation. Our design is flexible and can be seamlessly combined with other topology-based methods to further enhance segmentation performance. Experimental results show that integrating topological features enhances model robustness, effectively handling challenges like overexposure and blurring in medical imaging. Our approach on three curvilinear benchmarks demonstrate state-of-the-art performance in both pixel-level accuracy and topological fidelity.</description><author>cs.CV updates on arXiv.org</author><pubDate>Tue, 27 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2601.18045v1</guid></item><item><title>[arXiv-CV] Agentic Very Long Video Understanding</title><link>https://arxiv.org/abs/2601.18157</link><description>arXiv:2601.18157v1 Announce Type: new 
Abstract: The advent of always-on personal AI assistants, enabled by all-day wearable devices such as smart glasses, demands a new level of contextual understanding, one that goes beyond short, isolated events to encompass the continuous, longitudinal stream of egocentric video. Achieving this vision requires advances in long-horizon video understanding, where systems must interpret and recall visual and audio information spanning days or even weeks. Existing methods, including large language models and retrieval-augmented generation, are constrained by limited context windows and lack the ability to perform compositional, multi-hop reasoning over very long video streams. In this work, we address these challenges through EGAgent, an enhanced agentic framework centered on entity scene graphs, which represent people, places, objects, and their relationships over time. Our system equips a planning agent with tools for structured search and reasoning over these graphs, as well as hybrid visual and audio search capabilities, enabling detailed, cross-modal, and temporally coherent reasoning. Experiments on the EgoLifeQA and Video-MME (Long) datasets show that our method achieves state-of-the-art performance on EgoLifeQA (57.5%) and competitive performance on Video-MME (Long) (74.1%) for complex longitudinal video understanding tasks.</description><author>cs.CV updates on arXiv.org</author><pubDate>Tue, 27 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2601.18157v1</guid></item><item><title>[arXiv-CV] From Cold Start to Active Learning: Embedding-Based Scan Selection for Medical Image Segmentation</title><link>https://arxiv.org/abs/2601.18532</link><description>arXiv:2601.18532v1 Announce Type: new 
Abstract: Accurate segmentation annotations are critical for disease monitoring, yet manual labeling remains a major bottleneck due to the time and expertise required. Active learning (AL) alleviates this burden by prioritizing informative samples for annotation, typically through a diversity-based cold-start phase followed by uncertainty-driven selection. We propose a novel cold-start sampling strategy that combines foundation-model embeddings with clustering, including automatic selection of the number of clusters and proportional sampling across clusters, to construct a diverse and representative initial training. This is followed by an uncertainty-based AL framework that integrates spatial diversity to guide sample selection. The proposed method is intuitive and interpretable, enabling visualization of the feature-space distribution of candidate samples. We evaluate our approach on three datasets spanning X-ray and MRI modalities. On the CheXmask dataset, the cold-start strategy outperforms random selection, improving Dice from 0.918 to 0.929 and reducing the Hausdorff distance from 32.41 to 27.66 mm. In the AL setting, combined entropy and diversity selection improves Dice from 0.919 to 0.939 and reduces the Hausdorff distance from 30.10 to 19.16 mm. On the Montgomery dataset, cold-start gains are substantial, with Dice improving from 0.928 to 0.950 and Hausdorff distance decreasing from 14.22 to 9.38 mm. On the SynthStrip dataset, cold-start selection slightly affects Dice but reduces the Hausdorff distance from 9.43 to 8.69 mm, while active learning improves Dice from 0.816 to 0.826 and reduces the Hausdorff distance from 7.76 to 6.38 mm. Overall, the proposed framework consistently outperforms baseline methods in low-data regimes, improving segmentation accuracy.</description><author>cs.CV updates on arXiv.org</author><pubDate>Tue, 27 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2601.18532v1</guid></item><item><title>[arXiv-CV] A Multimodal Feature Distillation with Mamba-Transformer Network for Brain Tumor Segmentation with Incomplete Modalities</title><link>https://arxiv.org/abs/2404.14019</link><description>arXiv:2404.14019v2 Announce Type: replace 
Abstract: Existing brain tumor segmentation methods usually utilize multiple Magnetic Resonance Imaging (MRI) modalities in brain tumor images for segmentation, which can achieve better segmentation performance. However, in clinical applications, some modalities are often missing due to resource constraints, resulting in significant performance degradation for methods that rely on complete modality segmentation. In this paper, we propose a Multimodal feature distillation with Mamba-Transformer hybrid network (MMTSeg) for accurate brain tumor segmentation with missing modalities. We first employ a Multimodal Feature Distillation (MFD) module to distill feature-level multimodal knowledge into different unimodalities to extract complete modality information. We further develop an Unimodal Feature Enhancement (UFE) module to model the semantic relationship between global and local information. Finally, we built a Cross-Modal Fusion (CMF) module to explicitly align the global correlations across modalities, even when some modalities are missing. Complementary features within and across modalities are refined by the Mamba-Transformer hybrid architectures in both the UFE and CMF modules, dynamically capturing long-range dependencies and global semantic information for complex spatial contexts. A boundary-wise loss function is employed as the segmentation loss of the proposed MMTSeg to minimize boundary discrepancies for a distance-based metric. Our ablation study demonstrates the importance of the proposed feature enhancement and fusion modules in the proposed network and the Transformer with Mamba block for improving the performance of brain tumor segmentation with missing modalities. Extensive experiments on the BraTS 2018 and BraTS 2020 datasets demonstrate that the proposed MMTSeg framework outperforms state-of-the-art methods when modalities are missing.</description><author>cs.CV updates on arXiv.org</author><pubDate>Tue, 27 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2404.14019v2</guid></item><item><title>[arXiv-CV] When Swin Transformer Meets KANs: An Improved Transformer Architecture for Medical Image Segmentation</title><link>https://arxiv.org/abs/2511.04084</link><description>arXiv:2511.04084v2 Announce Type: replace 
Abstract: Medical image segmentation is critical for accurate diagnostics and treatment planning, but remains challenging due to complex anatomical structures and limited annotated training data. CNN-based segmentation methods excel at local feature extraction, but struggle with modeling long-range dependencies. Transformers, on the other hand, capture global context more effectively, but are inherently data-hungry and computationally expensive. In this work, we introduce UKAST, a U-Net like architecture that integrates rational-function based Kolmogorov-Arnold Networks (KANs) into Swin Transformer encoders. By leveraging rational base functions and Group Rational KANs (GR-KANs) from the Kolmogorov-Arnold Transformer (KAT), our architecture addresses the inefficiencies of vanilla spline-based KANs, yielding a more expressive and data-efficient framework with reduced FLOPs and only a very small increase in parameter count compared to SwinUNETR. UKAST achieves state-of-the-art performance on four diverse 2D and 3D medical image segmentation benchmarks, consistently surpassing both CNN- and Transformer-based baselines. Notably, it attains superior accuracy in data-scarce settings, alleviating the data-hungry limitations of standard Vision Transformers. These results show the potential of KAN-enhanced Transformers to advance data-efficient medical image segmentation. Code is available at: https://github.com/nsapkota417/UKAST</description><author>cs.CV updates on arXiv.org</author><pubDate>Tue, 27 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2511.04084v2</guid></item><item><title>[arXiv-CV] Not All Pixels Are Equal: Pixel-wise Meta-Learning for Medical Segmentation with Noisy Labels</title><link>https://arxiv.org/abs/2511.18894</link><description>arXiv:2511.18894v3 Announce Type: replace 
Abstract: Medical image segmentation is crucial for clinical applications, but it is frequently disrupted by noisy annotations and ambiguous anatomical boundaries, which lead to instability in model training. Existing methods typically rely on global noise assumptions or confidence-based sample selection, which inadequately mitigate the performance degradation caused by annotation noise, especially in challenging boundary regions. To address this issue, we propose MetaDCSeg, a robust framework that dynamically learns optimal pixel-wise weights to suppress the influence of noisy labels while preserving reliable annotations. By explicitly modeling boundary uncertainty through a Dynamic Center Distance (DCD) mechanism, our approach utilizes weighted feature distances for foreground, background, and boundary centers, directing the model's attention toward hard-to-segment pixels near ambiguous boundaries. This strategy enables more precise handling of structural boundaries, which are often overlooked by existing methods, and significantly enhances segmentation performance. Extensive experiments across four benchmark datasets with varying noise levels demonstrate that MetaDCSeg outperforms existing state-of-the-art methods.</description><author>cs.CV updates on arXiv.org</author><pubDate>Tue, 27 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2511.18894v3</guid></item><item><title>[IF] SU-RMT: Toward Bridging Semantic Representation and Structural Detail Modeling for Medical Image Segmentation</title><link>https://www.sciencedirect.com/science/article/pii/S1566253526000618?dgcid=rss_sd_all</link><description>&lt;p&gt;Publication date: Available online 26 January 2026&lt;/p&gt;&lt;p&gt;&lt;b&gt;Source:&lt;/b&gt; Information Fusion&lt;/p&gt;&lt;p&gt;Author(s): Peibo Song, Zihao Wang, Jinshuo Zhang, Shujun Fu, Yunfeng Zhang, Wei Wu, Fangxun Bao&lt;/p&gt;</description><author>ScienceDirect Publication: Information Fusion</author><pubDate>Mon, 26 Jan 2026 18:42:52 GMT</pubDate><guid isPermaLink="true">https://www.sciencedirect.com/science/article/pii/S1566253526000618</guid></item><item><title>[MedIA] Towards Boundary Confusion for Volumetric Medical Image Segmentation</title><link>https://www.sciencedirect.com/science/article/pii/S1361841526000307?dgcid=rss_sd_all</link><description>&lt;p&gt;Publication date: Available online 25 January 2026&lt;/p&gt;&lt;p&gt;&lt;b&gt;Source:&lt;/b&gt; Medical Image Analysis&lt;/p&gt;&lt;p&gt;Author(s): Xin You, Ming Ding, Minghui Zhang, Hanxiao Zhang, Junyang Wu, Yi Yu, Jie Yang, Yun Gu&lt;/p&gt;</description><author>ScienceDirect Publication: Medical Image Analysis</author><pubDate>Mon, 26 Jan 2026 18:42:50 GMT</pubDate><guid isPermaLink="true">https://www.sciencedirect.com/science/article/pii/S1361841526000307</guid></item><item><title>[arXiv-CV] FeTTL: Federated Template and Task Learning for Multi-Institutional Medical Imaging</title><link>https://arxiv.org/abs/2601.16302</link><description>arXiv:2601.16302v1 Announce Type: new 
Abstract: Federated learning enables collaborative model training across geographically distributed medical centers while preserving data privacy. However, domain shifts and heterogeneity in data often lead to a degradation in model performance. Medical imaging applications are particularly affected by variations in acquisition protocols, scanner types, and patient populations. To address these issues, we introduce Federated Template and Task Learning (FeTTL), a novel framework designed to harmonize multi-institutional medical imaging data in federated environments. FeTTL learns a global template together with a task model to align data distributions among clients. We evaluated FeTTL on two challenging and diverse multi-institutional medical imaging tasks: retinal fundus optical disc segmentation and histopathological metastasis classification. Experimental results show that FeTTL significantly outperforms the state-of-the-art federated learning baselines (p-values &lt;0.002) for optical disc segmentation and classification of metastases from multi-institutional data. Our experiments further highlight the importance of jointly learning the template and the task. These findings suggest that FeTTL offers a principled and extensible solution for mitigating distribution shifts in federated learning, supporting robust model deployment in real-world, multi-institutional environments.</description><author>cs.CV updates on arXiv.org</author><pubDate>Mon, 26 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2601.16302v1</guid></item><item><title>[arXiv-CV] Reliable Brain Tumor Segmentation Based on Spiking Neural Networks with Efficient Training</title><link>https://arxiv.org/abs/2601.16652</link><description>arXiv:2601.16652v1 Announce Type: new 
Abstract: We propose a reliable and energy-efficient framework for 3D brain tumor segmentation using spiking neural networks (SNNs). A multi-view ensemble of sagittal, coronal, and axial SNN models provides voxel-wise uncertainty estimation and enhances segmentation robustness. To address the high computational cost in training SNN models for semantic image segmentation, we employ Forward Propagation Through Time (FPTT), which maintains temporal learning efficiency with significantly reduced computational cost. Experiments on the Multimodal Brain Tumor Segmentation Challenges (BraTS 2017 and BraTS 2023) demonstrate competitive accuracy, well-calibrated uncertainty, and an 87% reduction in FLOPs, underscoring the potential of SNNs for reliable, low-power medical IoT and Point-of-Care systems.</description><author>cs.CV updates on arXiv.org</author><pubDate>Mon, 26 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2601.16652v1</guid></item><item><title>[arXiv-CV] Domain-invariant Mixed-domain Semi-supervised Medical Image Segmentation with Clustered Maximum Mean Discrepancy Alignment</title><link>https://arxiv.org/abs/2601.16954</link><description>arXiv:2601.16954v1 Announce Type: new 
Abstract: Deep learning has shown remarkable progress in medical image semantic segmentation, yet its success heavily depends on large-scale expert annotations and consistent data distributions. In practice, annotations are scarce, and images are collected from multiple scanners or centers, leading to mixed-domain settings with unknown domain labels and severe domain gaps. Existing semi-supervised or domain adaptation approaches typically assume either a single domain shift or access to explicit domain indices, which rarely hold in real-world deployment. In this paper, we propose a domain-invariant mixed-domain semi-supervised segmentation framework that jointly enhances data diversity and mitigates domain bias. A Copy-Paste Mechanism (CPM) augments the training set by transferring informative regions across domains, while a Cluster Maximum Mean Discrepancy (CMMD) block clusters unlabeled features and aligns them with labeled anchors via an MMD objective, encouraging domain-invariant representations. Integrated within a teacher-student framework, our method achieves robust and precise segmentation even with very few labeled examples and multiple unknown domain discrepancies. Experiments on Fundus and M&amp;amp;Ms benchmarks demonstrate that our approach consistently surpasses semi-supervised and domain adaptation methods, establishing a potential solution for mixed-domain semi-supervised medical image segmentation.</description><author>cs.CV updates on arXiv.org</author><pubDate>Mon, 26 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2601.16954v1</guid></item><item><title>[arXiv-CV] On The Robustness of Foundational 3D Medical Image Segmentation Models Against Imprecise Visual Prompts</title><link>https://arxiv.org/abs/2601.16383</link><description>arXiv:2601.16383v1 Announce Type: cross 
Abstract: While 3D foundational models have shown promise for promptable segmentation of medical volumes, their robustness to imprecise prompts remains under-explored. In this work, we aim to address this gap by systematically studying the effect of various controlled perturbations of dense visual prompts, that closely mimic real-world imprecision. By conducting experiments with two recent foundational models on a multi-organ abdominal segmentation task, we reveal several facets of promptable medical segmentation, especially pertaining to reliance on visual shape and spatial cues, and the extent of resilience of models towards certain perturbations. Codes are available at: https://github.com/ucsdbiag/Prompt-Robustness-MedSegFMs</description><author>cs.CV updates on arXiv.org</author><pubDate>Mon, 26 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2601.16383v1</guid></item><item><title>[arXiv-CV] NFL-BA: Near-Field Light Bundle Adjustment for SLAM in Dynamic Lighting</title><link>https://arxiv.org/abs/2412.13176</link><description>arXiv:2412.13176v4 Announce Type: replace 
Abstract: Simultaneous Localization and Mapping (SLAM) systems typically assume static, distant illumination; however, many real-world scenarios, such as endoscopy, subterranean robotics, and search &amp; rescue in collapsed environments, require agents to operate with a co-located light and camera in the absence of external lighting. In such cases, dynamic near-field lighting introduces strong, view-dependent shading that significantly degrades SLAM performance. We introduce Near-Field Lighting Bundle Adjustment Loss (NFL-BA) which explicitly models near-field lighting as a part of Bundle Adjustment loss and enables better performance for scenes captured with dynamic lighting. NFL-BA can be integrated into neural rendering-based SLAM systems with implicit or explicit scene representations. Our evaluations mainly focus on endoscopy procedure where SLAM can enable autonomous navigation, guidance to unsurveyed regions, blindspot detections, and 3D visualizations, which can significantly improve patient outcomes and endoscopy experience for both physicians and patients. Replacing Photometric Bundle Adjustment loss of SLAM systems with NFL-BA leads to significant improvement in camera tracking, 37% for MonoGS and 14% for EndoGS, and leads to state-of-the-art camera tracking and mapping performance on the C3VD colonoscopy dataset. Further evaluation on indoor scenes captured with phone camera with flashlight turned on, also demonstrate significant improvement in SLAM performance due to NFL-BA. See results at https://asdunnbe.github.io/NFL-BA/</description><author>cs.CV updates on arXiv.org</author><pubDate>Mon, 26 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2412.13176v4</guid></item><item><title>[arXiv-CV] Efficient Multi-scale Masked Autoencoders with Hybrid-Attention Mechanism for Breast Lesion Classification</title><link>https://arxiv.org/abs/2503.07157</link><description>arXiv:2503.07157v4 Announce Type: replace 
Abstract: Self-supervised learning (SSL) with Vision Transformers (ViT) has shown immense potential in medical image analysis. However, the quadratic complexity ($\mathcal{O}(N^2)$) of standard self-attention poses a severe barrier for high-resolution biomedical tasks, effectively excluding resource-constrained research labs from utilizing state-of-the-art models. To address this computational bottleneck without sacrificing diagnostic accuracy, we propose \textbf{MIRAM}, a Multi-scale Masked Autoencoder that leverages a \textbf{hybrid-attention mechanism}.
  Our architecture uniquely decouples semantic learning from detail reconstruction using a dual-decoder design: a standard transformer decoder captures global semantics at low resolution, while a linear-complexity decoder (comparing Linformer, Performer, and Nystr\"omformer) handles the computationally expensive high-resolution reconstruction. This reduces the complexity of the upscaling stage from quadratic to linear ($\mathcal{O}(N)$), enabling high-fidelity training on consumer-grade GPUs. We validate our approach on the CBIS-DDSM mammography dataset. Remarkably, our \textbf{Nystr\"omformer-based variant} achieves a classification accuracy of \textbf{61.0\%}, outperforming both standard MAE (58.9\%) and MoCo-v3 (60.2\%) while requiring significantly less memory. These results demonstrate that hybrid-attention architectures can democratize high-resolution medical AI, making powerful SSL accessible to researchers with limited hardware resources.</description><author>cs.CV updates on arXiv.org</author><pubDate>Mon, 26 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2503.07157v4</guid></item><item><title>[arXiv-CV] Hierarchy-Aware Multimodal Unlearning for Medical AI</title><link>https://arxiv.org/abs/2512.09867</link><description>arXiv:2512.09867v3 Announce Type: replace 
Abstract: Pretrained Multimodal Large Language Models (MLLMs) are increasingly used in sensitive domains such as medical AI, where privacy regulations like HIPAA and GDPR require specific removal of individuals' or institutions' data. This motivates machine unlearning, which aims to remove the influence of target data from a trained model. However, existing unlearning benchmarks fail to reflect the hierarchical and multimodal structure of real-world medical data, limiting their ability to properly evaluate unlearning in practice. Therefore, we introduce MedForget, a hierarchy-aware multimodal unlearning benchmark that models hospital data as a nested structure, enabling fine-grained evaluation of multimodal unlearning across retain and forget splits. Experiments with current unlearning methods show that existing approaches struggle to achieve effective hierarchy-aware forgetting without degrading downstream medical utility. To address this limitation, we propose Cross-modal Hierarchy-Informed Projection for unlearning (CHIP), a training-free, hierarchy-aware multimodal unlearning method that deletes information by selectively removing target-specific weight subspaces while preserving sibling-shared information. Experiments show that CHIP achieves the highest forget-retain performance gap across all hierarchy levels while maintaining competitive downstream utility compared to existing methods. Overall, MedForget provides a practical, HIPAA-aligned benchmark for evaluating structured multimodal unlearning for medical data, and CHIP offers an effective and general solution for hierarchy-aware forgetting that balances deletion with utility.</description><author>cs.CV updates on arXiv.org</author><pubDate>Mon, 26 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2512.09867v3</guid></item><item><title>[arXiv-CV] Scribble-Supervised Medical Image Segmentation with Dynamic Teacher Switching and Hierarchical Consistency</title><link>https://arxiv.org/abs/2601.14563</link><description>arXiv:2601.14563v3 Announce Type: replace 
Abstract: Scribble-supervised methods have emerged to mitigate the prohibitive annotation burden in medical image segmentation. However, the inherent sparsity of these annotations introduces significant ambiguity, which results in noisy pseudo-label propagation and hinders the learning of robust anatomical boundaries. To address this challenge, we propose SDT-Net, a novel dual-teacher, single-student framework designed to maximize supervision quality from these weak signals. Our method features a Dynamic Teacher Switching (DTS) module to adaptively select the most reliable teacher. This selected teacher then guides the student via two synergistic mechanisms: high-confidence pseudo-labels, refined by a Pick Reliable Pixels (PRP) mechanism, and multi-level feature alignment, enforced by a Hierarchical Consistency (HiCo) module. Extensive experiments on the ACDC and MSCMRseg datasets demonstrate that SDT-Net achieves state-of-the-art performance, producing more accurate and anatomically plausible segmentation.</description><author>cs.CV updates on arXiv.org</author><pubDate>Mon, 26 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2601.14563v3</guid></item><item><title>[arXiv-IV] Fine-tuned Transformer Models for Breast Cancer Detection and Classification</title><link>https://arxiv.org/abs/2512.02091</link><description>arXiv:2512.02091v2 Announce Type: replace 
Abstract: Breast cancer is still the second top cause of cancer deaths worldwide and this emphasizes the importance of necessary steps for early detection. Traditional diagnostic methods, such as mammography, ultrasound, and thermography, which have limitations when it comes to catching subtle patterns and reducing false positives. New technologies like artificial intelligence (AI) and deep learning have brought about the revolution in medical imaging analysis. Nevertheless, typical architectures such as Convolutional Neural Networks (CNNs) often have problems with modeling long-range dependencies. It explores the application of visual transformer models (here: Swin Tiny, DeiT, BEiT, ViT, and YOLOv8) for breast cancer detection through a collection of mammographic image sets. The ViT model reached the highest accuracy of 99.32% which showed its superiority in detecting global patterns as well as subtle image features. Data augmenting approaches, such as resizing croppings, flippings, and normalization, were further applied to the model for achieving higher performance. Although there were interesting results, the issues of dataset diversity and model optimization which present new avenues of research are also still present. Through this study, the crystal potential of transformer-based AI models in changing the detecting process of breast cancer and, thus, to patients health, is suggested.</description><author>eess.IV updates on arXiv.org</author><pubDate>Mon, 26 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2512.02091v2</guid></item><item><title>[arXiv-ML] LUMOS: Large User MOdels for User Behavior Prediction</title><link>https://arxiv.org/abs/2512.08957</link><description>arXiv:2512.08957v2 Announce Type: replace 
Abstract: User behavior prediction at scale remains a critical challenge for online B2C platforms. Traditional approaches rely heavily on task-specific models and domain-specific feature engineering. This is time-consuming, computationally expensive, and requires domain expertise and therefore, not scalable. We present LUMOS (Large User MOdel Series), a transformer-based architecture that eliminates task-specific models and manual feature engineering by learning multiple tasks jointly using only raw user activity data. LUMOS introduces a novel cross-attention mechanism that conditions predictions on future known events (e.g., holidays, sales, etc.), enabling the model to predict complex behavior patterns like "how will upcoming holidays affect user engagement?" The architecture also employs multi-modal tokenization, combining user activities, event context, and static user demographic attributes into rich representations processed through specialized embedding pathways.
  Through extensive experiments on a production dataset spanning 1.7 trillion user activity tokens from 250 million users, we demonstrate that LUMOS achieves superior performance compared to traditional task-specific models. Across 5 tasks with established baselines, we achieve an average improvement of 0.025 in ROC-AUC for binary classification tasks and 4.6\% reduction in MAPE for regression tasks. Online A/B testing validates these improvements translate to measurable business impact with a 3.15\% increase in Daily Active Users.</description><author>cs.LG updates on arXiv.org</author><pubDate>Mon, 26 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2512.08957v2</guid></item><item><title>[KBS] Mutual Masked Image Consistency and Feature Adversarial Training for Semi-Supervised Medical Image Segmentation</title><link>https://www.sciencedirect.com/science/article/pii/S0950705126000924?dgcid=rss_sd_all</link><description>&lt;p&gt;Publication date: Available online 23 January 2026&lt;/p&gt;&lt;p&gt;&lt;b&gt;Source:&lt;/b&gt; Knowledge-Based Systems&lt;/p&gt;&lt;p&gt;Author(s): Wei Li, Linye Ma, Wenyi Zhao, Huihua Yang&lt;/p&gt;</description><author>ScienceDirect Publication: Knowledge-Based Systems</author><pubDate>Sun, 25 Jan 2026 02:22:35 GMT</pubDate><guid isPermaLink="true">https://www.sciencedirect.com/science/article/pii/S0950705126000924</guid></item><item><title>[BSPC] Heterogeneous multi-network cross pseudo-supervised medical image segmentation</title><link>https://www.sciencedirect.com/science/article/pii/S1746809426001953?dgcid=rss_sd_all</link><description>&lt;p&gt;Publication date: 15 May 2026&lt;/p&gt;&lt;p&gt;&lt;b&gt;Source:&lt;/b&gt; Biomedical Signal Processing and Control, Volume 117&lt;/p&gt;&lt;p&gt;Author(s): Li Kang, Chuanghong Zhao, Jianjun Huang, Zhixin Gong&lt;/p&gt;</description><author>ScienceDirect Publication: Biomedical Signal Processing and Control</author><pubDate>Sat, 24 Jan 2026 18:36:20 GMT</pubDate><guid isPermaLink="true">https://www.sciencedirect.com/science/article/pii/S1746809426001953</guid></item><item><title>[BSPC] Multi-Source Unsupervised Domain Adaptation with dual alignment for medical image segmentation</title><link>https://www.sciencedirect.com/science/article/pii/S1746809426002326?dgcid=rss_sd_all</link><description>&lt;p&gt;Publication date: 15 May 2026&lt;/p&gt;&lt;p&gt;&lt;b&gt;Source:&lt;/b&gt; Biomedical Signal Processing and Control, Volume 117&lt;/p&gt;&lt;p&gt;Author(s): Chunping Gao, Lihua Guo, Qi Wu&lt;/p&gt;</description><author>ScienceDirect Publication: Biomedical Signal Processing and Control</author><pubDate>Fri, 23 Jan 2026 18:40:53 GMT</pubDate><guid isPermaLink="true">https://www.sciencedirect.com/science/article/pii/S1746809426002326</guid></item><item><title>[KBS] TransXV2S-NET: A Novel Hybrid Deep Learning Architecture with Dual-Contextual Graph Attention for Multi-Class Skin Lesion Classification</title><link>https://www.sciencedirect.com/science/article/pii/S0950705126001504?dgcid=rss_sd_all</link><description>&lt;p&gt;Publication date: Available online 22 January 2026&lt;/p&gt;&lt;p&gt;&lt;b&gt;Source:&lt;/b&gt; Knowledge-Based Systems&lt;/p&gt;&lt;p&gt;Author(s): Adnan Saeed, Khurram Shehzad, Muhammad Ghulam Abbas Malik, Saim Ahmed, Ahmad Taher Azar&lt;/p&gt;</description><author>ScienceDirect Publication: Knowledge-Based Systems</author><pubDate>Fri, 23 Jan 2026 18:40:51 GMT</pubDate><guid isPermaLink="true">https://www.sciencedirect.com/science/article/pii/S0950705126001504</guid></item><item><title>[MedIA] Channel-Wise Joint Disentanglement Representation Learning for B-Mode and Super-Resolution Ultrasound Based CAD of Breast Cancer</title><link>https://www.sciencedirect.com/science/article/pii/S1361841526000265?dgcid=rss_sd_all</link><description>&lt;p&gt;Publication date: Available online 22 January 2026&lt;/p&gt;&lt;p&gt;&lt;b&gt;Source:&lt;/b&gt; Medical Image Analysis&lt;/p&gt;&lt;p&gt;Author(s): Yuhang Zheng, Jiale Xu, Qing Hua, Xiaohong Jia, Xueqin Hou, Yanfeng Yao, Zheng Wei, Yulu Zhang, Fanggang Wu, Wei Guo, Yuan Tian, Jun Wang, Shujun Xia, Yijie Dong, Jun Shi, Jianqiao Zhou&lt;/p&gt;</description><author>ScienceDirect Publication: Medical Image Analysis</author><pubDate>Fri, 23 Jan 2026 18:40:50 GMT</pubDate><guid isPermaLink="true">https://www.sciencedirect.com/science/article/pii/S1361841526000265</guid></item><item><title>[MedIA] Anatomy-guided Prompting with Cross-Modal Self-Alignment for Whole-body PET-CT Breast Cancer Segmentation</title><link>https://www.sciencedirect.com/science/article/pii/S1361841526000253?dgcid=rss_sd_all</link><description>&lt;p&gt;Publication date: Available online 22 January 2026&lt;/p&gt;&lt;p&gt;&lt;b&gt;Source:&lt;/b&gt; Medical Image Analysis&lt;/p&gt;&lt;p&gt;Author(s): Jiaju Huang, Xiao Yang, Xinglong Liang, Shaobin Chen, Yue Sun, Greta Sp Mok, Shuo Li, Ying Wang, Tao Tan&lt;/p&gt;</description><author>ScienceDirect Publication: Medical Image Analysis</author><pubDate>Fri, 23 Jan 2026 18:40:50 GMT</pubDate><guid isPermaLink="true">https://www.sciencedirect.com/science/article/pii/S1361841526000253</guid></item><item><title>[arXiv-CV] Sub-Region-Aware Modality Fusion and Adaptive Prompting for Multi-Modal Brain Tumor Segmentation</title><link>https://arxiv.org/abs/2601.15734</link><description>arXiv:2601.15734v1 Announce Type: new 
Abstract: The successful adaptation of foundation models to multi-modal medical imaging is a critical yet unresolved challenge. Existing models often struggle to effectively fuse information from multiple sources and adapt to the heterogeneous nature of pathological tissues. To address this, we introduce a novel framework for adapting foundation models to multi-modal medical imaging, featuring two key technical innovations: sub-region-aware modality attention and adaptive prompt engineering. The attention mechanism enables the model to learn the optimal combination of modalities for each tumor sub-region, while the adaptive prompting strategy leverages the inherent capabilities of foundation models to refine segmentation accuracy. We validate our framework on the BraTS 2020 brain tumor segmentation dataset, demonstrating that our approach significantly outperforms baseline methods, particularly in the challenging necrotic core sub-region. Our work provides a principled and effective approach to multi-modal fusion and prompting, paving the way for more accurate and robust foundation model-based solutions in medical imaging.</description><author>cs.CV updates on arXiv.org</author><pubDate>Fri, 23 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2601.15734v1</guid></item><item><title>[arXiv-CV] ProGiDiff: Prompt-Guided Diffusion-Based Medical Image Segmentation</title><link>https://arxiv.org/abs/2601.16060</link><description>arXiv:2601.16060v1 Announce Type: new 
Abstract: Widely adopted medical image segmentation methods, although efficient, are primarily deterministic and remain poorly amenable to natural language prompts. Thus, they lack the capability to estimate multiple proposals, human interaction, and cross-modality adaptation. Recently, text-to-image diffusion models have shown potential to bridge the gap. However, training them from scratch requires a large dataset-a limitation for medical image segmentation. Furthermore, they are often limited to binary segmentation and cannot be conditioned on a natural language prompt. To this end, we propose a novel framework called ProGiDiff that leverages existing image generation models for medical image segmentation purposes. Specifically, we propose a ControlNet-style conditioning mechanism with a custom encoder, suitable for image conditioning, to steer a pre-trained diffusion model to output segmentation masks. It naturally extends to a multi-class setting simply by prompting the target organ. Our experiment on organ segmentation from CT images demonstrates strong performance compared to previous methods and could greatly benefit from an expert-in-the-loop setting to leverage multiple proposals. Importantly, we demonstrate that the learned conditioning mechanism can be easily transferred through low-rank, few-shot adaptation to segment MR images.</description><author>cs.CV updates on arXiv.org</author><pubDate>Fri, 23 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2601.16060v1</guid></item><item><title>[arXiv-CV] DSFedMed: Dual-Scale Federated Medical Image Segmentation via Mutual Distillation Between Foundation and Lightweight Models</title><link>https://arxiv.org/abs/2601.16073</link><description>arXiv:2601.16073v1 Announce Type: new 
Abstract: Foundation Models (FMs) have demonstrated strong generalization across diverse vision tasks. However, their deployment in federated settings is hindered by high computational demands, substantial communication overhead, and significant inference costs. We propose DSFedMed, a dual-scale federated framework that enables mutual knowledge distillation between a centralized foundation model and lightweight client models for medical image segmentation. To support knowledge distillation, a set of high-quality medical images is generated to replace real public datasets, and a learnability-guided sample selection strategy is proposed to enhance efficiency and effectiveness in dual-scale distillation. This mutual distillation enables the foundation model to transfer general knowledge to lightweight clients, while also incorporating client-specific insights to refine the foundation model. Evaluations on five medical imaging segmentation datasets show that DSFedMed achieves an average 2 percent improvement in Dice score while reducing communication costs and inference time by nearly 90 percent compared to existing federated foundation model baselines. These results demonstrate significant efficiency gains and scalability for resource-limited federated deployments.</description><author>cs.CV updates on arXiv.org</author><pubDate>Fri, 23 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2601.16073v1</guid></item><item><title>[arXiv-CV] A Machine Vision Approach to Preliminary Skin Lesion Assessments</title><link>https://arxiv.org/abs/2601.15539</link><description>arXiv:2601.15539v1 Announce Type: cross 
Abstract: Early detection of malignant skin lesions is critical for improving patient outcomes in aggressive, metastatic skin cancers. This study evaluates a comprehensive system for preliminary skin lesion assessment that combines the clinically established ABCD rule of dermoscopy (analyzing Asymmetry, Borders, Color, and Dermoscopic Structures) with machine learning classification. Using a 1,000-image subset of the HAM10000 dataset, the system implements an automated, rule-based pipeline to compute a Total Dermoscopy Score (TDS) for each lesion. This handcrafted approach is compared against various machine learning solutions, including traditional classifiers (Logistic Regression, Random Forest, and SVM) and deep learning models. While the rule-based system provides high clinical interpretability, results indicate a performance bottleneck when reducing complex morphology to five numerical features. Experimental findings show that transfer learning with EfficientNet-B0 failed significantly due to domain shift between natural and medical images. In contrast, a custom three-layer Convolutional Neural Network (CNN) trained from scratch achieved 78.5% accuracy and 86.5% recall on median-filtered images, representing a 19-point accuracy improvement over traditional methods. The results demonstrate that direct pixel-level learning captures diagnostic patterns beyond handcrafted features and that purpose-built lightweight architectures can outperform large pretrained models for small, domain-specific medical datasets.</description><author>cs.CV updates on arXiv.org</author><pubDate>Fri, 23 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2601.15539v1</guid></item><item><title>[arXiv-CV] Phi-SegNet: Phase-Integrated Supervision for Medical Image Segmentation</title><link>https://arxiv.org/abs/2601.16064</link><description>arXiv:2601.16064v1 Announce Type: cross 
Abstract: Deep learning has substantially advanced medical image segmentation, yet achieving robust generalization across diverse imaging modalities and anatomical structures remains a major challenge. A key contributor to this limitation lies in how existing architectures, ranging from CNNs to Transformers and their hybrids, primarily encode spatial information while overlooking frequency-domain representations that capture rich structural and textural cues. Although few recent studies have begun exploring spectral information at the feature level, supervision-level integration of frequency cues-crucial for fine-grained object localization-remains largely untapped. To this end, we propose Phi-SegNet, a CNN-based architecture that incorporates phase-aware information at both architectural and optimization levels. The network integrates Bi-Feature Mask Former (BFMF) modules that blend neighboring encoder features to reduce semantic gaps, and Reverse Fourier Attention (RFA) blocks that refine decoder outputs using phase-regularized features. A dedicated phase-aware loss aligns these features with structural priors, forming a closed feedback loop that emphasizes boundary precision. Evaluated on five public datasets spanning X-ray, US, histopathology, MRI, and colonoscopy, Phi-SegNet consistently achieved state-of-the-art performance, with an average relative improvement of 1.54+/-1.26% in IoU and 0.98+/-0.71% in F1-score over the next best-performing model. In cross-dataset generalization scenarios involving unseen datasets from the known domain, Phi-SegNet also exhibits robust and superior performance, highlighting its adaptability and modality-agnostic design. These findings demonstrate the potential of leveraging spectral priors in both feature representation and supervision, paving the way for generalized segmentation frameworks that excel in fine-grained object localization.</description><author>cs.CV updates on arXiv.org</author><pubDate>Fri, 23 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2601.16064v1</guid></item><item><title>[arXiv-CV] Language-guided Medical Image Segmentation with Target-informed Multi-level Contrastive Alignments</title><link>https://arxiv.org/abs/2412.13533</link><description>arXiv:2412.13533v3 Announce Type: replace 
Abstract: Medical image segmentation is a fundamental task in numerous medical engineering applications. Recently, language-guided segmentation has shown promise in medical scenarios where textual clinical reports are readily available as semantic guidance. Clinical reports contain diagnostic information provided by clinicians, which can provide auxiliary textual semantics to guide segmentation. However, existing language-guided segmentation methods neglect the inherent pattern gaps between image and text modalities, resulting in sub-optimal visual-language integration. Contrastive learning is a well-recognized approach to align image-text patterns, but it has not been optimized for bridging the pattern gaps in medical language-guided segmentation that relies primarily on medical image details to characterize the underlying disease/targets. Current contrastive alignment techniques typically align high-level global semantics without involving low-level localized target information, and thus cannot deliver fine-grained textual guidance on crucial image details. In this study, we propose a Target-informed Multi-level Contrastive Alignment framework (TMCA) to bridge image-text pattern gaps for medical language-guided segmentation. TMCA enables target-informed image-text alignments and fine-grained textual guidance by introducing: (i) a target-sensitive semantic distance module that utilizes target information for more granular image-text alignment modeling, (ii) a multi-level contrastive alignment strategy that directs fine-grained textual guidance to multi-scale image details, and (iii) a language-guided target enhancement module that reinforces attention to critical image regions based on the aligned image-text patterns. Extensive experiments on four public benchmark datasets demonstrate that TMCA enabled superior performance over state-of-the-art language-guided medical image segmentation methods.</description><author>cs.CV updates on arXiv.org</author><pubDate>Fri, 23 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2412.13533v3</guid></item><item><title>[arXiv-CV] Skin Lesion Phenotyping via Nested Multi-modal Contrastive Learning</title><link>https://arxiv.org/abs/2505.23709</link><description>arXiv:2505.23709v2 Announce Type: replace 
Abstract: We introduce SLIMP (Skin Lesion Image-Metadata Pre-training) for learning rich representations of skin lesions through a novel nested contrastive learning approach that captures complex relationships between images and metadata. Melanoma detection and skin lesion classification based solely on images, pose significant challenges due to large variations in imaging conditions (lighting, color, resolution, distance, etc.) and lack of clinical and phenotypical context. Clinicians typically follow a holistic approach for assessing the risk level of the patient and for deciding which lesions may be malignant and need to be excised, by considering the patient's medical history as well as the appearance of other lesions of the patient. Inspired by this, SLIMP combines the appearance and the metadata of individual skin lesions with patient-level metadata relating to their medical record and other clinically relevant information. By fully exploiting all available data modalities throughout the learning process, the proposed pre-training strategy improves performance compared to other pre-training strategies on downstream skin lesions classification tasks highlighting the learned representations quality.</description><author>cs.CV updates on arXiv.org</author><pubDate>Fri, 23 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2505.23709v2</guid></item><item><title>[arXiv-CV] MetaDCSeg: Robust Medical Image Segmentation via Meta Dynamic Center Weighting</title><link>https://arxiv.org/abs/2511.18894</link><description>arXiv:2511.18894v2 Announce Type: replace 
Abstract: Medical image segmentation is crucial for clinical applications, but it is frequently disrupted by noisy annotations and ambiguous anatomical boundaries, which lead to instability in model training. Existing methods typically rely on global noise assumptions or confidence-based sample selection, which inadequately mitigate the performance degradation caused by annotation noise, especially in challenging boundary regions. To address this issue, we propose MetaDCSeg, a robust framework that dynamically learns optimal pixel-wise weights to suppress the influence of noisy ground-truth labels while preserving reliable annotations. By explicitly modeling boundary uncertainty through a Dynamic Center Distance (DCD) mechanism, our approach utilizes weighted feature distances for foreground, background, and boundary centers, directing the model's attention toward hard-to-segment pixels near ambiguous boundaries. This strategy enables more precise handling of structural boundaries, which are often overlooked by existing methods, and significantly enhances segmentation performance. Extensive experiments across four benchmark datasets with varying noise levels demonstrate that MetaDCSeg consistently outperforms existing state-of-the-art methods.</description><author>cs.CV updates on arXiv.org</author><pubDate>Fri, 23 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2511.18894v2</guid></item><item><title>[arXiv-CV] GutenOCR: A Grounded Vision-Language Front-End for Documents</title><link>https://arxiv.org/abs/2601.14490</link><description>arXiv:2601.14490v2 Announce Type: replace 
Abstract: GutenOCR is a family of grounded OCR front-ends obtained by fine-tuning Qwen2.5-VL-3B and Qwen2.5-VL-7B. The resulting single-checkpoint vision-language models expose reading, detection, and grounding through a unified, prompt-based interface. Trained on business documents, scientific articles, and synthetic grounding data, the models support full-page and localized reading with line- and paragraph-level bounding boxes and conditional ``where is x?'' queries. We introduce a grounded OCR evaluation protocol and show that GutenOCR-7B more than doubles the composite grounded OCR score of its Qwen2.5-VL-7B backbone on 10.5K held-out business and scientific pages (0.40 to 0.82). On Fox and OmniDocBench v1.5, our approach substantially improves region- and line-level OCR as well as text-detection recall, but reveals trade-offs in page-level linearization, color-guided OCR, and formula-heavy layouts.</description><author>cs.CV updates on arXiv.org</author><pubDate>Fri, 23 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2601.14490v2</guid></item><item><title>[arXiv-CV] Scribble-Supervised Medical Image Segmentation with Dynamic Teacher Switching and Hierarchical Consistency</title><link>https://arxiv.org/abs/2601.14563</link><description>arXiv:2601.14563v2 Announce Type: replace 
Abstract: Scribble-supervised methods have emerged to mitigate the prohibitive annotation burden in medical image segmentation. However, the inherent sparsity of these annotations introduces significant ambiguity, which results in noisy pseudo-label propagation and hinders the learning of robust anatomical boundaries. To address this challenge, we propose SDT-Net, a novel dual-teacher, single-student framework designed to maximize supervision quality from these weak signals. Our method features a Dynamic Teacher Switching (DTS) module to adaptively select the most reliable teacher. This selected teacher then guides the student via two synergistic mechanisms: high-confidence pseudo-labels, refined by a Pick Reliable Pixels (PRP) mechanism, and multi-level feature alignment, enforced by a Hierarchical Consistency (HiCo) module. Extensive experiments on the ACDC and MSCMRseg datasets demonstrate that SDT-Net achieves state-of-the-art performance, producing more accurate and anatomically plausible segmentation.</description><author>cs.CV updates on arXiv.org</author><pubDate>Fri, 23 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2601.14563v2</guid></item><item><title>[arXiv-ML] BanditLP: Large-Scale Stochastic Optimization for Personalized Recommendations</title><link>https://arxiv.org/abs/2601.15552</link><description>arXiv:2601.15552v1 Announce Type: new 
Abstract: We present BanditLP, a scalable multi-stakeholder contextual bandit framework that unifies neural Thompson Sampling for learning objective-specific outcomes with a large-scale linear program for constrained action selection at serving time. The methodology is application-agnostic, compatible with arbitrary neural architectures, and deployable at web scale, with an LP solver capable of handling billions of variables. Experiments on public benchmarks and synthetic data show consistent gains over strong baselines. We apply this approach in LinkedIn's email marketing system and demonstrate business win, illustrating the value of integrated exploration and constrained optimization in production.</description><author>cs.LG updates on arXiv.org</author><pubDate>Fri, 23 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2601.15552v1</guid></item><item><title>[arXiv-ML] Rethinking Drug-Drug Interaction Modeling as Generalizable Relation Learning</title><link>https://arxiv.org/abs/2601.15771</link><description>arXiv:2601.15771v1 Announce Type: new 
Abstract: Drug-drug interaction (DDI) prediction is central to drug discovery and clinical development, particularly in the context of increasingly prevalent polypharmacy. Although existing computational methods achieve strong performance on standard benchmarks, they often fail to generalize to realistic deployment scenarios, where most candidate drug pairs involve previously unseen drugs and validated interactions are scarce. We demonstrate that proximity in the embedding spaces of prevailing molecule-centric DDI models does not reliably correspond to interaction labels, and that simply scaling up model capacity therefore fails to improve generalization. To address these limitations, we propose GenRel-DDI, a generalizable relation learning framework that reformulates DDI prediction as a relation-centric learning problem, in which interaction representations are learned independently of drug identities. This relation-level abstraction enables the capture of transferable interaction patterns that generalize to unseen drugs and novel drug pairs. Extensive experiments across multiple benchmark demonstrate that GenRel-DDI consistently and significantly outperforms state-of-the-art methods, with particularly large gains on strict entity-disjoint evaluations, highlighting the effectiveness and practical utility of relation learning for robust DDI prediction. The code is available at https://github.com/SZU-ADDG/GenRel-DDI.</description><author>cs.LG updates on arXiv.org</author><pubDate>Fri, 23 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2601.15771v1</guid></item><item><title>[arXiv-ML] Who Benefits From Sinus Surgery? Comparing Generative AI and Supervised Machine Learning for Predicting Surgical Outcomes in Chronic Rhinosinusitis</title><link>https://arxiv.org/abs/2601.13710</link><description>arXiv:2601.13710v2 Announce Type: replace 
Abstract: Artificial intelligence has reshaped medical imaging, yet the use of AI on clinical data for prospective decision support remains limited. We study pre-operative prediction of clinically meaningful improvement in chronic rhinosinusitis (CRS), defining success as a more than 8.9-point reduction in SNOT-22 at 6 months (MCID). In a prospectively collected cohort where all patients underwent surgery, we ask whether models using only pre-operative clinical data could have identified those who would have poor outcomes, i.e. those who should have avoided surgery. We benchmark supervised ML (logistic regression, tree ensembles, and an in-house MLP) against generative AI (ChatGPT, Claude, Gemini, Perplexity), giving each the same structured inputs and constraining outputs to binary recommendations with confidence. Our best ML model (MLP) achieves 85 % accuracy with superior calibration and decision-curve net benefit. GenAI models underperform on discrimination and calibration across zero-shot setting. Notably, GenAI justifications align with clinician heuristics and the MLP's feature importance, repeatedly highlighting baseline SNOT-22, CT/endoscopy severity, polyp phenotype, and physchology/pain comorbidities. We provide a reproducible tabular-to-GenAI evaluation protocol and subgroup analyses. Findings support an ML-first, GenAI- augmented workflow: deploy calibrated ML for primary triage of surgical candidacy, with GenAI as an explainer to enhance transparency and shared decision-making.</description><author>cs.LG updates on arXiv.org</author><pubDate>Fri, 23 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2601.13710v2</guid></item><item><title>[arXiv-CV] GutenOCR: A Grounded Vision-Language Front-End for Documents</title><link>https://arxiv.org/abs/2601.14490</link><description>arXiv:2601.14490v1 Announce Type: new 
Abstract: GutenOCR is a family of grounded OCR front-ends obtained by fine-tuning Qwen2.5-VL-3B and Qwen2.5-VL-7B. The resulting single-checkpoint vision-language models expose reading, detection, and grounding through a unified, prompt-based interface. Trained on business documents, scientific articles, and synthetic grounding data, the models support full-page and localized reading with line- and paragraph-level bounding boxes and conditional ``where is x?'' queries. We introduce a grounded OCR evaluation protocol and show that GutenOCR-7B more than doubles the composite grounded OCR score of its Qwen2.5-VL-7B backbone on 10.5K held-out business and scientific pages (0.40 to 0.82). On Fox and OmniDocBench v1.5, our approach substantially improves region- and line-level OCR as well as text-detection recall, but reveals trade-offs in page-level linearization, color-guided OCR, and formula-heavy layouts.</description><author>cs.CV updates on arXiv.org</author><pubDate>Thu, 22 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2601.14490v1</guid></item><item><title>[arXiv-CV] Scribble-Supervised Medical Image Segmentation with Dynamic Teacher Switching and Hierarchical Consistency</title><link>https://arxiv.org/abs/2601.14563</link><description>arXiv:2601.14563v1 Announce Type: new 
Abstract: Scribble-supervised methods have emerged to mitigate the prohibitive annotation burden in medical image segmentation. However, the inherent sparsity of these annotations introduces significant ambiguity, which results in noisy pseudo-label propagation and hinders the learning of robust anatomical boundaries. To address this challenge, we propose SDT-Net, a novel dual-teacher, single-student framework designed to maximize supervision quality from these weak signals. Our method features a Dynamic Teacher Switching (DTS) module to adaptively select the most reliable teacher. This selected teacher then guides the student via two synergistic mechanisms: high-confidence pseudo-labels, refined by a Pick Reliable Pixels (PRP) mechanism, and multi-level feature alignment, enforced by a Hierarchical Consistency (HiCo) module. Extensive experiments on the ACDC and MSCMRseg datasets demonstrate that SDT-Net achieves state-of-the-art performance, producing more accurate and anatomically plausible segmentation.</description><author>cs.CV updates on arXiv.org</author><pubDate>Thu, 22 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2601.14563v1</guid></item><item><title>[arXiv-CV] U-Harmony: Enhancing Joint Training for Segmentation Models with Universal Harmonization</title><link>https://arxiv.org/abs/2601.14605</link><description>arXiv:2601.14605v1 Announce Type: new 
Abstract: In clinical practice, medical segmentation datasets are often limited and heterogeneous, with variations in modalities, protocols, and anatomical targets across institutions. Existing deep learning models struggle to jointly learn from such diverse data, often sacrificing either generalization or domain-specific knowledge. To overcome these challenges, we propose a joint training method called Universal Harmonization (U-Harmony), which can be integrated into deep learning-based architectures with a domain-gated head, enabling a single segmentation model to learn from heterogeneous datasets simultaneously. By integrating U-Harmony, our approach sequentially normalizes and then denormalizes feature distributions to mitigate domain-specific variations while preserving original dataset-specific knowledge. More appealingly, our framework also supports universal modality adaptation, allowing the seamless learning of new imaging modalities and anatomical classes. Extensive experiments on cross-institutional brain lesion datasets demonstrate the effectiveness of our approach, establishing a new benchmark for robust and adaptable 3D medical image segmentation models in real-world clinical settings.</description><author>cs.CV updates on arXiv.org</author><pubDate>Thu, 22 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2601.14605v1</guid></item><item><title>[arXiv-CV] Using Multi-Instance Learning to Identify Unique Polyps in Colon Capsule Endoscopy Images</title><link>https://arxiv.org/abs/2601.14771</link><description>arXiv:2601.14771v1 Announce Type: new 
Abstract: Identifying unique polyps in colon capsule endoscopy (CCE) images is a critical yet challenging task for medical personnel due to the large volume of images, the cognitive load it creates for clinicians, and the ambiguity in labeling specific frames. This paper formulates this problem as a multi-instance learning (MIL) task, where a query polyp image is compared with a target bag of images to determine uniqueness. We employ a multi-instance verification (MIV) framework that incorporates attention mechanisms, such as variance-excited multi-head attention (VEMA) and distance-based attention (DBA), to enhance the model's ability to extract meaningful representations. Additionally, we investigate the impact of self-supervised learning using SimCLR to generate robust embeddings. Experimental results on a dataset of 1912 polyps from 754 patients demonstrate that attention mechanisms significantly improve performance, with DBA L1 achieving the highest test accuracy of 86.26\% and a test AUC of 0.928 using a ConvNeXt backbone with SimCLR pretraining. This study underscores the potential of MIL and self-supervised learning in advancing automated analysis of Colon Capsule Endoscopy images, with implications for broader medical imaging applications.</description><author>cs.CV updates on arXiv.org</author><pubDate>Thu, 22 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2601.14771v1</guid></item><item><title>[arXiv-CV] LocBAM: Advancing 3D Patch-Based Image Segmentation by Integrating Location Contex</title><link>https://arxiv.org/abs/2601.14802</link><description>arXiv:2601.14802v1 Announce Type: new 
Abstract: Patch-based methods are widely used in 3D medical image segmentation to address memory constraints in processing high-resolution volumetric data. However, these approaches often neglect the patch's location within the global volume, which can limit segmentation performance when anatomical context is important. In this paper, we investigate the role of location context in patch-based 3D segmentation and propose a novel attention mechanism, LocBAM, that explicitly processes spatial information. Experiments on BTCV, AMOS22, and KiTS23 demonstrate that incorporating location context stabilizes training and improves segmentation performance, particularly under low patch-to-volume coverage where global context is missing. Furthermore, LocBAM consistently outperforms classical coordinate encoding via CoordConv. Code is publicly available at https://github.com/compai-lab/2026-ISBI-hooft</description><author>cs.CV updates on arXiv.org</author><pubDate>Thu, 22 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2601.14802v1</guid></item><item><title>[arXiv-CV] Federated Transformer-GNN for Privacy-Preserving Brain Tumor Localization with Modality-Level Explainability</title><link>https://arxiv.org/abs/2601.15042</link><description>arXiv:2601.15042v1 Announce Type: new 
Abstract: Deep learning models for brain tumor analysis require large and diverse datasets that are often siloed across healthcare institutions due to privacy regulations. We present a federated learning framework for brain tumor localization that enables multi-institutional collaboration without sharing sensitive patient data. Our method extends a hybrid Transformer-Graph Neural Network architecture derived from prior decoder-free supervoxel GNNs and is deployed within CAFEIN\textsuperscript{\textregistered}, CERN's federated learning platform designed for healthcare environments. We provide an explainability analysis through Transformer attention mechanisms that reveals which MRI modalities drive the model predictions. Experiments on the BraTS dataset demonstrate a key finding: while isolated training on individual client data triggers early stopping well before reaching full training capacity, federated learning enables continued model improvement by leveraging distributed data, ultimately matching centralized performance. This result provides strong justification for federated learning when dealing with complex tasks and high-dimensional input data, as aggregating knowledge from multiple institutions significantly benefits the learning process. Our explainability analysis, validated through rigorous statistical testing on the full test set (paired t-tests with Bonferroni correction), reveals that deeper network layers significantly increase attention to T2 and FLAIR modalities ($p&lt;0.001$, Cohen's $d$=1.50), aligning with clinical practice.</description><author>cs.CV updates on arXiv.org</author><pubDate>Thu, 22 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2601.15042v1</guid></item><item><title>[arXiv-CV] BREPS: Bounding-Box Robustness Evaluation of Promptable Segmentation</title><link>https://arxiv.org/abs/2601.15123</link><description>arXiv:2601.15123v1 Announce Type: new 
Abstract: Promptable segmentation models such as SAM have established a powerful paradigm, enabling strong generalization to unseen objects and domains with minimal user input, including points, bounding boxes, and text prompts. Among these, bounding boxes stand out as particularly effective, often outperforming points while significantly reducing annotation costs. However, current training and evaluation protocols typically rely on synthetic prompts generated through simple heuristics, offering limited insight into real-world robustness. In this paper, we investigate the robustness of promptable segmentation models to natural variations in bounding box prompts. First, we conduct a controlled user study and collect thousands of real bounding box annotations. Our analysis reveals substantial variability in segmentation quality across users for the same model and instance, indicating that SAM-like models are highly sensitive to natural prompt noise. Then, since exhaustive testing of all possible user inputs is computationally prohibitive, we reformulate robustness evaluation as a white-box optimization problem over the bounding box prompt space. We introduce BREPS, a method for generating adversarial bounding boxes that minimize or maximize segmentation error while adhering to naturalness constraints. Finally, we benchmark state-of-the-art models across 10 datasets, spanning everyday scenes to medical imaging. Code - https://github.com/emb-ai/BREPS.</description><author>cs.CV updates on arXiv.org</author><pubDate>Thu, 22 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2601.15123v1</guid></item><item><title>[arXiv-CV] Partial Decoder Attention Network with Contour-weighted Loss Function for Data-Imbalance Medical Image Segmentation</title><link>https://arxiv.org/abs/2601.14338</link><description>arXiv:2601.14338v1 Announce Type: cross 
Abstract: Image segmentation is pivotal in medical image analysis, facilitating clinical diagnosis, treatment planning, and disease evaluation. Deep learning has significantly advanced automatic segmentation methodologies by providing superior modeling capability for complex structures and fine-grained anatomical regions. However, medical images often suffer from data imbalance issues, such as large volume disparities among organs or tissues, and uneven sample distributions across different anatomical structures. This imbalance tends to bias the model toward larger organs or more frequently represented structures, while overlooking smaller or less represented structures, thereby affecting the segmentation accuracy and robustness. To address these challenges, we proposed a novel contour-weighted segmentation approach, which improves the model's capability to represent small and underrepresented structures. We developed PDANet, a lightweight and efficient segmentation network based on a partial decoder mechanism. We evaluated our method using three prominent public datasets. The experimental results show that our methodology excelled in three distinct tasks: segmenting multiple abdominal organs, brain tumors, and pelvic bone fragments with injuries. It consistently outperformed nine state-of-the-art methods. Moreover, the proposed contour-weighted strategy improved segmentation for other comparison methods across the three datasets, yielding average enhancements in Dice scores of 2.32%, 1.67%, and 3.60%, respectively. These results demonstrate that our contour-weighted segmentation method surpassed current leading approaches in both accuracy and robustness. As a model-independent strategy, it can seamlessly fit various segmentation frameworks, enhancing their performance. This flexibility highlighted its practical importance and potential for broad use in medical image analysis.</description><author>cs.CV updates on arXiv.org</author><pubDate>Thu, 22 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2601.14338v1</guid></item><item><title>[arXiv-CV] Unlocking Generalization in Polyp Segmentation with DINO Self-Attention "keys"</title><link>https://arxiv.org/abs/2512.13376</link><description>arXiv:2512.13376v2 Announce Type: replace 
Abstract: Automatic polyp segmentation is crucial for improving the clinical identification of colorectal cancer (CRC). While Deep Learning (DL) techniques have been extensively researched for this problem, current methods frequently struggle with generalization, particularly in data-constrained or challenging settings. Moreover, many existing polyp segmentation methods rely on complex, task-specific architectures. To address these limitations, we present a framework that leverages the intrinsic robustness of DINO self-attention "key" features for robust segmentation. Unlike traditional methods that extract tokens from the deepest layers of the Vision Transformer (ViT), our approach leverages the key features of the self-attention module with a simple convolutional decoder to predict polyp masks, resulting in enhanced performance and better generalizability. We validate our approach using a multi-center dataset under two rigorous protocols: Domain Generalization (DG) and Extreme Single Domain Generalization (ESDG). Our results, supported by a comprehensive statistical analysis, demonstrate that this pipeline achieves state-of-the-art (SOTA) performance, significantly enhancing generalization, particularly in data-scarce and challenging scenarios. While avoiding a polyp-specific architecture, we surpass well-established models like nnU-Net and UM-Net. Additionally, we provide a systematic benchmark of the DINO framework's evolution, quantifying the specific impact of architectural advancements on downstream polyp segmentation performance.</description><author>cs.CV updates on arXiv.org</author><pubDate>Thu, 22 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2512.13376v2</guid></item><item><title>[arXiv-CV] Weakly-supervised segmentation using inherently-explainable classification models and their application to brain tumour classification</title><link>https://arxiv.org/abs/2206.05148</link><description>arXiv:2206.05148v3 Announce Type: replace-cross 
Abstract: Deep learning has demonstrated significant potential in medical imaging; however, the opacity of "black-box" models hinders clinical trust, while segmentation tasks typically necessitate labourious, hard-to-obtain pixel-wise annotations. To address these challenges simultaneously, this paper introduces a framework for three inherently explainable classifiers (GP-UNet, GP-ShuffleUNet, and GP-ReconResNet). By integrating a global pooling mechanism, these networks generate localisation heatmaps that directly influence classification decisions, offering inherent interpretability without relying on potentially unreliable post-hoc methods. These heatmaps are subsequently thresholded to achieve weakly-supervised segmentation, requiring only image-level classification labels for training. Validated on two datasets for multi-class brain tumour classification, the proposed models achieved a peak F1-score of 0.93. For the weakly-supervised segmentation task, a median Dice score of 0.728 (95% CI 0.715-0.739) was recorded. Notably, on a subset of tumour-only images, the best model achieved an accuracy of 98.7%, outperforming state-of-the-art glioma grading binary classifiers. Furthermore, comparative Precision-Recall analysis validated the framework's robustness against severe class imbalance, establishing a direct correlation between diagnostic confidence and segmentation fidelity. These results demonstrate that the proposed framework successfully combines high diagnostic accuracy with essential transparency, offering a promising direction for trustworthy clinical decision support. Code is available on GitHub: https://github.com/soumickmj/GPModels</description><author>cs.CV updates on arXiv.org</author><pubDate>Thu, 22 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2206.05148v3</guid></item><item><title>[arXiv-CV] Karhunen-Lo\`eve Expansion-Based Residual Anomaly Map for Resource-Efficient Glioma MRI Segmentation</title><link>https://arxiv.org/abs/2601.11833</link><description>arXiv:2601.11833v2 Announce Type: replace-cross 
Abstract: Accurate segmentation of brain tumors is essential for clinical diagnosis and treatment planning. Deep learning is currently the state-of-the-art for brain tumor segmentation, yet it requires either large datasets or extensive computational resources that are inaccessible in most areas. This makes the problem increasingly difficult: state-of-the-art models use thousands of training cases and vast computational power, where performance drops sharply when either is limited. The top performer in the Brats GLI 2023 competition relied on supercomputers trained on over 92,000 augmented MRI scans using an AMD EPYC 7402 CPU, six NVIDIA RTX 6000 GPUs (48GB VRAM each), and 1024GB of RAM over multiple weeks. To address this, the Karhunen--Lo\`eve Expansion (KLE) was implemented as a feature extraction step on downsampled, z-score normalized MRI volumes. Each 240$\times$240$\times$155 multi-modal scan is reduced to four $48^3$ channels and compressed into 32 KL coefficients. The resulting approximate reconstruction enables a residual-based anomaly map, which is upsampled and added as a fifth channel to a compact 3D U-Net. All experiments were run on a consumer workstation (AMD Ryzen 5 7600X CPU, RTX 4060Ti (8GB VRAM), and 64GB RAM while using far fewer training cases. This model achieves post-processed Dice scores of 0.929 (WT), 0.856 (TC), and 0.821 (ET), with HD95 distances of 2.93, 6.78, and 10.35 voxels. These results are significantly better than the winning BraTS 2023 methodology for HD95 distances and WT dice scores. This demonstrates that a KLE-based residual anomaly map can dramatically reduce computational cost and data requirements while retaining state-of-the-art performance.</description><author>cs.CV updates on arXiv.org</author><pubDate>Thu, 22 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2601.11833v2</guid></item><item><title>[arXiv-ML] Anytime Optimal Decision Tree Learning with Continuous Features</title><link>https://arxiv.org/abs/2601.14765</link><description>arXiv:2601.14765v1 Announce Type: new 
Abstract: In recent years, significant progress has been made on algorithms for learning optimal decision trees, primarily in the context of binary features. Extending these methods to continuous features remains substantially more challenging due to the large number of potential splits for each feature. Recently, an elegant exact algorithm was proposed for learning optimal decision trees with continuous features; however, the rapidly increasing computational time limits its practical applicability to shallow depths (typically 3 or 4). It relies on a depth-first search optimization strategy that fully optimizes the left subtree of each split before exploring the corresponding right subtree. While effective in finding optimal solutions given sufficient time, this strategy can lead to poor anytime behavior: when interrupted early, the best-found tree is often highly unbalanced and suboptimal. In such cases, purely greedy methods such as C4.5 may, paradoxically, yield better solutions. To address this limitation, we propose an anytime, yet complete approach leveraging limited discrepancy search, distributing the computational effort more evenly across the entire tree structure, and thus ensuring that a high-quality decision tree is available at any interruption point. Experimental results show that our approach outperforms the existing one in terms of anytime performance.</description><author>cs.LG updates on arXiv.org</author><pubDate>Thu, 22 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2601.14765v1</guid></item><item><title>[arXiv-ML] Learning from Discriminatory Training Data</title><link>https://arxiv.org/abs/1912.08189</link><description>arXiv:1912.08189v5 Announce Type: replace 
Abstract: Supervised learning systems are trained using historical data and, if the data was tainted by discrimination, they may unintentionally learn to discriminate against protected groups. We propose that fair learning methods, despite training on potentially discriminatory datasets, shall perform well on fair test datasets. Such dataset shifts crystallize application scenarios for specific fair learning methods. For instance, the removal of direct discrimination can be represented as a particular dataset shift problem. For this scenario, we propose a learning method that provably minimizes model error on fair datasets, while blindly training on datasets poisoned with direct additive discrimination. The method is compatible with existing legal systems and provides a solution to the widely discussed issue of protected groups' intersectionality by striking a balance between the protected groups. Technically, the method applies probabilistic interventions, has causal and counterfactual formulations, and is computationally lightweight - it can be used with any supervised learning model to prevent direct and indirect discrimination via proxies while maximizing model accuracy for business necessity.</description><author>cs.LG updates on arXiv.org</author><pubDate>Thu, 22 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:1912.08189v5</guid></item><item><title>[arXiv-ML] Guardrailed Elasticity Pricing: A Churn-Aware Forecasting Playbook for Subscription Strategy</title><link>https://arxiv.org/abs/2512.20932</link><description>arXiv:2512.20932v2 Announce Type: replace 
Abstract: This paper presents a marketing analytics framework that operationalizes subscription pricing as a dynamic, guardrailed decision system, uniting multivariate demand forecasting, segment-level price elasticity, and churn propensity to optimize revenue, margin, and retention. The approach blends seasonal time-series models with tree-based learners, runs Monte Carlo scenario tests to map risk envelopes, and solves a constrained optimization that enforces business guardrails on customer experience, margin floors, and allowable churn. Validated across heterogeneous SaaS portfolios, the method consistently outperforms static tiers and uniform uplifts by reallocating price moves toward segments with higher willingness-to-pay while protecting price-sensitive cohorts. The system is designed for real-time recalibration via modular APIs and includes model explainability for governance and compliance. Managerially, the framework functions as a strategy playbook that clarifies when to shift from flat to dynamic pricing, how to align pricing with CLV and MRR targets, and how to embed ethical guardrails, enabling durable growth without eroding customer trust.</description><author>cs.LG updates on arXiv.org</author><pubDate>Thu, 22 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2512.20932v2</guid></item><item><title>[BSPC] Efficient 3D medical image segmentation via Single-Slice Scribble Annotations: A near unsupervised approach</title><link>https://www.sciencedirect.com/science/article/pii/S1746809426001643?dgcid=rss_sd_all</link><description>&lt;p&gt;Publication date: 15 May 2026&lt;/p&gt;&lt;p&gt;&lt;b&gt;Source:&lt;/b&gt; Biomedical Signal Processing and Control, Volume 117&lt;/p&gt;&lt;p&gt;Author(s): Peihong Teng, Shaonan Yu, Binyu Zhang, Feiyang Yang, Tianyang Zhang, Bo Wang, Yiqing Yang, Guifeng Liu&lt;/p&gt;</description><author>ScienceDirect Publication: Biomedical Signal Processing and Control</author><pubDate>Wed, 21 Jan 2026 18:50:30 GMT</pubDate><guid isPermaLink="true">https://www.sciencedirect.com/science/article/pii/S1746809426001643</guid></item><item><title>[BSPC] XceSCNN: an interpretable deep learning framework for accurate skin cancer classification using enhanced segmentation and SHAP explanations</title><link>https://www.sciencedirect.com/science/article/pii/S174680942600114X?dgcid=rss_sd_all</link><description>&lt;p&gt;Publication date: 15 May 2026&lt;/p&gt;&lt;p&gt;&lt;b&gt;Source:&lt;/b&gt; Biomedical Signal Processing and Control, Volume 117&lt;/p&gt;&lt;p&gt;Author(s): H.Helintha Graceline, C.Helen Sulochana&lt;/p&gt;</description><author>ScienceDirect Publication: Biomedical Signal Processing and Control</author><pubDate>Wed, 21 Jan 2026 18:50:30 GMT</pubDate><guid isPermaLink="true">https://www.sciencedirect.com/science/article/pii/S174680942600114X</guid></item><item><title>[CBM] Research on breast ultrasound images lesion localization and diagnosis based on knowledge-driven and data-driven methods</title><link>https://www.sciencedirect.com/science/article/pii/S0010482526000260?dgcid=rss_sd_all</link><description>&lt;p&gt;Publication date: 15 February 2026&lt;/p&gt;&lt;p&gt;&lt;b&gt;Source:&lt;/b&gt; Computers in Biology and Medicine, Volume 203&lt;/p&gt;&lt;p&gt;Author(s): Jianqiang Li, Lintao Song, Xiaoling Liu, Yiming Liu, Tianbao Ma, Jun Bai, Qing Zhao, Xi Xu&lt;/p&gt;</description><author>ScienceDirect Publication: Computers in Biology and Medicine</author><pubDate>Wed, 21 Jan 2026 13:00:25 GMT</pubDate><guid isPermaLink="true">https://www.sciencedirect.com/science/article/pii/S0010482526000260</guid></item><item><title>[arXiv-CV] IMSAHLO: Integrating Multi-Scale Attention and Hybrid Loss Optimization Framework for Robust Neuronal Brain Cell Segmentation</title><link>https://arxiv.org/abs/2601.11645</link><description>arXiv:2601.11645v1 Announce Type: new 
Abstract: Accurate segmentation of neuronal cells in fluorescence microscopy is a fundamental task for quantitative analysis in computational neuroscience. However, it is significantly impeded by challenges such as the coexistence of densely packed and sparsely distributed cells, complex overlapping morphologies, and severe class imbalance. Conventional deep learning models often fail to preserve fine topological details or accurately delineate boundaries under these conditions. To address these limitations, we propose a novel deep learning framework, IMSAHLO (Integrating Multi-Scale Attention and Hybrid Loss Optimization), for robust and adaptive neuronal segmentation. The core of our model features Multi-Scale Dense Blocks (MSDBs) to capture features at various receptive fields, effectively handling variations in cell density, and a Hierarchical Attention (HA) mechanism that adaptively focuses on salient morphological features to preserve Region of Interest (ROI) boundary details. Furthermore, we introduce a novel hybrid loss function synergistically combining Tversky and Focal loss to combat class imbalance, alongside a topology-aware Centerline Dice (clDice) loss and a Contour-Weighted Boundary loss to ensure topological continuity and precise separation of adjacent cells. Large-scale experiments on the public Fluorescent Neuronal Cells (FNC) dataset demonstrate that our framework outperforms state-of-the-art architectures, achieving precision of 81.4%, macro F1 score of 82.7%, micro F1 score of 83.3%, and balanced accuracy of 99.5% on difficult dense and sparse cases. Ablation studies validate the synergistic benefits of multi-scale attention and hybrid loss terms. This work establishes a foundation for generalizable segmentation models applicable to a wide range of biomedical imaging modalities, pushing AI-assisted analysis toward high-throughput neurobiological pipelines.</description><author>cs.CV updates on arXiv.org</author><pubDate>Wed, 21 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2601.11645v1</guid></item><item><title>[arXiv-CV] A Hierarchical Benchmark of Foundation Models for Dermatology</title><link>https://arxiv.org/abs/2601.12382</link><description>arXiv:2601.12382v1 Announce Type: new 
Abstract: Foundation models have transformed medical image analysis by providing robust feature representations that reduce the need for large-scale task-specific training. However, current benchmarks in dermatology often reduce the complex diagnostic taxonomy to flat, binary classification tasks, such as distinguishing melanoma from benign nevi. This oversimplification obscures a model's ability to perform fine-grained differential diagnoses, which is critical for clinical workflow integration. This study evaluates the utility of embeddings derived from ten foundation models, spanning general computer vision, general medical imaging, and dermatology-specific domains, for hierarchical skin lesion classification. Using the DERM12345 dataset, which comprises 40 lesion subclasses, we calculated frozen embeddings and trained lightweight adapter models using a five-fold cross-validation. We introduce a hierarchical evaluation framework that assesses performance across four levels of clinical granularity: 40 Subclasses, 15 Main Classes, 2 and 4 Superclasses, and Binary Malignancy. Our results reveal a "granularity gap" in model capabilities: MedImageInsights achieved the strongest overall performance (97.52% weighted F1-Score on Binary Malignancy detection) but declined to 65.50% on fine-grained 40-class subtype classification. Conversely, MedSigLip (69.79%) and dermatology-specific models (Derm Foundation and MONET) excelled at fine-grained 40-class subtype discrimination while achieving lower overall performance than MedImageInsights on broader classification tasks. Our findings suggest that while general medical foundation models are highly effective for high-level screening, specialized modeling strategies are necessary for the granular distinctions required in diagnostic support systems.</description><author>cs.CV updates on arXiv.org</author><pubDate>Wed, 21 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2601.12382v1</guid></item><item><title>[arXiv-CV] GridNet-HD: A High-Resolution Multi-Modal Dataset for LiDAR-Image Fusion on Power Line Infrastructure</title><link>https://arxiv.org/abs/2601.13052</link><description>arXiv:2601.13052v1 Announce Type: new 
Abstract: This paper presents GridNet-HD, a multi-modal dataset for 3D semantic segmentation of overhead electrical infrastructures, pairing high-density LiDAR with high-resolution oblique imagery. The dataset comprises 7,694 images and 2.5 billion points annotated into 11 classes, with predefined splits and mIoU metrics. Unimodal (LiDAR-only, image-only) and multi-modal fusion baselines are provided. On GridNet-HD, fusion models outperform the best unimodal baseline by +5.55 mIoU, highlighting the complementarity of geometry and appearance. As reviewed in Sec. 2, no public dataset jointly provides high-density LiDAR and high-resolution oblique imagery with 3D semantic labels for power-line assets. Dataset, baselines, and codes are available: https://huggingface.co/collections/heig-vd-geo/gridnet-hd.</description><author>cs.CV updates on arXiv.org</author><pubDate>Wed, 21 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2601.13052v1</guid></item><item><title>[arXiv-CV] Finally Outshining the Random Baseline: A Simple and Effective Solution for Active Learning in 3D Biomedical Imaging</title><link>https://arxiv.org/abs/2601.13677</link><description>arXiv:2601.13677v1 Announce Type: new 
Abstract: Active learning (AL) has the potential to drastically reduce annotation costs in 3D biomedical image segmentation, where expert labeling of volumetric data is both time-consuming and expensive. Yet, existing AL methods are unable to consistently outperform improved random sampling baselines adapted to 3D data, leaving the field without a reliable solution. We introduce Class-stratified Scheduled Power Predictive Entropy (ClaSP PE), a simple and effective query strategy that addresses two key limitations of standard uncertainty-based AL methods: class imbalance and redundancy in early selections. ClaSP PE combines class-stratified querying to ensure coverage of underrepresented structures and log-scale power noising with a decaying schedule to enforce query diversity in early-stage AL and encourage exploitation later. In our evaluation on 24 experimental settings using four 3D biomedical datasets within the comprehensive nnActive benchmark, ClaSP PE is the only method that generally outperforms improved random baselines in terms of both segmentation quality with statistically significant gains, whilst remaining annotation efficient. Furthermore, we explicitly simulate the real-world application by testing our method on four previously unseen datasets without manual adaptation, where all experiment parameters are set according to predefined guidelines. The results confirm that ClaSP PE robustly generalizes to novel tasks without requiring dataset-specific tuning. Within the nnActive framework, we present compelling evidence that an AL method can consistently outperform random baselines adapted to 3D segmentation, in terms of both performance and annotation efficiency in a realistic, close-to-production scenario. Our open-source implementation and clear deployment guidelines make it readily applicable in practice. Code is at https://github.com/MIC-DKFZ/nnActive.</description><author>cs.CV updates on arXiv.org</author><pubDate>Wed, 21 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2601.13677v1</guid></item><item><title>[arXiv-CV] Dynamic Differential Linear Attention: Enhancing Linear Diffusion Transformer for High-Quality Image Generation</title><link>https://arxiv.org/abs/2601.13683</link><description>arXiv:2601.13683v1 Announce Type: new 
Abstract: Diffusion transformers (DiTs) have emerged as a powerful architecture for high-fidelity image generation, yet the quadratic cost of self-attention poses a major scalability bottleneck. To address this, linear attention mechanisms have been adopted to reduce computational cost; unfortunately, the resulting linear diffusion transformers (LiTs) models often come at the expense of generative performance, frequently producing over-smoothed attention weights that limit expressiveness. In this work, we introduce Dynamic Differential Linear Attention (DyDiLA), a novel linear attention formulation that enhances the effectiveness of LiTs by mitigating the oversmoothing issue and improving generation quality. Specifically, the novelty of DyDiLA lies in three key designs: (i) dynamic projection module, which facilitates the decoupling of token representations by learning with dynamically assigned knowledge; (ii) dynamic measure kernel, which provides a better similarity measurement to capture fine-grained semantic distinctions between tokens by dynamically assigning kernel functions for token processing; and (iii) token differential operator, which enables more robust query-to-key retrieval by calculating the differences between the tokens and their corresponding information redundancy produced by dynamic measure kernel. To capitalize on DyDiLA, we introduce a refined LiT, termed DyDi-LiT, that systematically incorporates our advancements. Extensive experiments show that DyDi-LiT consistently outperforms current state-of-the-art (SOTA) models across multiple metrics, underscoring its strong practical potential.</description><author>cs.CV updates on arXiv.org</author><pubDate>Wed, 21 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2601.13683v1</guid></item><item><title>[arXiv-CV] MVGD-Net: A Novel Motion-aware Video Glass Surface Detection Network</title><link>https://arxiv.org/abs/2601.13715</link><description>arXiv:2601.13715v1 Announce Type: new 
Abstract: Glass surface ubiquitous in both daily life and professional environments presents a potential threat to vision-based systems, such as robot and drone navigation. To solve this challenge, most recent studies have shown significant interest in Video Glass Surface Detection (VGSD). We observe that objects in the reflection (or transmission) layer appear farther from the glass surfaces. Consequently, in video motion scenarios, the notable reflected (or transmitted) objects on the glass surface move slower than objects in non-glass regions within the same spatial plane, and this motion inconsistency can effectively reveal the presence of glass surfaces. Based on this observation, we propose a novel network, named MVGD-Net, for detecting glass surfaces in videos by leveraging motion inconsistency cues. Our MVGD-Net features three novel modules: the Cross-scale Multimodal Fusion Module (CMFM) that integrates extracted spatial features and estimated optical flow maps, the History Guided Attention Module (HGAM) and Temporal Cross Attention Module (TCAM), both of which further enhances temporal features. A Temporal-Spatial Decoder (TSD) is also introduced to fuse the spatial and temporal features for generating the glass region mask. Furthermore, for learning our network, we also propose a large-scale dataset, which comprises 312 diverse glass scenarios with a total of 19,268 frames. Extensive experiments demonstrate that our MVGD-Net outperforms relevant state-of-the-art methods.</description><author>cs.CV updates on arXiv.org</author><pubDate>Wed, 21 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2601.13715v1</guid></item><item><title>[arXiv-CV] Generalizing Abstention for Noise-Robust Learning in Medical Image Segmentation</title><link>https://arxiv.org/abs/2601.14039</link><description>arXiv:2601.14039v1 Announce Type: new 
Abstract: Label noise is a critical problem in medical image segmentation, often arising from the inherent difficulty of manual annotation. Models trained on noisy data are prone to overfitting, which degrades their generalization performance. While a number of methods and strategies have been proposed to mitigate noisy labels in the segmentation domain, this area remains largely under-explored. The abstention mechanism has proven effective in classification tasks by enhancing the capabilities of Cross Entropy, yet its potential in segmentation remains unverified. In this paper, we address this gap by introducing a universal and modular abstention framework capable of enhancing the noise-robustness of a diverse range of loss functions. Our framework improves upon prior work with two key components: an informed regularization term to guide abstention behaviour, and a more flexible power-law-based auto-tuning algorithm for the abstention penalty. We demonstrate the framework's versatility by systematically integrating it with three distinct loss functions to create three novel, noise-robust variants: GAC, SAC, and ADS. Experiments on the CaDIS and DSAD medical datasets show our methods consistently and significantly outperform their non-abstaining baselines, especially under high noise levels. This work establishes that enabling models to selectively ignore corrupted samples is a powerful and generalizable strategy for building more reliable segmentation models. Our code is publicly available at https://github.com/wemous/abstention-for-segmentation.</description><author>cs.CV updates on arXiv.org</author><pubDate>Wed, 21 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2601.14039v1</guid></item><item><title>[arXiv-CV] Decoder-Free Supervoxel GNN for Accurate Brain-Tumor Localization in Multi-Modal MRI</title><link>https://arxiv.org/abs/2601.14055</link><description>arXiv:2601.14055v1 Announce Type: new 
Abstract: Modern vision backbones for 3D medical imaging typically process dense voxel grids through parameter-heavy encoder-decoder structures, a design that allocates a significant portion of its parameters to spatial reconstruction rather than feature learning. Our approach introduces SVGFormer, a decoder-free pipeline built upon a content-aware grouping stage that partitions the volume into a semantic graph of supervoxels. Its hierarchical encoder learns rich node representations by combining a patch-level Transformer with a supervoxel-level Graph Attention Network, jointly modeling fine-grained intra-region features and broader inter-regional dependencies. This design concentrates all learnable capacity on feature encoding and provides inherent, dual-scale explainability from the patch to the region level. To validate the framework's flexibility, we trained two specialized models on the BraTS dataset: one for node-level classification and one for tumor proportion regression. Both models achieved strong performance, with the classification model achieving a F1-score of 0.875 and the regression model a MAE of 0.028, confirming the encoder's ability to learn discriminative and localized features. Our results establish that a graph-based, encoder-only paradigm offers an accurate and inherently interpretable alternative for 3D medical image representation.</description><author>cs.CV updates on arXiv.org</author><pubDate>Wed, 21 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2601.14055v1</guid></item><item><title>[arXiv-CV] Karhunen-Lo\`eve Expansion-Based Residual Anomaly Map for Resource-Efficient Glioma MRI Segmentation</title><link>https://arxiv.org/abs/2601.11833</link><description>arXiv:2601.11833v1 Announce Type: cross 
Abstract: Accurate segmentation of brain tumors is essential for clinical diagnosis and treatment planning. Deep learning is currently the state-of-the-art for brain tumor segmentation, yet it requires either large datasets or extensive computational resources that are inaccessible in most areas. This makes the problem increasingly difficult: state-of-the-art models use thousands of training cases and vast computational power, where performance drops sharply when either is limited. The top performer in the Brats GLI 2023 competition relied on supercomputers trained on over 92,000 augmented MRI scans using an AMD EPYC 7402 CPU, six NVIDIA RTX 6000 GPUs (48GB VRAM each), and 1024GB of RAM over multiple weeks. To address this, the Karhunen--Lo\`eve Expansion (KLE) was implemented as a feature extraction step on downsampled, z-score normalized MRI volumes. Each 240$\times$240$\times$155 multi-modal scan is reduced to four $48^3$ channels and compressed into 32 KL coefficients. The resulting approximate reconstruction enables a residual-based anomaly map, which is upsampled and added as a fifth channel to a compact 3D U-Net. All experiments were run on a consumer workstation (AMD Ryzen 5 7600X CPU, RTX 4060Ti (8GB VRAM), and 64GB RAM while using far fewer training cases. This model achieves post-processed Dice scores of 0.929 (WT), 0.856 (TC), and 0.821 (ET), with HD95 distances of 2.93, 6.78, and 10.35 voxels. These results are significantly better than the winning BraTS 2023 methodology for HD95 distances and WT dice scores. This demonstrates that a KLE-based residual anomaly map can dramatically reduce computational cost and data requirements while retaining state-of-the-art performance.</description><author>cs.CV updates on arXiv.org</author><pubDate>Wed, 21 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2601.11833v1</guid></item><item><title>[arXiv-CV] Reasoning is a Modality</title><link>https://arxiv.org/abs/2601.13562</link><description>arXiv:2601.13562v1 Announce Type: cross 
Abstract: The Abstraction and Reasoning Corpus (ARC) provides a compact laboratory for studying abstract reasoning, an ability central to human intelligence. Modern AI systems, including LLMs and ViTs, largely operate as sequence-of-behavior prediction machines: they match observable behaviors by modeling token statistics without a persistent, readable mental state. This creates a gap with human-like behavior: humans can explain an action by decoding internal state, while AI systems can produce fluent post-hoc rationalizations that are not grounded in such a state. We hypothesize that reasoning is a modality: reasoning should exist as a distinct channel separate from the low-level workspace on which rules are applied. To test this hypothesis, on solving ARC tasks as a visual reasoning problem, we designed a novel role-separated transformer block that splits global controller tokens from grid workspace tokens, enabling iterative rule execution. Trained and evaluated within the VARC vision-centric protocol, our method achieved 62.6% accuracy on ARC-1, surpassing average human performance (60.2%) and outperforming prior methods significantly. Qualitatively, our models exhibit more coherent rule-application structure than the dense ViT baseline, consistent with a shift away from plausible probability blobs toward controller-driven reasoning.</description><author>cs.CV updates on arXiv.org</author><pubDate>Wed, 21 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2601.13562v1</guid></item><item><title>[arXiv-CV] PraNet-V2: Dual-Supervised Reverse Attention for Medical Image Segmentation</title><link>https://arxiv.org/abs/2504.10986</link><description>arXiv:2504.10986v2 Announce Type: replace 
Abstract: Accurate medical image segmentation is essential for effective diagnosis and treatment. Previously, PraNet-V1 was proposed to enhance polyp segmentation by introducing a reverse attention (RA) module that utilizes background information. However, PraNet-V1 struggles with multi-class segmentation tasks. To address this limitation, we propose PraNet-V2, which, compared to PraNet-V1, effectively performs a broader range of tasks including multi-class segmentation. At the core of PraNet-V2 is the Dual-Supervised Reverse Attention (DSRA) module, which incorporates explicit background supervision, independent background modeling, and semantically enriched attention fusion. Our PraNet-V2 framework demonstrates strong performance on four polyp segmentation datasets. Additionally, by integrating DSRA to iteratively enhance foreground segmentation results in three state-of-the-art semantic segmentation models, we achieve up to a 1.36% improvement in mean Dice score. Code is available at: https://github.com/ai4colonoscopy/PraNet-V2/tree/main/binary_seg/jittor.</description><author>cs.CV updates on arXiv.org</author><pubDate>Wed, 21 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2504.10986v2</guid></item><item><title>[arXiv-CV] TalkingHeadBench: A Multi-Modal Benchmark &amp; Analysis of Talking-Head DeepFake Detection</title><link>https://arxiv.org/abs/2505.24866</link><description>arXiv:2505.24866v3 Announce Type: replace 
Abstract: The rapid advancement of talking-head deepfake generation fueled by advanced generative models has elevated the realism of synthetic videos to a level that poses substantial risks in domains such as media, politics, and finance. However, current benchmarks for deepfake talking-head detection fail to reflect this progress, relying on outdated generators and offering limited insight into model robustness and generalization. We introduce TalkingHeadBench, a comprehensive multi-model multi-generator benchmark and curated dataset designed to evaluate the performance of state-of-the-art detectors on the most advanced generators. Our dataset includes deepfakes synthesized by leading academic and commercial models and features carefully constructed protocols to assess generalization under distribution shifts in identity and generator characteristics. We benchmark a diverse set of existing detection methods, including CNNs, vision transformers, and temporal models, and analyze their robustness and generalization capabilities. In addition, we provide error analysis using Grad-CAM visualizations to expose common failure modes and detector biases. TalkingHeadBench is hosted on https://huggingface.co/datasets/luchaoqi/TalkingHeadBench with open access to all data splits and protocols. Our benchmark aims to accelerate research towards more robust and generalizable detection models in the face of rapidly evolving generative techniques.</description><author>cs.CV updates on arXiv.org</author><pubDate>Wed, 21 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2505.24866v3</guid></item><item><title>[arXiv-CV] Paired Image Generation with Diffusion-Guided Diffusion Models</title><link>https://arxiv.org/abs/2507.14833</link><description>arXiv:2507.14833v2 Announce Type: replace 
Abstract: The segmentation of mass lesions in digital breast tomosynthesis (DBT) images is very significant for the early screening of breast cancer. However, the high-density breast tissue often leads to high concealment of the mass lesions, which makes manual annotation difficult and time-consuming. As a result, there is a lack of annotated data for model training. Diffusion models are commonly used for data augmentation, but the existing methods face two challenges. First, due to the high concealment of lesions, it is difficult for the model to learn the features of the lesion area. This leads to the low generation quality of the lesion areas, thus limiting the quality of the generated images. Second, existing methods can only generate images and cannot generate corresponding annotations, which restricts the usability of the generated images in supervised training. In this work, we propose a paired image generation method. The method does not require external conditions and can achieve the generation of paired images by training an extra diffusion guider for the conditional diffusion model. During the experimental phase, we generated paired DBT slices and mass lesion masks. Then, we incorporated them into the supervised training process of the mass lesion segmentation task. The experimental results show that our method can improve the generation quality without external conditions. Moreover, it contributes to alleviating the shortage of annotated data, thus enhancing the performance of downstream tasks. The source code is available at https://github.com/zhanghx1320/PIG.</description><author>cs.CV updates on arXiv.org</author><pubDate>Wed, 21 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2507.14833v2</guid></item><item><title>[arXiv-CV] Does DINOv3 Set a New Medical Vision Standard? Benchmarking 2D and 3D Classification, Segmentation, and Registration</title><link>https://arxiv.org/abs/2509.06467</link><description>arXiv:2509.06467v3 Announce Type: replace 
Abstract: The advent of large-scale vision foundation models, pre-trained on diverse natural images, has marked a paradigm shift in computer vision. However, how the frontier vision foundation models' efficacies transfer to specialised domains such as medical imaging remains an open question. This report investigates whether DINOv3, a state-of-the-art self-supervised vision transformer (ViT) pre-trained on natural images, can directly serve as a powerful, unified encoder for medical vision tasks without domain-specific fine-tuning. To answer this, we benchmark DINOv3 across common medical vision tasks, including 2D and 3D classification, segmentation, and registration on a wide range of medical imaging modalities. We systematically analyse its scalability by varying model sizes and input image resolutions. Our findings reveal that DINOv3 shows impressive performance and establishes a formidable new baseline. Remarkably, it can even outperform medical-specific foundation models like BiomedCLIP and CT-Net on several tasks, despite being trained solely on natural images. However, we identify clear limitations: The model's features degrade in scenarios requiring deep domain specialisation, such as in whole-slide images (WSIs), electron microscopy (EM), and positron emission tomography (PET). Furthermore, we observe that DINOv3 does not consistently follow the scaling law in the medical domain. Its performance does not reliably increase with larger models or finer feature resolutions, showing diverse scaling behaviours across tasks. Overall, our work establishes DINOv3 as a strong baseline, whose powerful visual features can serve as a robust prior for multiple medical tasks. This opens promising future directions, such as leveraging its features to enforce multiview consistency in 3D reconstruction.</description><author>cs.CV updates on arXiv.org</author><pubDate>Wed, 21 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2509.06467v3</guid></item><item><title>[arXiv-CV] Hierarchy-Aware Multimodal Unlearning for Medical AI</title><link>https://arxiv.org/abs/2512.09867</link><description>arXiv:2512.09867v2 Announce Type: replace 
Abstract: Pretrained Multimodal Large Language Models (MLLMs) are increasingly used in sensitive domains such as medical AI, where privacy regulations like HIPAA and GDPR require specific removal of individuals' or institutions' data. This motivates machine unlearning, which aims to remove the influence of target data from a trained model. However, existing unlearning benchmarks fail to reflect the hierarchical and multimodal structure of real-world medical data, limiting their ability to properly evaluate unlearning in practice. Therefore, we introduce MedForget, a hierarchy-aware multimodal unlearning benchmark that models hospital data as a nested structure, enabling fine-grained evaluation of multimodal unlearning across retain and forget splits. Experiments with current unlearning methods show that existing approaches struggle to achieve effective hierarchy-aware forgetting without degrading downstream medical utility. To address this limitation, we propose Cross-modal Hierarchy-Informed Projection for unlearning (CHIP), a training-free, hierarchy-aware multimodal unlearning method that deletes information by selectively removing target-specific weight subspaces while preserving sibling-shared information. Experiments show that CHIP achieves the highest forget-retain performance gap across all hierarchy levels while maintaining competitive downstream utility compared to existing methods. Overall, MedForget provides a practical, HIPAA-aligned benchmark for evaluating structured multimodal unlearning for medical data, and CHIP offers an effective and general solution for hierarchy-aware forgetting that balances deletion with utility.</description><author>cs.CV updates on arXiv.org</author><pubDate>Wed, 21 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2512.09867v2</guid></item><item><title>[arXiv-CV] BikeActions: An Open Platform and Benchmark for Cyclist-Centric VRU Action Recognition</title><link>https://arxiv.org/abs/2601.10521</link><description>arXiv:2601.10521v2 Announce Type: replace 
Abstract: Anticipating the intentions of Vulnerable Road Users (VRUs) is a critical challenge for safe autonomous driving (AD) and mobile robotics. While current research predominantly focuses on pedestrian crossing behaviors from a vehicle's perspective, interactions within dense shared spaces remain underexplored. To bridge this gap, we introduce FUSE-Bike, the first fully open perception platform of its kind. Equipped with two LiDARs, a camera, and GNSS, it facilitates high-fidelity, close-range data capture directly from a cyclist's viewpoint. Leveraging this platform, we present BikeActions, a novel multi-modal dataset comprising 852 annotated samples across 5 distinct action classes, specifically tailored to improve VRU behavior modeling. We establish a rigorous benchmark by evaluating state-of-the-art graph convolution and transformer-based models on our publicly released data splits, establishing the first performance baselines for this challenging task. We release the full dataset together with data curation tools, the open hardware design, and the benchmark code to foster future research in VRU action understanding under https://iv.ee.hm.edu/bikeactions/.</description><author>cs.CV updates on arXiv.org</author><pubDate>Wed, 21 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2601.10521v2</guid></item><item><title>[arXiv-CV] Jordan-Segmentable Masks: A Topology-Aware definition for characterizing Binary Image Segmentation</title><link>https://arxiv.org/abs/2601.10577</link><description>arXiv:2601.10577v2 Announce Type: replace 
Abstract: Image segmentation plays a central role in computer vision. However, widely used evaluation metrics, whether pixel-wise, region-based, or boundary-focused, often struggle to capture the structural and topological coherence of a segmentation. In many practical scenarios, such as medical imaging or object delineation, small inaccuracies in boundary, holes, or fragmented predictions can result in high metric scores, despite the fact that the resulting masks fail to preserve the object global shape or connectivity. This highlights a limitation of conventional metrics: they are unable to assess whether a predicted segmentation partitions the image into meaningful interior and exterior regions.
  In this work, we introduce a topology-aware notion of segmentation based on the Jordan Curve Theorem, and adapted for use in digital planes. We define the concept of a \emph{Jordan-segmentatable mask}, which is a binary segmentation whose structure ensures a topological separation of the image domain into two connected components. We analyze segmentation masks through the lens of digital topology and homology theory, extracting a $4$-curve candidate from the mask, verifying its topological validity using Betti numbers. A mask is considered Jordan-segmentatable when this candidate forms a digital 4-curve with $\beta_0 = \beta_1 = 1$, or equivalently when its complement splits into exactly two $8$-connected components.
  This framework provides a mathematically rigorous, unsupervised criterion with which to assess the structural coherence of segmentation masks. By combining digital Jordan theory and homological invariants, our approach provides a valuable alternative to standard evaluation metrics, especially in applications where topological correctness must be preserved.</description><author>cs.CV updates on arXiv.org</author><pubDate>Wed, 21 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2601.10577v2</guid></item><item><title>[arXiv-IV] Synthetic Volumetric Data Generation Enables Zero-Shot Generalization of Foundation Models in 3D Medical Image Segmentation</title><link>https://arxiv.org/abs/2601.12297</link><description>arXiv:2601.12297v1 Announce Type: new 
Abstract: Foundation models such as Segment Anything Model 2 (SAM 2) exhibit strong generalization on natural images and videos but perform poorly on medical data due to differences in appearance statistics, imaging physics, and three-dimensional structure. To address this gap, we introduce SynthFM-3D, an analytical framework that mathematically models 3D variability in anatomy, contrast, boundary definition, and noise to generate synthetic data for training promptable segmentation models without real annotations. We fine-tuned SAM 2 on 10,000 SynthFM-3D volumes and evaluated it on eleven anatomical structures across three medical imaging modalities (CT, MR, ultrasound) from five public datasets. SynthFM-3D training led to consistent and statistically significant Dice score improvements over the pretrained SAM 2 baseline, demonstrating stronger zero-shot generalization across modalities. When compared with the supervised SAM-Med3D model on unseen cardiac ultrasound data, SynthFM-3D achieved 2-3x higher Dice scores, establishing analytical 3D data modeling as an effective pathway to modality-agnostic medical segmentation.</description><author>eess.IV updates on arXiv.org</author><pubDate>Wed, 21 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2601.12297v1</guid></item><item><title>[arXiv-IV] Towards Modality-Agnostic Continual Domain-Incremental Brain Lesion Segmentation</title><link>https://arxiv.org/abs/2601.13927</link><description>arXiv:2601.13927v1 Announce Type: new 
Abstract: Brain lesion segmentation from multi-modal MRI often assumes fixed modality sets or predefined pathologies, making existing models difficult to adapt across cohorts and imaging protocols. Continual learning (CL) offers a natural solution but current approaches either impose a maximum modality configuration or suffer from severe forgetting in buffer-free settings. We introduce CLMU-Net, a replay-based CL framework for 3D brain lesion segmentation that supports arbitrary and variable modality combinations without requiring prior knowledge of the maximum set. A conceptually simple yet effective channel-inflation strategy maps any modality subset into a unified multi-channel representation, enabling a single model to operate across diverse datasets. To enrich inherently local 3D patch features, we incorporate lightweight domain-conditioned textual embeddings that provide global modality-disease context for each training case. Forgetting is further reduced through principled replay using a compact buffer composed of both prototypical and challenging samples. Experiments on five heterogeneous MRI brain datasets demonstrate that CLMU-Net consistently outperforms popular CL baselines. Notably, our method yields an average Dice score improvement of $\geq$ 18\% while remaining robust under heterogeneous-modality conditions. These findings underscore the value of flexible modality handling, targeted replay, and global contextual cues for continual medical image segmentation. Our implementation is available at https://github.com/xmindflow/CLMU-Net.</description><author>eess.IV updates on arXiv.org</author><pubDate>Wed, 21 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2601.13927v1</guid></item><item><title>[arXiv-ML] GRADE: Replacing Policy Gradients with Backpropagation for LLM Alignment</title><link>https://arxiv.org/abs/2601.11574</link><description>arXiv:2601.11574v1 Announce Type: new 
Abstract: Reinforcement learning from human feedback (RLHF) has become the dominant paradigm for aligning large language models with human preferences. However, policy gradient methods such as PPO suffer from high variance gradient estimates, requiring careful hyperparameter tuning and extensive computational resources. We introduce GRADE (Gumbel-softmax Relaxation for Alignment via Differentiable Estimation), a method that replaces high-variance policy gradient estimation with direct backpropagation through a differentiable relaxation of the discrete token sampling process. Using the Gumbel-Softmax reparameterization with straight-through estimation (GRADE-STE), we enable end-to-end gradient flow from reward signals through generated tokens to model parameters. On sentiment-controlled text generation using the IMDB dataset, GRADE-STE achieves a test reward of 0.763 +- 0.344 compared to PPO's 0.510 +- 0.313 and REINFORCE's 0.617 +- 0.378, representing a 50% relative improvement over PPO. Critically, GRADE-STE exhibits gradient variance over 14 times lower than REINFORCE and maintains stable training dynamics throughout optimization. Our rigorous evaluation with proper train/validation/test splits demonstrates that these improvements generalize to held-out data, with GRADE-STE showing the best generalization characteristics among all methods tested. GRADE offers a simpler, more stable, and more effective alternative to reinforcement learning for LLM alignment.</description><author>cs.LG updates on arXiv.org</author><pubDate>Wed, 21 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2601.11574v1</guid></item><item><title>[arXiv-ML] Suspicious Alignment of SGD: A Fine-Grained Step Size Condition Analysis</title><link>https://arxiv.org/abs/2601.11789</link><description>arXiv:2601.11789v1 Announce Type: new 
Abstract: This paper explores the suspicious alignment phenomenon in stochastic gradient descent (SGD) under ill-conditioned optimization, where the Hessian spectrum splits into dominant and bulk subspaces. This phenomenon describes the behavior of gradient alignment in SGD updates. Specifically, during the initial phase of SGD updates, the alignment between the gradient and the dominant subspace tends to decrease. Subsequently, it enters a rising phase and eventually stabilizes in a high-alignment phase. The alignment is considered ``suspicious'' because, paradoxically, the projected gradient update along this highly-aligned dominant subspace proves ineffective at reducing the loss. The focus of this work is to give a fine-grained analysis in a high-dimensional quadratic setup about how step size selection produces this phenomenon. Our main contribution can be summarized as follows: We propose a step-size condition revealing that in low-alignment regimes, an adaptive critical step size $\eta_t^*$ separates alignment-decreasing ($\eta_t &lt; \eta_t^*$) from alignment-increasing ($\eta_t &gt; \eta_t^*$) regimes, whereas in high-alignment regimes, the alignment is self-correcting and decreases regardless of the step size. We further show that under sufficient ill-conditioning, a step size interval exists where projecting the SGD updates to the bulk space decreases the loss while projecting them to the dominant space increases the loss, which explains a recent empirical observation that projecting gradient updates to the dominant subspace is ineffective. Finally, based on this adaptive step-size theory, we prove that for a constant step size and large initialization, SGD exhibits this distinct two-phase behavior: an initial alignment-decreasing phase, followed by stabilization at high alignment.</description><author>cs.LG updates on arXiv.org</author><pubDate>Wed, 21 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2601.11789v1</guid></item><item><title>[arXiv-ML] Approximating splits for decision trees quickly in sparse data streams</title><link>https://arxiv.org/abs/2601.12525</link><description>arXiv:2601.12525v1 Announce Type: new 
Abstract: Decision trees are one of the most popular classifiers in the machine learning literature. While the most common decision tree learning algorithms treat data as a batch, numerous algorithms have been proposed to construct decision trees from a data stream. A standard training strategy involves augmenting the current tree by changing a leaf node into a split. Here we typically maintain counters in each leaf which allow us to determine the optimal split, and whether the split should be done. In this paper we focus on how to speed up the search for the optimal split when dealing with sparse binary features and a binary class. We focus on finding splits that have the approximately optimal information gain or Gini index. In both cases finding the optimal split can be done in $O(d)$ time, where $d$ is the number of features. We propose an algorithm that yields $(1 + \alpha)$ approximation when using conditional entropy in amortized $O(\alpha^{-1}(1 + m\log d) \log \log n)$ time, where $m$ is the number of 1s in a data point, and $n$ is the number of data points. Similarly, for Gini index, we achieve $(1 + \alpha)$ approximation in amortized $O(\alpha^{-1} + m \log d)$ time. Our approach is beneficial for sparse data where $m \ll d$. In our experiments we find almost-optimal splits efficiently, faster than the baseline, overperforming the theoretical approximation guarantees.</description><author>cs.LG updates on arXiv.org</author><pubDate>Wed, 21 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2601.12525v1</guid></item><item><title>[arXiv-ML] Who Should Have Surgery? A Comparative Study of GenAI vs Supervised ML for CRS Surgical Outcome Prediction</title><link>https://arxiv.org/abs/2601.13710</link><description>arXiv:2601.13710v1 Announce Type: new 
Abstract: Artificial intelligence has reshaped medical imaging, yet the use of AI on clinical data for prospective decision support remains limited. We study pre-operative prediction of clinically meaningful improvement in chronic rhinosinusitis (CRS), defining success as a more than 8.9-point reduction in SNOT-22 at 6 months (MCID). In a prospectively collected cohort where all patients underwent surgery, we ask whether models using only pre-operative clinical data could have identified those who would have poor outcomes, i.e. those who should have avoided surgery. We benchmark supervised ML (logistic regression, tree ensembles, and an in-house MLP) against generative AI (ChatGPT, Claude, Gemini, Perplexity), giving each the same structured inputs and constraining outputs to binary recommendations with confidence. Our best ML model (MLP) achieves 85 % accuracy with superior calibration and decision-curve net benefit. GenAI models underperform on discrimination and calibration across zero-shot setting. Notably, GenAI justifications align with clinician heuristics and the MLP's feature importance, repeatedly highlighting baseline SNOT-22, CT/endoscopy severity, polyp phenotype, and physchology/pain comorbidities. We provide a reproducible tabular-to-GenAI evaluation protocol and subgroup analyses. Findings support an ML-first, GenAI- augmented workflow: deploy calibrated ML for primary triage of surgical candidacy, with GenAI as an explainer to enhance transparency and shared decision-making.</description><author>cs.LG updates on arXiv.org</author><pubDate>Wed, 21 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2601.13710v1</guid></item><item><title>[arXiv-ML] ELSA: Efficient LLM-Centric Split Aggregation for Privacy-Aware Hierarchical Federated Learning over Resource-Constrained Edge Networks</title><link>https://arxiv.org/abs/2601.13824</link><description>arXiv:2601.13824v1 Announce Type: new 
Abstract: Training large language models (LLMs) at the network edge faces fundamental challenges arising from device resource constraints, severe data heterogeneity, and heightened privacy risks. To address these, we propose ELSA (Efficient LLM-centric Split Aggregation), a novel framework that systematically integrates split learning (SL) and hierarchical federated learning (HFL) for distributed LLM fine-tuning over resource-constrained edge networks. ELSA introduces three key innovations. First, it employs a task-agnostic, behavior-aware client clustering mechanism that constructs semantic fingerprints using public probe inputs and symmetric KL divergence, further enhanced by prediction-consistency-based trust scoring and latency-aware edge assignment to jointly address data heterogeneity, client unreliability, and communication constraints. Second, it splits the LLM into three parts across clients and edge servers, with the cloud used only for adapter aggregation, enabling an effective balance between on-device computation cost and global convergence stability. Third, it incorporates a lightweight communication scheme based on computational sketches combined with semantic subspace orthogonal perturbation (SS-OP) to reduce communication overhead while mitigating privacy leakage during model exchanges. Experiments across diverse NLP tasks demonstrate that ELSA consistently outperforms state-of-the-art methods in terms of adaptability, convergence behavior, and robustness, establishing a scalable and privacy-aware solution for edge-side LLM fine-tuning under resource constraints.</description><author>cs.LG updates on arXiv.org</author><pubDate>Wed, 21 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2601.13824v1</guid></item><item><title>[arXiv-ML] Early Linguistic Pattern of Anxiety from Social Media Using Interpretable Linguistic Features: A Multi-Faceted Validation Study with Author-Disjoint Evaluation</title><link>https://arxiv.org/abs/2601.11758</link><description>arXiv:2601.11758v1 Announce Type: cross 
Abstract: Anxiety affects hundreds of millions of individuals globally, yet large-scale screening remains limited. Social media language provides an opportunity for scalable detection, but current models often lack interpretability, keyword-robustness validation, and rigorous user-level data integrity. This work presents a transparent approach to social media-based anxiety detection through linguistically interpretable feature-grounded modeling and cross-domain validation. Using a substantial dataset of Reddit posts, we trained a logistic regression classifier on carefully curated subreddits for training, validation, and test splits. Comprehensive evaluation included feature ablation, keyword masking experiments, and varying-density difference analyses comparing anxious and control groups, along with external validation using clinically interviewed participants with diagnosed anxiety disorders. The model achieved strong performance while maintaining high accuracy even after sentiment removal or keyword masking. Early detection using minimal post history significantly outperformed random classification, and cross-domain analysis demonstrated strong consistency with clinical interview data. Results indicate that transparent linguistic features can support reliable, generalizable, and keyword-robust anxiety detection. The proposed framework provides a reproducible baseline for interpretable mental health screening across diverse online contexts.</description><author>cs.LG updates on arXiv.org</author><pubDate>Wed, 21 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2601.11758v1</guid></item><item><title>[arXiv-ML] Industry-Aligned Granular Topic Modeling</title><link>https://arxiv.org/abs/2601.11762</link><description>arXiv:2601.11762v1 Announce Type: cross 
Abstract: Topic modeling has extensive applications in text mining and data analysis across various industrial sectors. Although the concept of granularity holds significant value for business applications by providing deeper insights, the capability of topic modeling methods to produce granular topics has not been thoroughly explored. In this context, this paper introduces a framework called TIDE, which primarily provides a novel granular topic modeling method based on large language models (LLMs) as a core feature, along with other useful functionalities for business applications, such as summarizing long documents, topic parenting, and distillation. Through extensive experiments on a variety of public and real-world business datasets, we demonstrate that TIDE's topic modeling approach outperforms modern topic modeling methods, and our auxiliary components provide valuable support for dealing with industrial business scenarios. The TIDE framework is currently undergoing the process of being open sourced.</description><author>cs.LG updates on arXiv.org</author><pubDate>Wed, 21 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2601.11762v1</guid></item><item><title>[arXiv-ML] Temporal Data and Short-Time Averages Improve Multiphase Mass Flow Metering</title><link>https://arxiv.org/abs/2601.12433</link><description>arXiv:2601.12433v1 Announce Type: cross 
Abstract: Reliable flow measurements are essential in many industries, but current instruments often fail to accurately estimate multiphase flows, which are frequently encountered in real-world operations. Combining machine learning (ML) algorithms with accurate single-phase flowmeters has therefore received extensive research attention in recent years. The Coriolis mass flowmeter is a widely used single-phase meter that provides direct mass flow measurements, which ML models can be trained to correct, thereby reducing measurement errors in multiphase conditions. This paper demonstrates that preserving temporal information significantly improves model performance in such scenarios. We compare a multilayer perceptron, a windowed multilayer perceptron, and a convolutional neural network (CNN) on three-phase air-water-oil flow data from 342 experiments. Whereas prior work typically compresses each experiment into a single averaged sample, we instead compute short-time averages from within each experiment and train models that preserve temporal information at several downsampling intervals. The CNN performed best at 0.25 Hz with approximately 95 % of relative errors below 13 %, a normalized root mean squared error of 0.03, and a mean absolute percentage error of approximately 4.3 %, clearly outperforming the best single-averaged model and demonstrating that short-time averaging within individual experiments is preferable. Results are consistent across multiple data splits and random seeds, demonstrating robustness.</description><author>cs.LG updates on arXiv.org</author><pubDate>Wed, 21 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2601.12433v1</guid></item><item><title>[arXiv-ML] The Tag is the Signal: URL-Agnostic Credibility Scoring for Messages on Telegram</title><link>https://arxiv.org/abs/2601.13294</link><description>arXiv:2601.13294v1 Announce Type: cross 
Abstract: Telegram has become one of the leading platforms for disseminating misinformational messages. However, many existing pipelines still classify each message's credibility based on the reputation of its associated domain names or its lexical features. Such methods work well on traditional long-form news articles published by well-known sources, but high-risk posts on Telegram are short and URL-sparse, leading to failures for link-based and standard TF-IDF models. To this end, we propose the TAG2CRED pipeline, a method designed for such short, convoluted messages. Our model will directly score each post based on the tags assigned to the text. We designed a concise label system that covers the dimensions of theme, claim type, call to action, and evidence. The fine-tuned large language model (LLM) assigns tags to messages and then maps these tags to calibrated risk scores in the [0,1] interval through L2-regularized logistic regression. We evaluated 87,936 Telegram messages associated with Media Bias/Fact Check (MBFC), using URL masking and domain disjoint splits. The results showed that the ROC-AUC of the TAG2CRED model reached 0.871, the macro-F1 value was 0.787, and the Brier score was 0.167, outperforming the baseline TF-IDF (macro-F1 value 0.737, Brier score 0.248); at the same time, the number of features used in this model is much smaller, and the generalization ability on infrequent domains is stronger. The performance of the stacked ensemble model (TF-IDF + TAG2CRED + SBERT) was further improved over the baseline SBERT. ROC-AUC reached 0.901, and the macro-F1 value was 0.813 (Brier score 0.114). This indicates that style labels and lexical features may capture different but complementary dimensions of information risk.</description><author>cs.LG updates on arXiv.org</author><pubDate>Wed, 21 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2601.13294v1</guid></item><item><title>[arXiv-ML] Compton Form Factor Extraction using Quantum Deep Neural Networks</title><link>https://arxiv.org/abs/2504.15458</link><description>arXiv:2504.15458v3 Announce Type: replace 
Abstract: We extract Compton form factors (CFFs) from deeply virtual Compton scattering measurements at the Thomas Jefferson National Accelerator Facility (JLab) using quantum-inspired deep neural networks (QDNNs). The analysis implements the twist-2 Belitsky-Kirchner-M\"uller formalism and employs a fitting strategy that emulates standard local fits. Using pseudodata, we benchmark QDNNs against classical deep neural networks (CDNNs) and find that QDNNs often deliver higher predictive accuracy and tighter uncertainties at comparable model complexity. Guided by these results, we introduce a quantitative selection metric that indicates when QDNNs or CDNNs are optimal for a given experimental fit. After obtaining local extractions from the JLab data, we perform a standard neural-network global CFF fit and compare with previous global analyses. The results support QDNNs as an efficient and complementary tool to CDNNs for CFF determination and for future multidimensional studies of parton distributions and hadronic structure.</description><author>cs.LG updates on arXiv.org</author><pubDate>Wed, 21 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2504.15458v3</guid></item><item><title>[arXiv-ML] A Multi-Head Attention Soft Random Forest for Interpretable Patient No-Show Prediction</title><link>https://arxiv.org/abs/2505.17344</link><description>arXiv:2505.17344v2 Announce Type: replace 
Abstract: Unattended scheduled appointments, defined as patient no-shows, adversely affect both healthcare providers and patients' health, disrupting the continuity of care, operational efficiency, and the efficient allocation of medical resources. Accurate predictive modeling is needed to reduce the impact of no-shows. Although machine learning methods, such as logistic regression, random forest models, and decision trees, are widely used in predicting patient no-shows, they often rely on hard decision splits and static feature importance, limiting their adaptability to specific or complex patient behaviors. To address this limitation, we propose a new hybrid Multi-Head Attention Soft Random Forest (MHASRF) model that integrates attention mechanisms into a random forest model using probabilistic soft splitting instead of hard splitting. The MHASRF model assigns attention weights differently across the trees, enabling attention on specific patient behaviors. The model exhibited 93.72% accuracy, 94.77% specificity, 90.23% precision, 89.38% recall, a 91.54% F1 score and AUC 97.87%, demonstrated high and balance performance across metrics, outperforming decision tree, random forest, logistic regression, and naive bayes models overall. Furthermore, MHASRF was able to identify key predictors of patient no-shows using two levels of feature importance (tree level and attention mechanism level), offering deeper insights into patient no-show predictors. The proposed model is a robust, adaptable, and interpretable method for predicting patient no-shows that will help healthcare providers in optimizing resources.</description><author>cs.LG updates on arXiv.org</author><pubDate>Wed, 21 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2505.17344v2</guid></item><item><title>[arXiv-ML] Position: Foundation Models for Tabular Data within Systemic Contexts Need Grounding</title><link>https://arxiv.org/abs/2505.19825</link><description>arXiv:2505.19825v2 Announce Type: replace 
Abstract: This position paper argues that foundation models for tabular data face inherent limitations when isolated from operational context - the procedural logic, declarative rules, and domain knowledge that define how data is created and governed. Current approaches focus on single-table generalization or schema-level relationships, fundamentally missing the operational knowledge that gives data meaning. We introduce Semantically Linked Tables (SLT) and Foundation Models for SLT (FMSLT) as a new model class that grounds tabular data in its operational context. We propose dual-phase training: pre-training on open-source code-data pairs and synthetic systems to learn business logic mechanics, followed by zero-shot inference on proprietary data. We introduce the ``Operational Turing Test'' benchmark and argue that operational grounding is essential for autonomous agents in complex data environments.</description><author>cs.LG updates on arXiv.org</author><pubDate>Wed, 21 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2505.19825v2</guid></item><item><title>[arXiv-ML] GraphBench: Next-generation graph learning benchmarking</title><link>https://arxiv.org/abs/2512.04475</link><description>arXiv:2512.04475v4 Announce Type: replace 
Abstract: Machine learning on graphs has recently achieved impressive progress in various domains, including molecular property prediction and chip design. However, benchmarking practices remain fragmented, often relying on narrow, task-specific datasets and inconsistent evaluation protocols, which hampers reproducibility and broader progress. To address this, we introduce GraphBench, a comprehensive benchmarking suite that spans diverse domains and prediction tasks, including node-level, edge-level, graph-level, and generative settings. GraphBench provides standardized evaluation protocols -- with consistent dataset splits and performance metrics that account for out-of-distribution generalization -- as well as a unified hyperparameter tuning framework. Additionally, we benchmark GraphBench using message-passing neural networks and graph transformer models, providing principled baselines and establishing a reference performance. See www.graphbench.io for further details.</description><author>cs.LG updates on arXiv.org</author><pubDate>Wed, 21 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2512.04475v4</guid></item><item><title>[arXiv-ML] Multi-Scenario Highway Lane-Change Intention Prediction: A Temporal Physics-Informed Multi-Modal Framework</title><link>https://arxiv.org/abs/2512.24075</link><description>arXiv:2512.24075v2 Announce Type: replace 
Abstract: Lane-change intention prediction is safety-critical for autonomous driving and ADAS, but remains difficult in naturalistic traffic due to noisy kinematics, severe class imbalance, and limited generalization across heterogeneous highway scenarios. We propose Temporal Physics-Informed AI (TPI-AI), a hybrid framework that fuses deep temporal representations with physics-inspired interaction cues. A two-layer bidirectional LSTM (Bi-LSTM) encoder learns compact embeddings from multi-step trajectory histories; we concatenate these embeddings with kinematics-, safety-, and interaction-aware features (e.g., headway, TTC, and safe-gap indicators) and train a LightGBM classifier for three-class intention recognition (No-LC, Left-LC, Right-LC). To improve minority-class reliability, we apply imbalance-aware optimization including resampling/weighting and fold-wise threshold calibration. Experiments on two large-scale drone-based datasets, highD (straight highways) and exiD (ramp-rich environments), use location-based splits and evaluate prediction horizons T = 1, 2, 3 s. TPI-AI outperforms standalone LightGBM and Bi-LSTM baselines, achieving macro-F1 of 0.9562, 0.9124, 0.8345 on highD and 0.9247, 0.8197, 0.7605 on exiD at T = 1, 2, 3 s, respectively. These results show that combining physics-informed interaction features with learned temporal embeddings yields robust multi-scenario lane-change intention prediction.</description><author>cs.LG updates on arXiv.org</author><pubDate>Wed, 21 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2512.24075v2</guid></item><item><title>[arXiv-ML] Horizon Activation Mapping for Neural Networks in Time Series Forecasting</title><link>https://arxiv.org/abs/2601.02094</link><description>arXiv:2601.02094v3 Announce Type: replace 
Abstract: Neural networks for time series forecasting have relied on error metrics and architecture-specific interpretability approaches for model selection that don't apply across models of different families. To interpret forecasting models agnostic to the types of layers across state-of-the-art model families, we introduce Horizon Activation Mapping (HAM), a visual interpretability technique inspired by grad-CAM that uses gradient norm averages to study the horizon's subseries where grad-CAM studies attention maps over image data. We introduce causal and anti-causal modes to calculate gradient update norm averages across subseries at every timestep and lines of proportionality signifying uniform distributions of the norm averages. Optimization landscape studies with respect to changes in batch sizes, early stopping, train-val-test splits, architectural choices, univariate forecasting and dropouts are studied with respect to performances and subseries in HAM. Interestingly, batch size based differences in activities seem to indicate potential for existence of an exponential approximation across them per epoch relative to each other. Multivariate forecasting models including MLP-based CycleNet, N-Linear, N-HITS, self attention-based FEDformer, Pyraformer, SSM-based SpaceTime and diffusion-based Multi-Resolution DDPM over different horizon sizes trained over the ETTm2 dataset are used for HAM plots in this study. NHITS' neural approximation theorem and SpaceTime's exponential autoregressive activities have been attributed to trends in HAM plots over their training, validation and test sets. In general, HAM can be used for granular model selection, validation set choices and comparisons across different neural network model families.</description><author>cs.LG updates on arXiv.org</author><pubDate>Wed, 21 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2601.02094v3</guid></item><item><title>[arXiv-ML] XMAD-Bench: Cross-Domain Multilingual Audio Deepfake Benchmark</title><link>https://arxiv.org/abs/2506.00462</link><description>arXiv:2506.00462v2 Announce Type: replace-cross 
Abstract: Recent advances in audio generation led to an increasing number of deepfakes, making the general public more vulnerable to financial scams, identity theft, and misinformation. Audio deepfake detectors promise to alleviate this issue, with many recent studies reporting accuracy rates close to 99%. However, these methods are typically tested in an in-domain setup, where the deepfake samples from the training and test sets are produced by the same generative models. To this end, we introduce XMAD-Bench, a large-scale cross-domain multilingual audio deepfake benchmark comprising 668.8 hours of real and deepfake speech. In our novel dataset, the speakers, the generative methods, and the real audio sources are distinct across training and test splits. This leads to a challenging cross-domain evaluation setup, where audio deepfake detectors can be tested "in the wild". Our in-domain and cross-domain experiments indicate a clear disparity between the in-domain performance of deepfake detectors, which is usually as high as 100%, and the cross-domain performance of the same models, which is sometimes similar to random chance. Our benchmark highlights the need for the development of robust audio deepfake detectors, which maintain their generalization capacity across different languages, speakers, generative methods, and data sources. Our benchmark is publicly released at https://github.com/ristea/xmad-bench/.</description><author>cs.LG updates on arXiv.org</author><pubDate>Wed, 21 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2506.00462v2</guid></item><item><title>[NN] Anatomical connectivity reconstruction of biological neuronal networks using Granger causality</title><link>https://www.sciencedirect.com/science/article/pii/S0893608025014091?dgcid=rss_sd_all</link><description>&lt;p&gt;Publication date: June 2026&lt;/p&gt;&lt;p&gt;&lt;b&gt;Source:&lt;/b&gt; Neural Networks, Volume 198&lt;/p&gt;&lt;p&gt;Author(s): Bo Wang, Kai Chen, Shouwei Luo, Yanyang Xiao, Songting Li, Douglas Zhou&lt;/p&gt;</description><author>ScienceDirect Publication: Neural Networks</author><pubDate>Tue, 20 Jan 2026 18:45:35 GMT</pubDate><guid isPermaLink="true">https://www.sciencedirect.com/science/article/pii/S0893608025014091</guid></item><item><title>[KBS] RMViM-Net: Residual multi-path vision mamba with graph interaction attention for medical image segmentation</title><link>https://www.sciencedirect.com/science/article/pii/S0950705126000705?dgcid=rss_sd_all</link><description>&lt;p&gt;Publication date: 15 March 2026&lt;/p&gt;&lt;p&gt;&lt;b&gt;Source:&lt;/b&gt; Knowledge-Based Systems, Volume 336&lt;/p&gt;&lt;p&gt;Author(s): Shen Jiang, Xiaoyan Kui, Xingzhuo Bao, Qinsong Li, Zhipeng Hu, Beiji Zou&lt;/p&gt;</description><author>ScienceDirect Publication: Knowledge-Based Systems</author><pubDate>Tue, 20 Jan 2026 18:45:35 GMT</pubDate><guid isPermaLink="true">https://www.sciencedirect.com/science/article/pii/S0950705126000705</guid></item><item><title>[arXiv-CV] Medical SAM3: A Foundation Model for Universal Prompt-Driven Medical Image Segmentation</title><link>https://arxiv.org/abs/2601.10880</link><description>arXiv:2601.10880v1 Announce Type: new 
Abstract: Promptable segmentation foundation models such as SAM3 have demonstrated strong generalization capabilities through interactive and concept-based prompting. However, their direct applicability to medical image segmentation remains limited by severe domain shifts, the absence of privileged spatial prompts, and the need to reason over complex anatomical and volumetric structures. Here we present Medical SAM3, a foundation model for universal prompt-driven medical image segmentation, obtained by fully fine-tuning SAM3 on large-scale, heterogeneous 2D and 3D medical imaging datasets with paired segmentation masks and text prompts. Through a systematic analysis of vanilla SAM3, we observe that its performance degrades substantially on medical data, with its apparent competitiveness largely relying on strong geometric priors such as ground-truth-derived bounding boxes. These findings motivate full model adaptation beyond prompt engineering alone. By fine-tuning SAM3's model parameters on 33 datasets spanning 10 medical imaging modalities, Medical SAM3 acquires robust domain-specific representations while preserving prompt-driven flexibility. Extensive experiments across organs, imaging modalities, and dimensionalities demonstrate consistent and significant performance gains, particularly in challenging scenarios characterized by semantic ambiguity, complex morphology, and long-range 3D context. Our results establish Medical SAM3 as a universal, text-guided segmentation foundation model for medical imaging and highlight the importance of holistic model adaptation for achieving robust prompt-driven segmentation under severe domain shift. Code and model will be made available at https://github.com/AIM-Research-Lab/Medical-SAM3.</description><author>cs.CV updates on arXiv.org</author><pubDate>Mon, 19 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2601.10880v1</guid></item><item><title>[arXiv-CV] Causal-SAM-LLM: Large Language Models as Causal Reasoners for Robust Medical Segmentation</title><link>https://arxiv.org/abs/2507.03585</link><description>arXiv:2507.03585v2 Announce Type: replace 
Abstract: The clinical utility of deep learning models for medical image segmentation is severely constrained by their inability to generalize to unseen domains. This failure is often rooted in the models learning spurious correlations between anatomical content and domain-specific imaging styles. To overcome this fundamental challenge, we introduce Causal-SAM-LLM, a novel framework that elevates Large Language Models (LLMs) to the role of causal reasoners. Our framework, built upon a frozen Segment Anything Model (SAM) encoder, incorporates two synergistic innovations. First, Linguistic Adversarial Disentanglement (LAD) employs a Vision-Language Model to generate rich, textual descriptions of confounding image styles. By training the segmentation model's features to be contrastively dissimilar to these style descriptions, it learns a representation robustly purged of non-causal information. Second, Test-Time Causal Intervention (TCI) provides an interactive mechanism where an LLM interprets a clinician's natural language command to modulate the segmentation decoder's features in real-time, enabling targeted error correction. We conduct an extensive empirical evaluation on a composite benchmark from four public datasets (BTCV, CHAOS, AMOS, BraTS), assessing generalization under cross-scanner, cross-modality, and cross-anatomy settings. Causal-SAM-LLM establishes a new state of the art in out-of-distribution (OOD) robustness, improving the average Dice score by up to 6.2 points and reducing the Hausdorff Distance by 15.8 mm over the strongest baseline, all while using less than 9% of the full model's trainable parameters. Our work charts a new course for building robust, efficient, and interactively controllable medical AI systems.</description><author>cs.CV updates on arXiv.org</author><pubDate>Mon, 19 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2507.03585v2</guid></item><item><title>[arXiv-CV] Exploring the Challenge and Value of Deep Learning in Automated Skin Disease Diagnosis</title><link>https://arxiv.org/abs/2510.03869</link><description>arXiv:2510.03869v2 Announce Type: replace 
Abstract: Skin cancer is one of the most prevalent and deadly forms of cancer worldwide, highlighting the critical importance of early detection and diagnosis in improving patient outcomes. Deep learning (DL) has shown significant promise in enhancing the accuracy and efficiency of automated skin disease diagnosis, particularly in detecting and classifying skin lesions. However, several challenges remain for DL-based skin cancer diagnosis, including complex features, image noise, intra-class variation, inter-class similarity, and data imbalance. This review synthesizes recent research and discusses innovative approaches to address these challenges, such as data augmentation, hybrid models, and feature fusion. Furthermore, the review highlights the integration of DL models into clinical workflows, offering insights into the potential of deep learning to revolutionize skin disease diagnosis and improve clinical decision-making. This review uniquely integrates a PRISMA-based methodology with a challenge-oriented taxonomy, providing a systematic and transparent synthesis of recent deep learning advances for skin disease diagnosis. It further highlights emerging directions such as hybrid CNN-Transformer architectures and uncertainty-aware models, emphasizing its contribution to future dermatological AI research.</description><author>cs.CV updates on arXiv.org</author><pubDate>Mon, 19 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2510.03869v2</guid></item><item><title>[arXiv-ML] Soft Bayesian Context Tree Models for Real-Valued Time Series</title><link>https://arxiv.org/abs/2601.11079</link><description>arXiv:2601.11079v1 Announce Type: new 
Abstract: This paper proposes the soft Bayesian context tree model (Soft-BCT), which is a novel BCT model for real-valued time series. The Soft-BCT considers soft (probabilistic) splits of the context space, instead of hard (deterministic) splits of the context space as in the previous BCT for real-valued time series. A learning algorithm of the Soft-BCT is proposed based on the variational inference. For some real-world datasets, the Soft-BCT demonstrates almost the same or superior performance to the previous BCT.</description><author>cs.LG updates on arXiv.org</author><pubDate>Mon, 19 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2601.11079v1</guid></item><item><title>[arXiv-ML] Scalable Music Cover Retrieval Using Lyrics-Aligned Audio Embeddings</title><link>https://arxiv.org/abs/2601.11262</link><description>arXiv:2601.11262v1 Announce Type: cross 
Abstract: Music Cover Retrieval, also known as Version Identification, aims to recognize distinct renditions of the same underlying musical work, a task central to catalog management, copyright enforcement, and music retrieval. State-of-the-art approaches have largely focused on harmonic and melodic features, employing increasingly complex audio pipelines designed to be invariant to musical attributes that often vary widely across covers. While effective, these methods demand substantial training time and computational resources. By contrast, lyrics constitute a strong invariant across covers, though their use has been limited by the difficulty of extracting them accurately and efficiently from polyphonic audio. Early methods relied on simple frameworks that limited downstream performance, while more recent systems deliver stronger results but require large models integrated within complex multimodal architectures. We introduce LIVI (Lyrics-Informed Version Identification), an approach that seeks to balance retrieval accuracy with computational efficiency. First, LIVI leverages supervision from state-of-the-art transcription and text embedding models during training to achieve retrieval accuracy on par with--or superior to--harmonic-based systems. Second, LIVI remains lightweight and efficient by removing the transcription step at inference, challenging the dominance of complexity-heavy pipelines.</description><author>cs.LG updates on arXiv.org</author><pubDate>Mon, 19 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2601.11262v1</guid></item><item><title>[arXiv-ML] A New Decomposition Paradigm for Graph-structured Nonlinear Programs via Message Passing</title><link>https://arxiv.org/abs/2512.24676</link><description>arXiv:2512.24676v2 Announce Type: replace-cross 
Abstract: We study finite-sum nonlinear programs with localized variable coupling encoded by a (hyper)graph. We introduce a graph-compliant decomposition framework that brings message passing into continuous optimization in a rigorous, implementable, and provable way. The (hyper)graph is partitioned into tree clusters (hypertree factor graphs). At each iteration, agents update in parallel by solving local subproblems whose objective splits into an {\it intra}-cluster term summarized by cost-to-go messages from one min-sum sweep on the cluster tree, and an {\it inter}-cluster coupling term handled Jacobi-style using the latest out-of-cluster variables. To reduce computation/communication, the method supports graph-compliant surrogates that replace exact messages/local solves with compact low-dimensional parametrizations; in hypergraphs, the same principle enables surrogate hyperedge splitting, to tame heavy hyperedge overlaps while retaining finite-time intra-cluster message updates and efficient computation/communication. We establish convergence for (strongly) convex and nonconvex objectives, with topology- and partition-explicit rates that quantify curvature/coupling effects and guide clustering and scalability. To our knowledge, this is the first convergent message-passing method on loopy graphs.</description><author>cs.LG updates on arXiv.org</author><pubDate>Mon, 19 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2512.24676v2</guid></item><item><title>[EAAI] Artificial Intelligence-based back-calculation model for scrap compiling optimization</title><link>https://www.sciencedirect.com/science/article/pii/S0952197626000904?dgcid=rss_sd_all</link><description>&lt;p&gt;Publication date: 1 March 2026&lt;/p&gt;&lt;p&gt;&lt;b&gt;Source:&lt;/b&gt; Engineering Applications of Artificial Intelligence, Volume 167, Part 2&lt;/p&gt;&lt;p&gt;Author(s): Michael Schfer, Ulrike Faltings, Bjrn Glaser&lt;/p&gt;</description><author>ScienceDirect Publication: Engineering Applications of Artificial Intelligence</author><pubDate>Fri, 16 Jan 2026 18:39:53 GMT</pubDate><guid isPermaLink="true">https://www.sciencedirect.com/science/article/pii/S0952197626000904</guid></item><item><title>[AIM] Rethinking U-Net architecture in medical imaging: Advancing the efficient and interpretable UKAN-CBAM framework for colorectal polyp segmentation</title><link>https://www.sciencedirect.com/science/article/pii/S0933365726000047?dgcid=rss_sd_all</link><description>&lt;p&gt;Publication date: Available online 15 January 2026&lt;/p&gt;&lt;p&gt;&lt;b&gt;Source:&lt;/b&gt; Artificial Intelligence in Medicine&lt;/p&gt;&lt;p&gt;Author(s): Md. Faysal Ahamed, Fariya Bintay Shafi, Md. Rabiul Islam, Md. Fahmidun Nabi, Julfikar Haider&lt;/p&gt;</description><author>ScienceDirect Publication: Artificial Intelligence in Medicine</author><pubDate>Fri, 16 Jan 2026 18:39:53 GMT</pubDate><guid isPermaLink="true">https://www.sciencedirect.com/science/article/pii/S0933365726000047</guid></item><item><title>[ESWA] AdLER: Adversarial Training with Label Error Rectification for One-Shot Medical Image Segmentation</title><link>https://www.sciencedirect.com/science/article/pii/S0957417426001387?dgcid=rss_sd_all</link><description>&lt;p&gt;Publication date: Available online 15 January 2026&lt;/p&gt;&lt;p&gt;&lt;b&gt;Source:&lt;/b&gt; Expert Systems with Applications&lt;/p&gt;&lt;p&gt;Author(s): Xiangyu Zhao, Sheng Wang, Zhiyun Song, Zhenrong Shen, Linlin Yao, Haolei Yuan, Qian Wang, Lichi Zhang&lt;/p&gt;</description><author>ScienceDirect Publication: Expert Systems with Applications</author><pubDate>Fri, 16 Jan 2026 18:39:51 GMT</pubDate><guid isPermaLink="true">https://www.sciencedirect.com/science/article/pii/S0957417426001387</guid></item><item><title>[ESWA] Reusability and Benchmarking Potential of Architectural Cultural Heritage Datasets for Generative AI: An Analytical Study</title><link>https://www.sciencedirect.com/science/article/pii/S0957417425045312?dgcid=rss_sd_all</link><description>&lt;p&gt;Publication date: Available online 16 January 2026&lt;/p&gt;&lt;p&gt;&lt;b&gt;Source:&lt;/b&gt; Expert Systems with Applications&lt;/p&gt;&lt;p&gt;Author(s): Manar Abu Talib, Iman Ibrahim, Manar Anwer Abusirdaneh&lt;/p&gt;</description><author>ScienceDirect Publication: Expert Systems with Applications</author><pubDate>Fri, 16 Jan 2026 18:39:51 GMT</pubDate><guid isPermaLink="true">https://www.sciencedirect.com/science/article/pii/S0957417425045312</guid></item><item><title>[arXiv-CV] VibrantSR: Sub-Meter Canopy Height Models from Sentinel-2 Using Generative Flow Matching</title><link>https://arxiv.org/abs/2601.09866</link><description>arXiv:2601.09866v1 Announce Type: new 
Abstract: We present VibrantSR (Vibrant Super-Resolution), a generative super-resolution framework for estimating 0.5 meter canopy height models (CHMs) from 10 meter Sentinel-2 imagery. Unlike approaches based on aerial imagery that are constrained by infrequent and irregular acquisition schedules, VibrantSR leverages globally available Sentinel-2 seasonal composites, enabling consistent monitoring at a seasonal-to-annual cadence. Evaluated across 22 EPA Level 3 eco-regions in the western United States using spatially disjoint validation splits, VibrantSR achieves a Mean Absolute Error of 4.39 meters for canopy heights &gt;= 2 m, outperforming Meta (4.83 m), LANDFIRE (5.96 m), and ETH (7.05 m) satellite-based benchmarks. While aerial-based VibrantVS (2.71 m MAE) retains an accuracy advantage, VibrantSR enables operational forest monitoring and carbon accounting at continental scales without reliance on costly and temporally infrequent aerial acquisitions.</description><author>cs.CV updates on arXiv.org</author><pubDate>Fri, 16 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2601.09866v1</guid></item><item><title>[arXiv-CV] MedVL-SAM2: A unified 3D medical vision-language model for multimodal reasoning and prompt-driven segmentation</title><link>https://arxiv.org/abs/2601.09879</link><description>arXiv:2601.09879v1 Announce Type: new 
Abstract: Recent progress in medical vision-language models (VLMs) has achieved strong performance on image-level text-centric tasks such as report generation and visual question answering (VQA). However, achieving fine-grained visual grounding and volumetric spatial reasoning in 3D medical VLMs remains challenging, particularly when aiming to unify these capabilities within a single, generalizable framework. To address this challenge, we proposed MedVL-SAM2, a unified 3D medical multimodal model that concurrently supports report generation, VQA, and multi-paradigm segmentation, including semantic, referring, and interactive segmentation. MedVL-SAM2 integrates image-level reasoning and pixel-level perception through a cohesive architecture tailored for 3D medical imaging, and incorporates a SAM2-based volumetric segmentation module to enable precise multi-granular spatial reasoning. The model is trained in a multi-stage pipeline: it is first pre-trained on a large-scale corpus of 3D CT image-text pairs to align volumetric visual features with radiology-language embeddings. It is then jointly optimized with both language-understanding and segmentation objectives using a comprehensive 3D CT segmentation dataset. This joint training enables flexible interaction via language, point, or box prompts, thereby unifying high-level visual reasoning with spatially precise localization. Our unified architecture delivers state-of-the-art performance across report generation, VQA, and multiple 3D segmentation tasks. Extensive analyses further show that the model provides reliable 3D visual grounding, controllable interactive segmentation, and robust cross-modal reasoning, demonstrating that high-level semantic reasoning and precise 3D localization can be jointly achieved within a unified 3D medical VLM.</description><author>cs.CV updates on arXiv.org</author><pubDate>Fri, 16 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2601.09879v1</guid></item><item><title>[arXiv-CV] VQ-Seg: Vector-Quantized Token Perturbation for Semi-Supervised Medical Image Segmentation</title><link>https://arxiv.org/abs/2601.10124</link><description>arXiv:2601.10124v1 Announce Type: new 
Abstract: Consistency learning with feature perturbation is a widely used strategy in semi-supervised medical image segmentation. However, many existing perturbation methods rely on dropout, and thus require a careful manual tuning of the dropout rate, which is a sensitive hyperparameter and often difficult to optimize and may lead to suboptimal regularization. To overcome this limitation, we propose VQ-Seg, the first approach to employ vector quantization (VQ) to discretize the feature space and introduce a novel and controllable Quantized Perturbation Module (QPM) that replaces dropout. Our QPM perturbs discrete representations by shuffling the spatial locations of codebook indices, enabling effective and controllable regularization. To mitigate potential information loss caused by quantization, we design a dual-branch architecture where the post-quantization feature space is shared by both image reconstruction and segmentation tasks. Moreover, we introduce a Post-VQ Feature Adapter (PFA) to incorporate guidance from a foundation model (FM), supplementing the high-level semantic information lost during quantization. Furthermore, we collect a large-scale Lung Cancer (LC) dataset comprising 828 CT scans annotated for central-type lung carcinoma. Extensive experiments on the LC dataset and other public benchmarks demonstrate the effectiveness of our method, which outperforms state-of-the-art approaches. Code available at: https://github.com/script-Yang/VQ-Seg.</description><author>cs.CV updates on arXiv.org</author><pubDate>Fri, 16 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2601.10124v1</guid></item><item><title>[arXiv-CV] BikeActions: An Open Platform and Benchmark for Cyclist-Centric VRU Action Recognition</title><link>https://arxiv.org/abs/2601.10521</link><description>arXiv:2601.10521v1 Announce Type: new 
Abstract: Anticipating the intentions of Vulnerable Road Users (VRUs) is a critical challenge for safe autonomous driving (AD) and mobile robotics. While current research predominantly focuses on pedestrian crossing behaviors from a vehicle's perspective, interactions within dense shared spaces remain underexplored. To bridge this gap, we introduce FUSE-Bike, the first fully open perception platform of its kind. Equipped with two LiDARs, a camera, and GNSS, it facilitates high-fidelity, close-range data capture directly from a cyclist's viewpoint. Leveraging this platform, we present BikeActions, a novel multi-modal dataset comprising 852 annotated samples across 5 distinct action classes, specifically tailored to improve VRU behavior modeling. We establish a rigorous benchmark by evaluating state-of-the-art graph convolution and transformer-based models on our publicly released data splits, establishing the first performance baselines for this challenging task. We release the full dataset together with data curation tools, the open hardware design, and the benchmark code to foster future research in VRU action understanding under https://iv.ee.hm.edu/bikeactions/.</description><author>cs.CV updates on arXiv.org</author><pubDate>Fri, 16 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2601.10521v1</guid></item><item><title>[arXiv-CV] Jordan-Segmentable Masks: A Topology-Aware definition for characterizing Binary Image Segmentation</title><link>https://arxiv.org/abs/2601.10577</link><description>arXiv:2601.10577v1 Announce Type: new 
Abstract: Image segmentation plays a central role in computer vision. However, widely used evaluation metrics, whether pixel-wise, region-based, or boundary-focused, often struggle to capture the structural and topological coherence of a segmentation. In many practical scenarios, such as medical imaging or object delineation, small inaccuracies in boundary, holes, or fragmented predictions can result in high metric scores, despite the fact that the resulting masks fail to preserve the object global shape or connectivity. This highlights a limitation of conventional metrics: they are unable to assess whether a predicted segmentation partitions the image into meaningful interior and exterior regions.
  In this work, we introduce a topology-aware notion of segmentation based on the Jordan Curve Theorem, and adapted for use in digital planes. We define the concept of a \emph{Jordan-segmentatable mask}, which is a binary segmentation whose structure ensures a topological separation of the image domain into two connected components. We analyze segmentation masks through the lens of digital topology and homology theory, extracting a $4$-curve candidate from the mask, verifying its topological validity using Betti numbers. A mask is considered Jordan-segmentatable when this candidate forms a digital 4-curve with $\beta_0 = \beta_1 = 1$, or equivalently when its complement splits into exactly two $8$-connected components.
  This framework provides a mathematically rigorous, unsupervised criterion with which to assess the structural coherence of segmentation masks. By combining digital Jordan theory and homological invariants, our approach provides a valuable alternative to standard evaluation metrics, especially in applications where topological correctness must be preserved.</description><author>cs.CV updates on arXiv.org</author><pubDate>Fri, 16 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2601.10577v1</guid></item><item><title>[arXiv-CV] MHub.ai: A Simple, Standardized, and Reproducible Platform for AI Models in Medical Imaging</title><link>https://arxiv.org/abs/2601.10154</link><description>arXiv:2601.10154v1 Announce Type: cross 
Abstract: Artificial intelligence (AI) has the potential to transform medical imaging by automating image analysis and accelerating clinical research. However, research and clinical use are limited by the wide variety of AI implementations and architectures, inconsistent documentation, and reproducibility issues. Here, we introduce MHub.ai, an open-source, container-based platform that standardizes access to AI models with minimal configuration, promoting accessibility and reproducibility in medical imaging. MHub.ai packages models from peer-reviewed publications into standardized containers that support direct processing of DICOM and other formats, provide a unified application interface, and embed structured metadata. Each model is accompanied by publicly available reference data that can be used to confirm model operation. MHub.ai includes an initial set of state-of-the-art segmentation, prediction, and feature extraction models for different modalities. The modular framework enables adaptation of any model and supports community contributions. We demonstrate the utility of the platform in a clinical use case through comparative evaluation of lung segmentation models. To further strengthen transparency and reproducibility, we publicly release the generated segmentations and evaluation metrics and provide interactive dashboards that allow readers to inspect individual cases and reproduce or extend our analysis. By simplifying model use, MHub.ai enables side-by-side benchmarking with identical execution commands and standardized outputs, and lowers the barrier to clinical translation.</description><author>cs.CV updates on arXiv.org</author><pubDate>Fri, 16 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2601.10154v1</guid></item><item><title>[arXiv-CV] Graph Algorithm Unrolling with Douglas-Rachford Iterations for Image Interpolation with Guaranteed Initialization</title><link>https://arxiv.org/abs/2509.11926</link><description>arXiv:2509.11926v3 Announce Type: replace 
Abstract: Conventional deep neural nets (DNNs) initialize network parameters at random and then optimize each one via stochastic gradient descent (SGD), resulting in substantial risk of poor-performing local minima.Focusing on the image interpolation problem and leveraging a recent theorem that maps a (pseudo-)linear interpolator {\Theta} to a directed graph filter that is a solution to a MAP problem regularized with a graph shift variation (GSV) prior, we first initialize a directed graph adjacency matrix A based on a known interpolator {\Theta}, establishing a baseline performance.Then, towards further gain, we learn perturbation matrices P and P(2) from data to augment A, whose restoration effects are implemented via Douglas-Rachford (DR) iterations, which we unroll into a lightweight interpretable neural net.Experimental results demonstrate state-of-the-art image interpolation results, while drastically reducing network parameters.</description><author>cs.CV updates on arXiv.org</author><pubDate>Fri, 16 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2509.11926v3</guid></item><item><title>[arXiv-CV] Mamba Goes HoME: Hierarchical Soft Mixture-of-Experts for 3D Medical Image Segmentation</title><link>https://arxiv.org/abs/2507.06363</link><description>arXiv:2507.06363v3 Announce Type: replace-cross 
Abstract: In recent years, artificial intelligence has significantly advanced medical image segmentation. Nonetheless, challenges remain, including efficient 3D medical image processing across diverse modalities and handling data variability. In this work, we introduce Hierarchical Soft Mixture-of-Experts (HoME), a two-level token-routing layer for efficient long-context modeling, specifically designed for 3D medical image segmentation. Built on the Mamba Selective State Space Model (SSM) backbone, HoME enhances sequential modeling through adaptive expert routing. In the first level, a Soft Mixture-of-Experts (SMoE) layer partitions input sequences into local groups, routing tokens to specialized per-group experts for localized feature extraction. The second level aggregates these outputs through a global SMoE layer, enabling cross-group information fusion and global context refinement. This hierarchical design, combining local expert routing with global expert refinement, enhances generalizability and segmentation performance, surpassing state-of-the-art results across datasets from the three most widely used 3D medical imaging modalities and varying data qualities. The code is publicly available at https://github.com/gmum/MambaHoME.</description><author>cs.CV updates on arXiv.org</author><pubDate>Fri, 16 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2507.06363v3</guid></item><item><title>[arXiv-ML] Time Aggregation Features for XGBoost Models</title><link>https://arxiv.org/abs/2601.10019</link><description>arXiv:2601.10019v1 Announce Type: new 
Abstract: This paper studies time aggregation features for XGBoost models in click-through rate prediction. The setting is the Avazu click-through rate prediction dataset with strict out-of-time splits and a no-lookahead feature constraint. Features for hour H use only impressions from hours strictly before H. This paper compares a strong time-aware target encoding baseline to models augmented with entity history time aggregation under several window designs. Across two rolling-tail folds on a deterministic ten percent sample, a trailing window specification improves ROC AUC by about 0.0066 to 0.0082 and PR AUC by about 0.0084 to 0.0094 relative to target encoding alone. Within the time aggregation design grid, event count windows provide the only consistent improvement over trailing windows, and the gain is small. Gap windows and bucketized windows underperform simple trailing windows in this dataset and protocol. These results support a practical default of trailing windows, with an optional event count window when marginal ROC AUC gains matter.</description><author>cs.LG updates on arXiv.org</author><pubDate>Fri, 16 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2601.10019v1</guid></item><item><title>[arXiv-ML] CC-OR-Net: A Unified Framework for LTV Prediction through Structural Decoupling</title><link>https://arxiv.org/abs/2601.10176</link><description>arXiv:2601.10176v1 Announce Type: new 
Abstract: Customer Lifetime Value (LTV) prediction, a central problem in modern marketing, is characterized by a unique zero-inflated and long-tail data distribution. This distribution presents two fundamental challenges: (1) the vast majority of low-to-medium value users numerically overwhelm the small but critically important segment of high-value "whale" users, and (2) significant value heterogeneity exists even within the low-to-medium value user base. Common approaches either rely on rigid statistical assumptions or attempt to decouple ranking and regression using ordered buckets; however, they often enforce ordinality through loss-based constraints rather than inherent architectural design, failing to balance global accuracy with high-value precision. To address this gap, we propose \textbf{C}onditional \textbf{C}ascaded \textbf{O}rdinal-\textbf{R}esidual Networks \textbf{(CC-OR-Net)}, a novel unified framework that achieves a more robust decoupling through \textbf{structural decomposition}, where ranking is architecturally guaranteed. CC-OR-Net integrates three specialized components: a \textit{structural ordinal decomposition module} for robust ranking, an \textit{intra-bucket residual module} for fine-grained regression, and a \textit{targeted high-value augmentation module} for precision on top-tier users. Evaluated on real-world datasets with over 300M users, CC-OR-Net achieves a superior trade-off across all key business metrics, outperforming state-of-the-art methods in creating a holistic and commercially valuable LTV prediction solution.</description><author>cs.LG updates on arXiv.org</author><pubDate>Fri, 16 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2601.10176v1</guid></item><item><title>[arXiv-ML] LangLasso: Interactive Cluster Descriptions through LLM Explanation</title><link>https://arxiv.org/abs/2601.10458</link><description>arXiv:2601.10458v1 Announce Type: cross 
Abstract: Dimensionality reduction is a powerful technique for revealing structure and potential clusters in data. However, as the axes are complex, non-linear combinations of features, they often lack semantic interpretability. Existing visual analytics (VA) methods support cluster interpretation through feature comparison and interactive exploration, but they require technical expertise and intense human effort. We present \textit{LangLasso}, a novel method that complements VA approaches through interactive, natural language descriptions of clusters using large language models (LLMs). It produces human-readable descriptions that make cluster interpretation accessible to non-experts and allow integration of external contextual knowledge beyond the dataset. We systematically evaluate the reliability of these explanations and demonstrate that \langlasso provides an effective first step for engaging broader audiences in cluster interpretation. The tool is available at https://langlasso.vercel.app</description><author>cs.LG updates on arXiv.org</author><pubDate>Fri, 16 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2601.10458v1</guid></item><item><title>[arXiv-ML] Autoencoding Random Forests</title><link>https://arxiv.org/abs/2505.21441</link><description>arXiv:2505.21441v4 Announce Type: replace-cross 
Abstract: We propose a principled method for autoencoding with random forests. Our strategy builds on foundational results from nonparametric statistics and spectral graph theory to learn a low-dimensional embedding of the model that optimally represents relationships in the data. We provide exact and approximate solutions to the decoding problem via constrained optimization, split relabeling, and nearest neighbors regression. These methods effectively invert the compression pipeline, establishing a map from the embedding space back to the input space using splits learned by the ensemble's constituent trees. The resulting decoders are universally consistent under common regularity assumptions. The procedure works with supervised or unsupervised models, providing a window into conditional or joint distributions. We demonstrate various applications of this autoencoder, including powerful new tools for visualization, compression, clustering, and denoising. Experiments illustrate the ease and utility of our method in a wide range of settings, including tabular, image, and genomic data.</description><author>cs.LG updates on arXiv.org</author><pubDate>Fri, 16 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2505.21441v4</guid></item><item><title>[NN] ReTri: Progressive domain bridging via representation disentanglement and triple-level consistency-driven feature alignment for unsupervised domain adaptive medical image segmentation</title><link>https://www.sciencedirect.com/science/article/pii/S0893608026000274?dgcid=rss_sd_all</link><description>&lt;p&gt;Publication date: June 2026&lt;/p&gt;&lt;p&gt;&lt;b&gt;Source:&lt;/b&gt; Neural Networks, Volume 198&lt;/p&gt;&lt;p&gt;Author(s): Xiaoru Gao, Guoyan Zheng&lt;/p&gt;</description><author>ScienceDirect Publication: Neural Networks</author><pubDate>Thu, 15 Jan 2026 18:43:14 GMT</pubDate><guid isPermaLink="true">https://www.sciencedirect.com/science/article/pii/S0893608026000274</guid></item><item><title>[BSPC] MFBRU-Net: Multi-scale feature fusion and boundary refining U-Net for medical image segmentation</title><link>https://www.sciencedirect.com/science/article/pii/S1746809426001473?dgcid=rss_sd_all</link><description>&lt;p&gt;Publication date: 15 May 2026&lt;/p&gt;&lt;p&gt;&lt;b&gt;Source:&lt;/b&gt; Biomedical Signal Processing and Control, Volume 117&lt;/p&gt;&lt;p&gt;Author(s): Han Gao, Xianran Zhang, Zhengpeng Li, Ziteng Wang, Ping Sun, Jiansheng Wu&lt;/p&gt;</description><author>ScienceDirect Publication: Biomedical Signal Processing and Control</author><pubDate>Thu, 15 Jan 2026 12:56:08 GMT</pubDate><guid isPermaLink="true">https://www.sciencedirect.com/science/article/pii/S1746809426001473</guid></item><item><title>[BSPC] Deep feature-based approaches for brain tumor classification and segmentation in medical imaging</title><link>https://www.sciencedirect.com/science/article/pii/S1746809426001576?dgcid=rss_sd_all</link><description>&lt;p&gt;Publication date: 15 May 2026&lt;/p&gt;&lt;p&gt;&lt;b&gt;Source:&lt;/b&gt; Biomedical Signal Processing and Control, Volume 117&lt;/p&gt;&lt;p&gt;Author(s): Agnesh Chandra Yadav, Maheshkumar H. Kolekar&lt;/p&gt;</description><author>ScienceDirect Publication: Biomedical Signal Processing and Control</author><pubDate>Thu, 15 Jan 2026 12:56:08 GMT</pubDate><guid isPermaLink="true">https://www.sciencedirect.com/science/article/pii/S1746809426001576</guid></item><item><title>[PR] Enhancing semi-supervised medical image segmentation via semantic transfer</title><link>https://www.sciencedirect.com/science/article/pii/S0031320326000026?dgcid=rss_sd_all</link><description>&lt;p&gt;Publication date: July 2026&lt;/p&gt;&lt;p&gt;&lt;b&gt;Source:&lt;/b&gt; Pattern Recognition, Volume 175&lt;/p&gt;&lt;p&gt;Author(s): Shiyuan Huang, Shudong Wang, Kuijie Zhang, Wenhao Wu, Yingye Liu, Tiyao Liu, Shanchen Pang&lt;/p&gt;</description><author>ScienceDirect Publication: Pattern Recognition</author><pubDate>Thu, 15 Jan 2026 12:56:06 GMT</pubDate><guid isPermaLink="true">https://www.sciencedirect.com/science/article/pii/S0031320326000026</guid></item><item><title>[arXiv-CV] Variance-Penalized MC-Dropout as a Learned Smoothing Prior for Brain Tumour Segmentation</title><link>https://arxiv.org/abs/2601.08956</link><description>arXiv:2601.08956v1 Announce Type: new 
Abstract: Brain tumor segmentation is essential for diagnosis and treatment planning, yet many CNN and U-Net based approaches produce noisy boundaries in regions of tumor infiltration. We introduce UAMSA-UNet, an Uncertainty-Aware Multi-Scale Attention-based Bayesian U-Net that in- stead leverages Monte Carlo Dropout to learn a data-driven smoothing prior over its predictions, while fusing multi-scale features and attention maps to capture both fine details and global context. Our smoothing-regularized loss augments binary cross-entropy with a variance penalty across stochas- tic forward passes, discouraging spurious fluctuations and yielding spatially coherent masks. On BraTS2023, UAMSA- UNet improves Dice Similarity Coefficient by up to 3.3% and mean IoU by up to 2.7% over U-Net; on BraTS2024, it delivers up to 4.5% Dice and 4.0% IoU gains over the best baseline. Remarkably, it also reduces FLOPs by 42.5% rel- ative to U-Net++ while maintaining higher accuracy. These results demonstrate that, by combining multi-scale attention with a learned smoothing prior, UAMSA-UNet achieves both better segmentation quality and computational efficiency, and provides a flexible foundation for future integration with transformer-based modules for further enhanced segmenta- tion results.</description><author>cs.CV updates on arXiv.org</author><pubDate>Thu, 15 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2601.08956v1</guid></item><item><title>[arXiv-CV] From Performance to Practice: Knowledge-Distilled Segmentator for On-Premises Clinical Workflows</title><link>https://arxiv.org/abs/2601.09191</link><description>arXiv:2601.09191v1 Announce Type: new 
Abstract: Deploying medical image segmentation models in routine clinical workflows is often constrained by on-premises infrastructure, where computational resources are fixed and cloud-based inference may be restricted by governance and security policies. While high-capacity models achieve strong segmentation accuracy, their computational demands hinder practical deployment and long-term maintainability in hospital environments. We present a deployment-oriented framework that leverages knowledge distillation to translate a high-performing segmentation model into a scalable family of compact student models, without modifying the inference pipeline. The proposed approach preserves architectural compatibility with existing clinical systems while enabling systematic capacity reduction. The framework is evaluated on a multi-site brain MRI dataset comprising 1,104 3D volumes, with independent testing on 101 curated cases, and is further examined on abdominal CT to assess cross-modality generalizability. Under aggressive parameter reduction (94%), the distilled student model preserves nearly all of the teacher's segmentation accuracy (98.7%), while achieving substantial efficiency gains, including up to a 67% reduction in CPU inference latency without additional deployment overhead. These results demonstrate that knowledge distillation provides a practical and reliable pathway for converting research-grade segmentation models into maintainable, deployment-ready components for on-premises clinical workflows in real-world health systems.</description><author>cs.CV updates on arXiv.org</author><pubDate>Thu, 15 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2601.09191v1</guid></item><item><title>[arXiv-CV] POWDR: Pathology-preserving Outpainting with Wavelet Diffusion for 3D MRI</title><link>https://arxiv.org/abs/2601.09044</link><description>arXiv:2601.09044v1 Announce Type: cross 
Abstract: Medical imaging datasets often suffer from class imbalance and limited availability of pathology-rich cases, which constrains the performance of machine learning models for segmentation, classification, and vision-language tasks. To address this challenge, we propose POWDR, a pathology-preserving outpainting framework for 3D MRI based on a conditioned wavelet diffusion model. Unlike conventional augmentation or unconditional synthesis, POWDR retains real pathological regions while generating anatomically plausible surrounding tissue, enabling diversity without fabricating lesions.
  Our approach leverages wavelet-domain conditioning to enhance high-frequency detail and mitigate blurring common in latent diffusion models. We introduce a random connected mask training strategy to overcome conditioning-induced collapse and improve diversity outside the lesion. POWDR is evaluated on brain MRI using BraTS datasets and extended to knee MRI to demonstrate tissue-agnostic applicability. Quantitative metrics (FID, SSIM, LPIPS) confirm image realism, while diversity analysis shows significant improvement with random-mask training (cosine similarity reduced from 0.9947 to 0.9580; KL divergence increased from 0.00026 to 0.01494). Clinically relevant assessments reveal gains in tumor segmentation performance using nnU-Net, with Dice scores improving from 0.6992 to 0.7137 when adding 50 synthetic cases. Tissue volume analysis indicates no significant differences for CSF and GM compared to real images.
  These findings highlight POWDR as a practical solution for addressing data scarcity and class imbalance in medical imaging. The method is extensible to multiple anatomies and offers a controllable framework for generating diverse, pathology-preserving synthetic data to support robust model development.</description><author>cs.CV updates on arXiv.org</author><pubDate>Thu, 15 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2601.09044v1</guid></item><item><title>[arXiv-CV] Comprehensive language-image pre-training for 3D medical image understanding</title><link>https://arxiv.org/abs/2510.15042</link><description>arXiv:2510.15042v2 Announce Type: replace 
Abstract: Vision-language pre-training, i.e., aligning images with paired text, is a powerful paradigm to create encoders that can be directly used for tasks such as classification, retrieval, and segmentation. In the 3D medical image domain, these capabilities allow vision-language encoders (VLEs) to support radiologists by retrieving patients with similar abnormalities, predicting likelihoods of abnormality, or, with downstream adaptation, generating radiological reports. While the methodology holds promise, data availability and domain-specific hurdles limit the capabilities of current 3D VLEs.
  In this paper, we overcome these challenges by injecting additional supervision via a report generation objective and combining vision-language with vision-only pre-training. This allows us to leverage both image-only and paired image-text 3D datasets, increasing the total amount of data to which our model is exposed. Through these additional objectives, paired with best practices of the 3D medical imaging domain, we develop the Comprehensive Language-Image Pre-training (COLIPRI) encoder family. Our COLIPRI encoders achieve state-of-the-art performance in report generation, semantic segmentation, classification probing, and zero-shot classification. The model is available at https://huggingface.co/microsoft/colipri.</description><author>cs.CV updates on arXiv.org</author><pubDate>Thu, 15 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2510.15042v2</guid></item><item><title>[arXiv-CV] BARL: Bilateral Alignment in Representation and Label Spaces for Semi-Supervised Volumetric Medical Image Segmentation</title><link>https://arxiv.org/abs/2510.16863</link><description>arXiv:2510.16863v2 Announce Type: replace 
Abstract: Semi-supervised medical image segmentation (SSMIS) seeks to match fully supervised performance while sharply reducing annotation cost. Mainstream SSMIS methods rely on \emph{label-space consistency}, yet they overlook the equally critical \emph{representation-space alignment}. Without harmonizing latent features, models struggle to learn representations that are both discriminative and spatially coherent. To this end, we introduce \textbf{Bilateral Alignment in Representation and Label spaces (BARL)}, a unified framework that couples two collaborative branches and enforces alignment in both spaces. For label-space alignment, inspired by co-training and multi-scale decoding, we devise \textbf{Dual-Path Regularization (DPR)} and \textbf{Progressively Cognitive Bias Correction (PCBC)} to impose fine-grained cross-branch consistency while mitigating error accumulation from coarse to fine scales. For representation-space alignment, we conduct region-level and lesion-instance matching between branches, explicitly capturing the fragmented, complex pathological patterns common in medical imagery. Extensive experiments on four public benchmarks and a proprietary CBCT dataset demonstrate that BARL consistently surpasses state-of-the-art SSMIS methods. Ablative studies further validate the contribution of each component. Code will be released soon.</description><author>cs.CV updates on arXiv.org</author><pubDate>Thu, 15 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2510.16863v2</guid></item><item><title>[arXiv-CV] egoEMOTION: Egocentric Vision and Physiological Signals for Emotion and Personality Recognition in Real-World Tasks</title><link>https://arxiv.org/abs/2510.22129</link><description>arXiv:2510.22129v2 Announce Type: replace 
Abstract: Understanding affect is central to anticipating human behavior, yet current egocentric vision benchmarks largely ignore the person's emotional states that shape their decisions and actions. Existing tasks in egocentric perception focus on physical activities, hand-object interactions, and attention modeling - assuming neutral affect and uniform personality. This limits the ability of vision systems to capture key internal drivers of behavior. In this paper, we present egoEMOTION, the first dataset that couples egocentric visual and physiological signals with dense self-reports of emotion and personality across controlled and real-world scenarios. Our dataset includes over 50 hours of recordings from 43 participants, captured using Meta's Project Aria glasses. Each session provides synchronized eye-tracking video, headmounted photoplethysmography, inertial motion data, and physiological baselines for reference. Participants completed emotion-elicitation tasks and naturalistic activities while self-reporting their affective state using the Circumplex Model and Mikels' Wheel as well as their personality via the Big Five model. We define three benchmark tasks: (1) continuous affect classification (valence, arousal, dominance); (2) discrete emotion classification; and (3) trait-level personality inference. We show that a classical learning-based method, as a simple baseline in real-world affect prediction, produces better estimates from signals captured on egocentric vision systems than processing physiological signals. Our dataset establishes emotion and personality as core dimensions in egocentric perception and opens new directions in affect-driven modeling of behavior, intent, and interaction.</description><author>cs.CV updates on arXiv.org</author><pubDate>Thu, 15 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2510.22129v2</guid></item><item><title>[arXiv-ML] LLM for Large-Scale Optimization Model Auto-Formulation: A Lightweight Few-Shot Learning Approach</title><link>https://arxiv.org/abs/2601.09635</link><description>arXiv:2601.09635v1 Announce Type: cross 
Abstract: Large-scale optimization is a key backbone of modern business decision-making. However, building these models is often labor-intensive and time-consuming. We address this by proposing LEAN-LLM-OPT, a LightwEight AgeNtic workflow construction framework for LLM-assisted large-scale OPTimization auto-formulation. LEAN-LLM-OPT takes as input a problem description together with associated datasets and orchestrates a team of LLM agents to produce an optimization formulation. Specifically, upon receiving a query, two upstream LLM agents dynamically construct a workflow that specifies, step-by-step, how optimization models for similar problems can be formulated. A downstream LLM agent then follows this workflow to generate the final output. Leveraging LLMs' text-processing capabilities and common modeling practices, the workflow decomposes the modeling task into a sequence of structured sub-tasks and offloads mechanical data-handling operations to auxiliary tools. This design alleviates the downstream agent's burden related to planning and data handling, allowing it to focus on the most challenging components that cannot be readily standardized. Extensive simulations show that LEAN-LLM-OPT, instantiated with GPT-4.1 and the open source gpt-oss-20B, achieves strong performance on large-scale optimization modeling tasks and is competitive with state-of-the-art approaches. In addition, in a Singapore Airlines choice-based revenue management use case, LEAN-LLM-OPT demonstrates practical value by achieving leading performance across a range of scenarios. Along the way, we introduce Large-Scale-OR and Air-NRM, the first comprehensive benchmarks for large-scale optimization auto-formulation. The code and data of this work is available at https://github.com/CoraLiang01/lean-llm-opt.</description><author>cs.LG updates on arXiv.org</author><pubDate>Thu, 15 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2601.09635v1</guid></item><item><title>[arXiv-ML] e-Profits: A Business-Aligned Evaluation Metric for Profit-Sensitive Customer Churn Prediction</title><link>https://arxiv.org/abs/2507.08860</link><description>arXiv:2507.08860v2 Announce Type: replace 
Abstract: Retention campaigns in customer relationship management often rely on churn prediction models evaluated using traditional metrics such as AUC and F1-score. However, these metrics fail to reflect financial outcomes and may mislead strategic decisions. We introduce e-Profits, a novel business-aligned evaluation metric that quantifies model performance based on customer lifetime value, retention probability, and intervention costs. Unlike existing profit-based metrics such as Expected Maximum Profit, which assume fixed population-level parameters, e-Profits uses Kaplan-Meier survival analysis to estimate tenure-conditioned (customer-level) one-period retention probabilities and supports granular, per-customer profit evaluation. We benchmark six classifiers across two telecom datasets (IBM Telco and Maven Telecom) and demonstrate that e-Profits reshapes model rankings compared to traditional metrics, revealing financial advantages in models previously overlooked by AUC or F1-score. The metric also enables segment-level insight into which models maximise return on investment for high-value customers. e-Profits provides a transparent, customer-level evaluation framework that bridges predictive modelling and profit-driven decision-making in operational churn management. All source code is available at: https://github.com/Awaismanzoor/eprofits.</description><author>cs.LG updates on arXiv.org</author><pubDate>Thu, 15 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2507.08860v2</guid></item><item><title>[arXiv-ML] Horizon Activation Mapping for Neural Networks in Time Series Forecasting</title><link>https://arxiv.org/abs/2601.02094</link><description>arXiv:2601.02094v2 Announce Type: replace 
Abstract: Neural networks for time series forecasting have relied on error metrics and architecture-specific interpretability approaches for model selection that don't apply across models of different families. To interpret forecasting models agnostic to the types of layers across state-of-the-art model families, we introduce Horizon Activation Mapping (HAM), a visual interpretability technique inspired by grad-CAM that uses gradient norm averages to study the horizon's subseries where grad-CAM studies attention maps over image data. We introduce causal and anti-causal modes to calculate gradient update norm averages across subseries at every timestep and lines of proportionality signifying uniform distributions of the norm averages. Optimization landscape studies with respect to changes in batch sizes, early stopping, train-val-test splits, architectural choices, univariate forecasting and dropouts are studied with respect to performances and subseries in HAM. Interestingly, batch size based differences in activities seem to indicate potential for existence of an exponential approximation across them per epoch relative to each other. Multivariate forecasting models including MLP-based CycleNet, N-Linear, N-HITS, self attention-based FEDformer, Pyraformer, SSM-based SpaceTime and diffusion-based Multi-Resolution DDPM over different horizon sizes trained over the ETTm2 dataset are used for HAM plots in this study. NHITS' neural approximation theorem and SpaceTime's exponential autoregressive activities have been attributed to trends in HAM plots over their training, validation and test sets. In general, HAM can be used for granular model selection, validation set choices and comparisons across different neural network model families.</description><author>cs.LG updates on arXiv.org</author><pubDate>Thu, 15 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2601.02094v2</guid></item><item><title>[arXiv-ML] QuFeX: Quantum feature extraction module for hybrid quantum-classical deep neural networks</title><link>https://arxiv.org/abs/2501.13165</link><description>arXiv:2501.13165v2 Announce Type: replace-cross 
Abstract: We introduce Quantum Feature Extraction (QuFeX), a novel quantum machine learning module. The proposed module enables feature extraction in a reduced-dimensional space, significantly decreasing the number of parallel evaluations required in typical quantum convolutional neural network architectures. Its design allows seamless integration into deep classical neural networks, making it particularly suitable for hybrid quantum-classical models. As an application of QuFeX, we propose Qu-Net -- a hybrid architecture which integrates QuFeX at the bottleneck of a U-Net architecture. The latter is widely used for image segmentation tasks such as medical imaging and autonomous driving. Our numerical analysis indicates that the Qu-Net can achieve superior segmentation performance compared to a U-Net baseline. These results highlight the potential of QuFeX to enhance deep neural networks by leveraging hybrid computational paradigms, providing a path towards a robust framework for real-world applications requiring precise feature extraction.</description><author>cs.LG updates on arXiv.org</author><pubDate>Thu, 15 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2501.13165v2</guid></item><item><title>[arXiv-ML] Learning About Learning: A Physics Path from Spin Glasses to Artificial Intelligence</title><link>https://arxiv.org/abs/2601.07635</link><description>arXiv:2601.07635v2 Announce Type: replace-cross 
Abstract: The Hopfield model, originally inspired by spin-glass physics, occupies a central place at the intersection of statistical mechanics, neural networks, and modern artificial intelligence. Despite its conceptual simplicity and broad applicability -- from associative memory to near-optimal solutions of combinatorial optimization problems -- it is rarely integrated into standard undergraduate physics curricula. In this paper, we present the Hopfield model as a pedagogically rich framework that naturally unifies core topics from undergraduate statistical physics, dynamical systems, linear algebra, and computational methods. We provide a concise and illustrated theoretical introduction grounded in familiar physics concepts, analyze the model's energy function, dynamics, and pattern stability, and discuss practical aspects of simulation, including a freely available simulation code. To support instruction, we conclude with classroom-ready example problems designed to mirror research practice. By explicitly connecting fundamental physics to contemporary AI applications, this work aims to help prepare physics students to understand, apply, and critically engage with the computational tools increasingly central to research, industry, and society.</description><author>cs.LG updates on arXiv.org</author><pubDate>Thu, 15 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2601.07635v2</guid></item><item><title>[BSPC] Pixel-level polyp segmentation network based on parallel feature enhancement and attention mechanism</title><link>https://www.sciencedirect.com/science/article/pii/S1746809426001023?dgcid=rss_sd_all</link><description>&lt;p&gt;Publication date: 1 May 2026&lt;/p&gt;&lt;p&gt;&lt;b&gt;Source:&lt;/b&gt; Biomedical Signal Processing and Control, Volume 116&lt;/p&gt;&lt;p&gt;Author(s): Haiping Xu, Jie Wang, Zuoyong Li, Xuesong Cheng, Shenghua Teng&lt;/p&gt;</description><author>ScienceDirect Publication: Biomedical Signal Processing and Control</author><pubDate>Wed, 14 Jan 2026 12:57:29 GMT</pubDate><guid isPermaLink="true">https://www.sciencedirect.com/science/article/pii/S1746809426001023</guid></item><item><title>[NN] AttCo: Attention-based co-Learning fusion of deep feature representation for medical image segmentation using multimodality</title><link>https://www.sciencedirect.com/science/article/pii/S0893608026000353?dgcid=rss_sd_all</link><description>&lt;p&gt;Publication date: June 2026&lt;/p&gt;&lt;p&gt;&lt;b&gt;Source:&lt;/b&gt; Neural Networks, Volume 198&lt;/p&gt;&lt;p&gt;Author(s): Duy-Phuong Dao, Hyung-Jeong Yang, Soo-Hyung Kim, Sae-Ryung Kang&lt;/p&gt;</description><author>ScienceDirect Publication: Neural Networks</author><pubDate>Wed, 14 Jan 2026 12:57:28 GMT</pubDate><guid isPermaLink="true">https://www.sciencedirect.com/science/article/pii/S0893608026000353</guid></item><item><title>[ESWA] Transferring cross-Dimensional knowledge via proxy task for medical image segmentation</title><link>https://www.sciencedirect.com/science/article/pii/S0957417426000382?dgcid=rss_sd_all</link><description>&lt;p&gt;Publication date: 1 May 2026&lt;/p&gt;&lt;p&gt;&lt;b&gt;Source:&lt;/b&gt; Expert Systems with Applications, Volume 308&lt;/p&gt;&lt;p&gt;Author(s): Cunhan Guo, Heyan Huang, Yang-Hao Zhou, Danjie Han, Changsen Yuan&lt;/p&gt;</description><author>ScienceDirect Publication: Expert Systems with Applications</author><pubDate>Wed, 14 Jan 2026 12:57:27 GMT</pubDate><guid isPermaLink="true">https://www.sciencedirect.com/science/article/pii/S0957417426000382</guid></item><item><title>[arXiv-CV] Rescind: Countering Image Misconduct in Biomedical Publications with Vision-Language and State-Space Modeling</title><link>https://arxiv.org/abs/2601.08040</link><description>arXiv:2601.08040v1 Announce Type: new 
Abstract: Scientific image manipulation in biomedical publications poses a growing threat to research integrity and reproducibility. Unlike natural image forensics, biomedical forgery detection is uniquely challenging due to domain-specific artifacts, complex textures, and unstructured figure layouts. We present the first vision-language guided framework for both generating and detecting biomedical image forgeries. By combining diffusion-based synthesis with vision-language prompting, our method enables realistic and semantically controlled manipulations, including duplication, splicing, and region removal, across diverse biomedical modalities. We introduce Rescind, a large-scale benchmark featuring fine-grained annotations and modality-specific splits, and propose Integscan, a structured state space modeling framework that integrates attention-enhanced visual encoding with prompt-conditioned semantic alignment for precise forgery localization. To ensure semantic fidelity, we incorporate a vision-language model based verification loop that filters generated forgeries based on consistency with intended prompts. Extensive experiments on Rescind and existing benchmarks demonstrate that Integscan achieves state of the art performance in both detection and localization, establishing a strong foundation for automated scientific integrity analysis.</description><author>cs.CV updates on arXiv.org</author><pubDate>Wed, 14 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2601.08040v1</guid></item><item><title>[arXiv-CV] Exploiting DINOv3-Based Self-Supervised Features for Robust Few-Shot Medical Image Segmentation</title><link>https://arxiv.org/abs/2601.08078</link><description>arXiv:2601.08078v1 Announce Type: new 
Abstract: Deep learning-based automatic medical image segmentation plays a critical role in clinical diagnosis and treatment planning but remains challenging in few-shot scenarios due to the scarcity of annotated training data. Recently, self-supervised foundation models such as DINOv3, which were trained on large natural image datasets, have shown strong potential for dense feature extraction that can help with the few-shot learning challenge. Yet, their direct application to medical images is hindered by domain differences. In this work, we propose DINO-AugSeg, a novel framework that leverages DINOv3 features to address the few-shot medical image segmentation challenge. Specifically, we introduce WT-Aug, a wavelet-based feature-level augmentation module that enriches the diversity of DINOv3-extracted features by perturbing frequency components, and CG-Fuse, a contextual information-guided fusion module that exploits cross-attention to integrate semantic-rich low-resolution features with spatially detailed high-resolution features. Extensive experiments on six public benchmarks spanning five imaging modalities, including MRI, CT, ultrasound, endoscopy, and dermoscopy, demonstrate that DINO-AugSeg consistently outperforms existing methods under limited-sample conditions. The results highlight the effectiveness of incorporating wavelet-domain augmentation and contextual fusion for robust feature representation, suggesting DINO-AugSeg as a promising direction for advancing few-shot medical image segmentation. Code and data will be made available on https://github.com/apple1986/DINO-AugSeg.</description><author>cs.CV updates on arXiv.org</author><pubDate>Wed, 14 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2601.08078v1</guid></item><item><title>[arXiv-CV] PathoGen: Diffusion-Based Synthesis of Realistic Lesions in Histopathology Images</title><link>https://arxiv.org/abs/2601.08127</link><description>arXiv:2601.08127v1 Announce Type: new 
Abstract: The development of robust artificial intelligence models for histopathology diagnosis is severely constrained by the scarcity of expert-annotated lesion data, particularly for rare pathologies and underrepresented disease subtypes. While data augmentation offers a potential solution, existing methods fail to generate sufficiently realistic lesion morphologies that preserve the complex spatial relationships and cellular architectures characteristic of histopathological tissues. Here we present PathoGen, a diffusion-based generative model that enables controllable, high-fidelity inpainting of lesions into benign histopathology images. Unlike conventional augmentation techniques, PathoGen leverages the iterative refinement process of diffusion models to synthesize lesions with natural tissue boundaries, preserved cellular structures, and authentic staining characteristics. We validate PathoGen across four diverse datasets representing distinct diagnostic challenges: kidney, skin, breast, and prostate pathology. Quantitative assessment confirms that PathoGen outperforms state-of-the-art generative baselines, including conditional GAN and Stable Diffusion, in image fidelity and distributional similarity. Crucially, we show that augmenting training sets with PathoGen-synthesized lesions enhances downstream segmentation performance compared to traditional geometric augmentations, particularly in data-scarce regimes. Besides, by simultaneously generating realistic morphology and pixel-level ground truth, PathoGen effectively overcomes the manual annotation bottleneck. This approach offers a scalable pathway for developing generalizable medical AI systems despite limited expert-labeled data.</description><author>cs.CV updates on arXiv.org</author><pubDate>Wed, 14 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2601.08127v1</guid></item><item><title>[arXiv-CV] ReCo-KD: Region- and Context-Aware Knowledge Distillation for Efficient 3D Medical Image Segmentation</title><link>https://arxiv.org/abs/2601.08301</link><description>arXiv:2601.08301v1 Announce Type: new 
Abstract: Accurate 3D medical image segmentation is vital for diagnosis and treatment planning, but state-of-the-art models are often too large for clinics with limited computing resources. Lightweight architectures typically suffer significant performance loss. To address these deployment and speed constraints, we propose Region- and Context-aware Knowledge Distillation (ReCo-KD), a training-only framework that transfers both fine-grained anatomical detail and long-range contextual information from a high-capacity teacher to a compact student network. The framework integrates Multi-Scale Structure-Aware Region Distillation (MS-SARD), which applies class-aware masks and scale-normalized weighting to emphasize small but clinically important regions, and Multi-Scale Context Alignment (MS-CA), which aligns teacher-student affinity patterns across feature levels. Implemented on nnU-Net in a backbone-agnostic manner, ReCo-KD requires no custom student design and is easily adapted to other architectures. Experiments on multiple public 3D medical segmentation datasets and a challenging aggregated dataset show that the distilled lightweight model attains accuracy close to the teacher while markedly reducing parameters and inference latency, underscoring its practicality for clinical deployment.</description><author>cs.CV updates on arXiv.org</author><pubDate>Wed, 14 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2601.08301v1</guid></item><item><title>[arXiv-CV] Quantization-Aware Neuromorphic Architecture for Skin Disease Classification on Resource-Constrained Devices</title><link>https://arxiv.org/abs/2507.15958</link><description>arXiv:2507.15958v3 Announce Type: replace-cross 
Abstract: On-device skin lesion analysis is constrained by the compute and energy cost of conventional CNN inference and by the need to update models as new patient data become available. We propose QANA, a quantization-aware CNN backbone embedded in an end-to-end pipeline engineered for conversion-stable neuromorphic execution. QANA replaces conversion-fragile components with spike-compatible transformations by bounding intermediate activations and aligning normalization with low-bit quantization, reducing conversion-induced distortion that disproportionately impacts rare classes. Efficiency is achieved through Ghost-based feature generation under tight FLOP budgets, while spatially-aware efficient channel attention and squeeze-and-excitation recalibrate channels without heavy global operators that are difficult to map to spiking cores. The resulting quantized projection head produces SNN-ready logits and enables incremental updates on edge hardware without full retraining or data offloading. On HAM10000, QANA achieves 91.6% Top-1 accuracy and 91.0% macro F1, improving the strongest converted SNN baseline by 3.5 percentage points in Top-1 accuracy, corresponding to a 4.0% relative gain, and by 12.0 points in macro F1, corresponding to a 15.2% relative gain. On a clinical dataset, QANA achieves 90.8% Top-1 accuracy and 81.7% macro F1, improving the strongest converted SNN baseline by 3.2 points in Top-1 accuracy, which corresponds to a 3.7% relative gain, and by 3.6 points in macro F1, corresponding to a 4.6% relative gain. When deployed on BrainChip Akida, QANA runs in 1.5 ms per image with 1.7 mJ per image, corresponding to 94.6% lower latency and 99.0% lower energy than its GPU-based CNN implementation.</description><author>cs.CV updates on arXiv.org</author><pubDate>Wed, 14 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2507.15958v3</guid></item><item><title>[arXiv-ML] Affect and Effect: Limitations of regularisation-based continual learning in EEG-based emotion classification</title><link>https://arxiv.org/abs/2601.07858</link><description>arXiv:2601.07858v1 Announce Type: new 
Abstract: Generalisation to unseen subjects in EEG-based emotion classification remains a challenge due to high inter-and intra-subject variability. Continual learning (CL) poses a promising solution by learning from a sequence of tasks while mitigating catastrophic forgetting. Regularisation-based CL approaches, such as Elastic Weight Consolidation (EWC), Synaptic Intelligence (SI), and Memory Aware Synapses (MAS), are commonly used as baselines in EEG-based CL studies, yet their suitability for this problem remains underexplored. This study theoretically and empirically finds that regularisation-based CL methods show limited performance for EEG-based emotion classification on the DREAMER and SEED datasets. We identify a fundamental misalignment in the stability-plasticity trade-off, where regularisation-based methods prioritise mitigating catastrophic forgetting (backward transfer) over adapting to new subjects (forward transfer). We investigate this limitation under subject-incremental sequences and observe that: (1) the heuristics for estimating parameter importance become less reliable under noisy data and covariate shift, (2) gradients on parameters deemed important by these heuristics often interfere with gradient updates required for new subjects, moving optimisation away from the minimum, (3) importance values accumulated across tasks over-constrain the model, and (4) performance is sensitive to subject order. Forward transfer showed no statistically significant improvement over sequential fine-tuning (p &gt; 0.05 across approaches and datasets). The high variability of EEG signals means past subjects provide limited value to future subjects. Regularisation-based continual learning approaches are therefore limited for robust generalisation to unseen subjects in EEG-based emotion classification.</description><author>cs.LG updates on arXiv.org</author><pubDate>Wed, 14 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2601.07858v1</guid></item><item><title>[arXiv-ML] Sherry: Hardware-Efficient 1.25-Bit Ternary Quantization via Fine-grained Sparsification</title><link>https://arxiv.org/abs/2601.07892</link><description>arXiv:2601.07892v1 Announce Type: new 
Abstract: The deployment of Large Language Models (LLMs) on resource-constrained edge devices is increasingly hindered by prohibitive memory and computational requirements. While ternary quantization offers a compelling solution by reducing weights to {-1, 0, +1}, current implementations suffer from a fundamental misalignment with commodity hardware. Most existing methods must choose between 2-bit aligned packing, which incurs significant bit wastage, or 1.67-bit irregular packing, which degrades inference speed. To resolve this tension, we propose Sherry, a hardware-efficient ternary quantization framework. Sherry introduces a 3:4 fine-grained sparsity that achieves a regularized 1.25-bit width by packing blocks of four weights into five bits, restoring power-of-two alignment. Furthermore, we identify weight trapping issue in sparse ternary training, which leads to representational collapse. To address this, Sherry introduces Arenas, an annealing residual synapse mechanism that maintains representational diversity during training. Empirical evaluations on LLaMA-3.2 across five benchmarks demonstrate that Sherry matches state-of-the-art ternary performance while significantly reducing model size. Notably, on an Intel i7-14700HX CPU, our 1B model achieves zero accuracy loss compared to SOTA baselines while providing 25% bit savings and 10% speed up. The code is available at https://github.com/Tencent/AngelSlim .</description><author>cs.LG updates on arXiv.org</author><pubDate>Wed, 14 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2601.07892v1</guid></item><item><title>[arXiv-ML] Intra-tree Column Subsampling Hinders XGBoost Learning of Ratio-like Interactions</title><link>https://arxiv.org/abs/2601.08121</link><description>arXiv:2601.08121v1 Announce Type: new 
Abstract: Many applied problems contain signal that becomes clear only after combining multiple raw measurements. Ratios and rates are common examples. In gradient boosted trees, this combination is not an explicit operation: the model must synthesize it through coordinated splits on the component features. We study whether intra-tree column subsampling in XGBoost makes that synthesis harder. We use two synthetic data generating processes with cancellation-style structure. In both, two primitive features share a strong nuisance factor, while the target depends on a smaller differential factor. A log ratio cancels the nuisance and isolates the signal. We vary colsample_bylevel and colsample_bynode over s in {0.4, 0.6, 0.8, 0.9}, emphasizing mild subsampling (s &gt;= 0.8). A control feature set includes the engineered ratio, removing the need for synthesis. Across both processes, intra-tree column subsampling reduces test PR-AUC in the primitives-only setting. In the main process the relative decrease reaches 54 percent when both parameters are set to 0.4. The effect largely disappears when the engineered ratio is present. A path-based co-usage metric drops in the same cells where performance deteriorates. Practically, if ratio-like structure is plausible, either avoid intra-tree subsampling or include the intended ratio features.</description><author>cs.LG updates on arXiv.org</author><pubDate>Wed, 14 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2601.08121v1</guid></item><item><title>[arXiv-ML] TabPFN Through The Looking Glass: An interpretability study of TabPFN and its internal representations</title><link>https://arxiv.org/abs/2601.08181</link><description>arXiv:2601.08181v1 Announce Type: new 
Abstract: Tabular foundational models are pre-trained models designed for a wide range of tabular data tasks. They have shown strong performance across domains, yet their internal representations and learned concepts remain poorly understood. This lack of interpretability makes it important to study how these models process and transform input features. In this work, we analyze the information encoded inside the model's hidden representations and examine how these representations evolve across layers. We run a set of probing experiments that test for the presence of linear regression coefficients, intermediate values from complex expressions, and the final answer in early layers. These experiments allow us to reason about the computations the model performs internally. Our results provide evidence that meaningful and structured information is stored inside the representations of tabular foundational models. We observe clear signals that correspond to both intermediate and final quantities involved in the model's prediction process. This gives insight into how the model refines its inputs and how the final output emerges. Our findings contribute to a deeper understanding of the internal mechanics of tabular foundational models. They show that these models encode concrete and interpretable information, which moves us closer to making their decision processes more transparent and trustworthy.</description><author>cs.LG updates on arXiv.org</author><pubDate>Wed, 14 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2601.08181v1</guid></item><item><title>[arXiv-ML] Taxon: Hierarchical Tax Code Prediction with Semantically Aligned LLM Expert Guidance</title><link>https://arxiv.org/abs/2601.08418</link><description>arXiv:2601.08418v1 Announce Type: new 
Abstract: Tax code prediction is a crucial yet underexplored task in automating invoicing and compliance management for large-scale e-commerce platforms. Each product must be accurately mapped to a node within a multi-level taxonomic hierarchy defined by national standards, where errors lead to financial inconsistencies and regulatory risks. This paper presents Taxon, a semantically aligned and expert-guided framework for hierarchical tax code prediction. Taxon integrates (i) a feature-gating mixture-of-experts architecture that adaptively routes multi-modal features across taxonomy levels, and (ii) a semantic consistency model distilled from large language models acting as domain experts to verify alignment between product titles and official tax definitions. To address noisy supervision in real business records, we design a multi-source training pipeline that combines curated tax databases, invoice validation logs, and merchant registration data to provide both structural and semantic supervision. Extensive experiments on the proprietary TaxCode dataset and public benchmarks demonstrate that Taxon achieves state-of-the-art performance, outperforming strong baselines. Further, an additional full hierarchical paths reconstruction procedure significantly improves structural consistency, yielding the highest overall F1 scores. Taxon has been deployed in production within Alibaba's tax service system, handling an average of over 500,000 tax code queries per day and reaching peak volumes above five million requests during business event with improved accuracy, interpretability, and robustness.</description><author>cs.LG updates on arXiv.org</author><pubDate>Wed, 14 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2601.08418v1</guid></item><item><title>[arXiv-ML] From Word Sequences to Behavioral Sequences: Adapting Modeling and Evaluation Paradigms for Longitudinal NLP</title><link>https://arxiv.org/abs/2601.07988</link><description>arXiv:2601.07988v1 Announce Type: cross 
Abstract: While NLP typically treats documents as independent and unordered samples, in longitudinal studies, this assumption rarely holds: documents are nested within authors and ordered in time, forming person-indexed, time-ordered $\textit{behavioral sequences}$. Here, we demonstrate the need for and propose a longitudinal modeling and evaluation paradigm that consequently updates four parts of the NLP pipeline: (1) evaluation splits aligned to generalization over people ($\textit{cross-sectional}$) and/or time ($\textit{prospective}$); (2) accuracy metrics separating between-person differences from within-person dynamics; (3) sequence inputs to incorporate history by default; and (4) model internals that support different $\textit{coarseness}$ of latent state over histories (pooled summaries, explicit dynamics, or interaction-based models). We demonstrate the issues ensued by traditional pipeline and our proposed improvements on a dataset of 17k daily diary transcripts paired with PTSD symptom severity from 238 participants, finding that traditional document-level evaluation can yield substantially different and sometimes reversed conclusions compared to our ecologically valid modeling and evaluation. We tie our results to a broader discussion motivating a shift from word-sequence evaluation toward $\textit{behavior-sequence}$ paradigms for NLP.</description><author>cs.LG updates on arXiv.org</author><pubDate>Wed, 14 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2601.07988v1</guid></item><item><title>[arXiv-ML] CSQL: Mapping Documents into Causal Databases</title><link>https://arxiv.org/abs/2601.08109</link><description>arXiv:2601.08109v1 Announce Type: cross 
Abstract: We describe a novel system, CSQL, which automatically converts a collection of unstructured text documents into an SQL-queryable causal database (CDB). A CDB differs from a traditional DB: it is designed to answer "why'' questions via causal interventions and structured causal queries. CSQL builds on our earlier system, DEMOCRITUS, which converts documents into thousands of local causal models derived from causal discourse. Unlike RAG-based systems or knowledge-graph based approaches, CSQL supports causal analysis over document collections rather than purely associative retrieval. For example, given an article on the origins of human bipedal walking, CSQL enables queries such as: "What are the strongest causal influences on bipedalism?'' or "Which variables act as causal hubs with the largest downstream influence?'' Beyond single-document case studies, we show that CSQL can also ingest RAG/IE-compiled causal corpora at scale by compiling the Testing Causal Claims (TCC) dataset of economics papers into a causal database containing 265,656 claim instances spanning 45,319 papers, 44 years, and 1,575 reported method strings, thereby enabling corpus-level causal queries and longitudinal analyses in CSQL. Viewed abstractly, CSQL functions as a compiler from unstructured documents into a causal database equipped with a principled algebra of queries, and can be applied broadly across many domains ranging from business, humanities, and science.</description><author>cs.LG updates on arXiv.org</author><pubDate>Wed, 14 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2601.08109v1</guid></item><item><title>[arXiv-ML] MLPlatt: Simple Calibration Framework for Ranking Models</title><link>https://arxiv.org/abs/2601.08345</link><description>arXiv:2601.08345v1 Announce Type: cross 
Abstract: Ranking models are extensively used in e-commerce for relevance estimation. These models often suffer from poor interpretability and no scale calibration, particularly when trained with typical ranking loss functions. This paper addresses the problem of post-hoc calibration of ranking models. We introduce MLPlatt: a simple yet effective ranking model calibration method that preserves the item ordering and converts ranker outputs to interpretable click-through rate (CTR) probabilities usable in downstream tasks. The method is context-aware by design and achieves good calibration metrics globally, and within strata corresponding to different values of a selected categorical field (such as user country or device), which is often important from a business perspective of an E-commerce platform. We demonstrate the superiority of MLPlatt over existing approaches on two datasets, achieving an improvement of over 10\% in F-ECE (Field Expected Calibration Error) compared to other methods. Most importantly, we show that high-quality calibration can be achieved without compromising the ranking quality.</description><author>cs.LG updates on arXiv.org</author><pubDate>Wed, 14 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2601.08345v1</guid></item><item><title>[arXiv-ML] Robust Single-Agent Reinforcement Learning for Regional Traffic Signal Control Under Demand Fluctuations</title><link>https://arxiv.org/abs/2511.00549</link><description>arXiv:2511.00549v2 Announce Type: replace 
Abstract: Traffic congestion, primarily driven by intersection queuing, significantly impacts urban living standards, safety, environmental quality, and economic efficiency. While Traffic Signal Control (TSC) systems hold potential for congestion mitigation, traditional optimization models often fail to capture real-world traffic complexity and dynamics. This study introduces a novel single-agent reinforcement learning (RL) framework for regional adaptive TSC, circumventing the coordination complexities inherent in multi-agent systems through a centralized decision-making paradigm. The model employs an adjacency matrix to unify the encoding of road network topology, real-time queue states derived from probe vehicle data, and current signal timing parameters. Leveraging the efficient learning capabilities of the DreamerV3 world model, the agent learns control policies where actions sequentially select intersections and adjust their signal phase splits to regulate traffic inflow/outflow, analogous to a feedback control system. Reward design prioritizes queue dissipation, directly linking congestion metrics (queue length) to control actions. Simulation experiments conducted in SUMO demonstrate the model's effectiveness: under inference scenarios with multi-level (10%, 20%, 30%) Origin-Destination (OD) demand fluctuations, the framework exhibits robust anti-fluctuation capability and significantly reduces queue lengths. This work establishes a new paradigm for intelligent traffic control compatible with probe vehicle technology. Future research will focus on enhancing practical applicability by incorporating stochastic OD demand fluctuations during training and exploring regional optimization mechanisms for contingency events.</description><author>cs.LG updates on arXiv.org</author><pubDate>Wed, 14 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2511.00549v2</guid></item><item><title>[BSPC] Dimensionality reduction for Explainable AI in skin lesion Classification: A sparse autoencoder and Lime-Based framework</title><link>https://www.sciencedirect.com/science/article/pii/S1746809426001357?dgcid=rss_sd_all</link><description>&lt;p&gt;Publication date: 1 May 2026&lt;/p&gt;&lt;p&gt;&lt;b&gt;Source:&lt;/b&gt; Biomedical Signal Processing and Control, Volume 116&lt;/p&gt;&lt;p&gt;Author(s): S. Praveena, T. Veeramakali&lt;/p&gt;</description><author>ScienceDirect Publication: Biomedical Signal Processing and Control</author><pubDate>Wed, 14 Jan 2026 02:08:39 GMT</pubDate><guid isPermaLink="true">https://www.sciencedirect.com/science/article/pii/S1746809426001357</guid></item><item><title>[BSPC] Exploring Image Processing Techniques for Skin Lesion Detection focusing on Segmentation and Classification: A Systematic Review</title><link>https://www.sciencedirect.com/science/article/pii/S1746809426000649?dgcid=rss_sd_all</link><description>&lt;p&gt;Publication date: 1 May 2026&lt;/p&gt;&lt;p&gt;&lt;b&gt;Source:&lt;/b&gt; Biomedical Signal Processing and Control, Volume 116&lt;/p&gt;&lt;p&gt;Author(s): Renu Bala, Neelam Duhan, Komal Kumar Bhatia&lt;/p&gt;</description><author>ScienceDirect Publication: Biomedical Signal Processing and Control</author><pubDate>Wed, 14 Jan 2026 02:08:39 GMT</pubDate><guid isPermaLink="true">https://www.sciencedirect.com/science/article/pii/S1746809426000649</guid></item><item><title>[arXiv-CV] HyperTopo-Adapters: Geometry- and Topology-Aware Segmentation of Leaf Lesions on Frozen Encoders</title><link>https://arxiv.org/abs/2601.06067</link><description>arXiv:2601.06067v1 Announce Type: new 
Abstract: Leaf-lesion segmentation is topology-sensitive: small merges, splits, or false holes can be biologically meaningful descriptors of biochemical pathways, yet they are weakly penalized by standard pixel-wise losses in Euclidean latents. I explore HyperTopo-Adapters, a lightweight, parameter-efficient head trained on top of a frozen vision encoder, which embeds features on a product manifold -- hyperbolic + Euclidean + spherical (H + E + S) -- to encourage hierarchical separation (H), local linear detail (E), and global closure (S). A topology prior complements Dice/BCE in two forms: (i) persistent-homology (PH) distance for evaluation and selection, and (ii) a differentiable surrogate that combines a soft Euler-characteristic match with total variation regularization for stable training. I introduce warm-ups for both the hyperbolic contrastive term and the topology prior, per-sample evaluation of structure-aware metrics (Boundary-F1, Betti errors, PD distance), and a min-PD within top-K Dice rule for checkpoint selection. On a Kaggle leaf-lesion dataset (N=2,940), early results show consistent gains in boundary and topology metrics (reducing Delta beta_1 hole error by 9%) while Dice/IoU remain competitive. The study is diagnostic by design: I report controlled ablations (curvature learning, latent dimensions, contrastive temperature, surrogate settings), and ongoing tests varying encoder strength (ResNet-50, DeepLabV3, DINOv2/v3), input resolution, PH weight, and partial unfreezing of late blocks. The contribution is an open, reproducible train/eval suite (available at https://github.com/ChimdiWalter/HyperTopo-Adapters) that isolates geometric/topological priors and surfaces failure modes to guide stronger, topology-preserving architectures.</description><author>cs.CV updates on arXiv.org</author><pubDate>Tue, 13 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2601.06067v1</guid></item><item><title>[arXiv-CV] A Unified Attention U-Net Framework for Cross-Modality Tumor Segmentation in MRI and CT</title><link>https://arxiv.org/abs/2601.06187</link><description>arXiv:2601.06187v1 Announce Type: new 
Abstract: This study presents a unified Attention U-Net architecture trained jointly on MRI (BraTS 2021) and CT (LIDC-IDRI) datasets to investigate the generalizability of a single model across diverse imaging modalities and anatomical sites. Our proposed pipeline incorporates modality-harmonized preprocessing, attention-gated skip connections, and a modality-aware Focal Tversky loss function. To the best of our knowledge, this study is among the first to evaluate a single Attention U-Net trained simultaneously on separate MRI (BraTS) and CT (LIDC-IDRI) tumor datasets, without relying on modality-specific encoders or domain adaptation. The unified model demonstrates competitive performance in terms of Dice coefficient, IoU, and AUC on both domains, thereby establishing a robust and reproducible baseline for future research in cross-modality tumor segmentation.</description><author>cs.CV updates on arXiv.org</author><pubDate>Tue, 13 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2601.06187v1</guid></item><item><title>[arXiv-CV] When Imbalance Comes Twice: Active Learning under Simulated Class Imbalance and Label Shift in Binary Semantic Segmentation</title><link>https://arxiv.org/abs/2601.06209</link><description>arXiv:2601.06209v1 Announce Type: new 
Abstract: The aim of Active Learning is to select the most informative samples from an unlabelled set of data. This is useful in cases where the amount of data is large and labelling is expensive, such as in machine vision or medical imaging. Two particularities of machine vision are first, that most of the images produced are free of defects, and second, that the amount of images produced is so big that we cannot store all acquired images. This results, on the one hand, in a strong class imbalance in defect distribution and, on the other hand, in a potential label shift caused by limited storage. To understand how these two forms of imbalance affect active learning algorithms, we propose a simulation study based on two open-source datasets. We artificially create datasets for which we control the levels of class imbalance and label shift. Three standard active learning selection strategies are compared: random sampling, entropy-based selection, and core-set selection. We demonstrate that active learning strategies, and in particular the entropy-based and core-set selections, remain interesting and efficient even for highly imbalanced datasets. We also illustrate and measure the loss of efficiency that occurs in the situation a strong label shift.</description><author>cs.CV updates on arXiv.org</author><pubDate>Tue, 13 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2601.06209v1</guid></item><item><title>[arXiv-CV] SIRR-LMM: Single-image Reflection Removal via Large Multimodal Model</title><link>https://arxiv.org/abs/2601.07209</link><description>arXiv:2601.07209v1 Announce Type: new 
Abstract: Glass surfaces create complex interactions of reflected and transmitted light, making single-image reflection removal (SIRR) challenging. Existing datasets suffer from limited physical realism in synthetic data or insufficient scale in real captures. We introduce a synthetic dataset generation framework that path-traces 3D glass models over real background imagery to create physically accurate reflection scenarios with varied glass properties, camera settings, and post-processing effects. To leverage the capabilities of Large Multimodal Model (LMM), we concatenate the image layers into a single composite input, apply joint captioning, and fine-tune the model using task-specific LoRA rather than full-parameter training. This enables our approach to achieve improved reflection removal and separation performance compared to state-of-the-art methods.</description><author>cs.CV updates on arXiv.org</author><pubDate>Tue, 13 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2601.07209v1</guid></item><item><title>[arXiv-CV] Explainable Deep Radiogenomic Molecular Imaging for MGMT Methylation Prediction in Glioblastoma</title><link>https://arxiv.org/abs/2601.07035</link><description>arXiv:2601.07035v1 Announce Type: cross 
Abstract: Glioblastoma (GBM) is a highly aggressive primary brain tumor with limited therapeutic options and poor prognosis. The methylation status of the O6-methylguanine-DNA methyltransferase (MGMT) gene promoter is a critical molecular biomarker that influences patient response to temozolomide chemotherapy. Traditional methods for determining MGMT status rely on invasive biopsies and are limited by intratumoral heterogeneity and procedural risks. This study presents a radiogenomic molecular imaging analysis framework for the non-invasive prediction of MGMT promoter methylation using multi-parametric magnetic resonance imaging (mpMRI).
  Our approach integrates radiomics, deep learning, and explainable artificial intelligence (XAI) to analyze MRI-derived imaging phenotypes and correlate them with molecular labels. Radiomic features are extracted from FLAIR, T1-weighted, T1-contrast-enhanced, and T2-weighted MRI sequences, while a 3D convolutional neural network learns deep representations from the same modalities. These complementary features are fused using both early fusion and attention-based strategies and classified to predict MGMT methylation status.
  To enhance clinical interpretability, we apply XAI methods such as Grad-CAM and SHAP to visualize and explain model decisions. The proposed framework is trained on the RSNA-MICCAI Radiogenomic Classification dataset and externally validated on the BraTS 2021 dataset. This work advances the field of molecular imaging by demonstrating the potential of AI-driven radiogenomics for precision oncology, supporting non-invasive, accurate, and interpretable prediction of clinically actionable molecular biomarkers in GBM.</description><author>cs.CV updates on arXiv.org</author><pubDate>Tue, 13 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2601.07035v1</guid></item><item><title>[arXiv-CV] A Multimodal Dataset of Student Oral Presentations with Sensors and Evaluation Data</title><link>https://arxiv.org/abs/2601.07576</link><description>arXiv:2601.07576v1 Announce Type: cross 
Abstract: Oral presentation skills are a critical component of higher education, yet comprehensive datasets capturing real-world student performance across multiple modalities remain scarce. To address this gap, we present SOPHIAS (Student Oral Presentation monitoring for Holistic Insights &amp; Analytics using Sensors), a 12-hour multimodal dataset containing recordings of 50 oral presentations (10-15-minute presentation followed by 5-15-minute Q&amp;amp;A) delivered by 65 undergraduate and master's students at the Universidad Autonoma de Madrid. SOPHIAS integrates eight synchronized sensor streams from high-definition webcams, ambient and webcam audio, eye-tracking glasses, smartwatch physiological sensors, and clicker, keyboard, and mouse interactions. In addition, the dataset includes slides and rubric-based evaluations from teachers, peers, and self-assessments, along with timestamped contextual annotations. The dataset captures presentations conducted in real classroom settings, preserving authentic student behaviors, interactions, and physiological responses. SOPHIAS enables the exploration of relationships between multimodal behavioral and physiological signals and presentation performance, supports the study of peer assessment, and provides a benchmark for developing automated feedback and Multimodal Learning Analytics tools. The dataset is publicly available for research through GitHub and Science Data Bank.</description><author>cs.CV updates on arXiv.org</author><pubDate>Tue, 13 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2601.07576v1</guid></item><item><title>[arXiv-CV] Does DINOv3 Set a New Medical Vision Standard?</title><link>https://arxiv.org/abs/2509.06467</link><description>arXiv:2509.06467v2 Announce Type: replace 
Abstract: The advent of large-scale vision foundation models, pre-trained on diverse natural images, has marked a paradigm shift in computer vision. However, how the frontier vision foundation models' efficacies transfer to specialized domains remains such as medical imaging remains an open question. This report investigates whether DINOv3, a state-of-the-art self-supervised vision transformer (ViT) that features strong capability in dense prediction tasks, can directly serve as a powerful, unified encoder for medical vision tasks without domain-specific pre-training. To answer this, we benchmark DINOv3 across common medical vision tasks, including 2D/3D classification and segmentation on a wide range of medical imaging modalities. We systematically analyze its scalability by varying model sizes and input image resolutions. Our findings reveal that DINOv3 shows impressive performance and establishes a formidable new baseline. Remarkably, it can even outperform medical-specific foundation models like BiomedCLIP and CT-Net on several tasks, despite being trained solely on natural images. However, we identify clear limitations: The model's features degrade in scenarios requiring deep domain specialization, such as in Whole-Slide Pathological Images (WSIs), Electron Microscopy (EM), and Positron Emission Tomography (PET). Furthermore, we observe that DINOv3 does not consistently obey scaling law in the medical domain; performance does not reliably increase with larger models or finer feature resolutions, showing diverse scaling behaviors across tasks. Ultimately, our work establishes DINOv3 as a strong baseline, whose powerful visual features can serve as a robust prior for multiple complex medical tasks. This opens promising future directions, such as leveraging its features to enforce multiview consistency in 3D reconstruction.</description><author>cs.CV updates on arXiv.org</author><pubDate>Tue, 13 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2509.06467v2</guid></item><item><title>[arXiv-CV] Parameter-efficient fine-tuning (PEFT) of Vision Foundation Models for Atypical Mitotic Figure Classification</title><link>https://arxiv.org/abs/2509.16935</link><description>arXiv:2509.16935v2 Announce Type: replace 
Abstract: Atypical mitotic figures (AMFs) are rare abnormal cell divisions associated with tumor aggressiveness and poor prognosis. Their detection remains a significant challenge due to subtle morphological cues, class imbalance, and inter-observer variability among pathologists. The MIDOG 2025 challenge introduced a dedicated track for atypical mitosis classification, enabling systematic evaluation of deep learning methods. In this study, we investigated the use of large vision foundation models, including Virchow, Virchow2, and UNI, with Low-Rank Adaptation (LoRA) for parameter-efficient fine-tuning. We conducted extensive experiments with different LoRA ranks, as well as random and group-based data splits, to analyze robustness under varied conditions. Our best approach, Virchow with LoRA rank 8 and ensemble of three-fold cross-validation, achieved a balanced accuracy of 88.37% on the preliminary test set, ranking joint 9th in the challenge leaderboard. These results highlight the promise of foundation models with efficient adaptation strategies for the classification of atypical mitosis, while underscoring the need for improvements in specificity and domain generalization.</description><author>cs.CV updates on arXiv.org</author><pubDate>Tue, 13 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2509.16935v2</guid></item><item><title>[arXiv-CV] Encoding Structural Constraints into Segment Anything Models via Probabilistic Graphical Models</title><link>https://arxiv.org/abs/2509.21750</link><description>arXiv:2509.21750v2 Announce Type: replace 
Abstract: While the Segment Anything Model (SAM) has achieved remarkable success in image segmentation, its direct application to medical imaging remains hindered by fundamental challenges, including ambiguous boundaries, insufficient modeling of anatomical relationships, and the absence of uncertainty quantification. To address these limitations, we introduce KG-SAM, a knowledge-guided framework that synergistically integrates anatomical priors with boundary refinement and uncertainty estimation. Specifically, KG-SAM incorporates (i) a medical knowledge graph to encode fine-grained anatomical relationships, (ii) an energy-based Conditional Random Field (CRF) to enforce anatomically consistent predictions, and (iii) an uncertainty-aware fusion module to enhance reliability in high-stakes clinical scenarios. Extensive experiments across multi-center medical datasets demonstrate the effectiveness of our approach: KG-SAM achieves an average Dice score of 82.69% on prostate segmentation and delivers substantial gains in abdominal segmentation, reaching 78.05% on MRI and 79.68% on CT. These results establish KG-SAM as a robust and generalizable framework for advancing medical image segmentation.</description><author>cs.CV updates on arXiv.org</author><pubDate>Tue, 13 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2509.21750v2</guid></item><item><title>[arXiv-CV] ReBrain: Brain MRI Reconstruction from Sparse CT Slice via Retrieval-Augmented Diffusion</title><link>https://arxiv.org/abs/2511.17068</link><description>arXiv:2511.17068v3 Announce Type: replace 
Abstract: Magnetic Resonance Imaging (MRI) plays a crucial role in brain disease diagnosis, but it is not always feasible for certain patients due to physical or clinical constraints. Recent studies attempt to synthesize MRI from Computed Tomography (CT) scans; however, low-dose protocols often result in highly sparse CT volumes with poor through-plane resolution, making accurate reconstruction of the full brain MRI volume particularly challenging. To address this, we propose ReBrain, a retrieval-augmented diffusion framework for brain MRI reconstruction. Given any 3D CT scan with limited slices, we first employ a Brownian Bridge Diffusion Model (BBDM) to synthesize MRI slices along the 2D dimension. Simultaneously, we retrieve structurally and pathologically similar CT slices from a comprehensive prior database via a fine-tuned retrieval model. These retrieved slices are used as references, incorporated through a ControlNet branch to guide the generation of intermediate MRI slices and ensure structural continuity. We further account for rare retrieval failures when the database lacks suitable references and apply spherical linear interpolation to provide supplementary guidance. Extensive experiments on SynthRAD2023 and BraTS demonstrate that ReBrain achieves state-of-the-art performance in cross-modal reconstruction under sparse conditions.</description><author>cs.CV updates on arXiv.org</author><pubDate>Tue, 13 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2511.17068v3</guid></item><item><title>[arXiv-CV] Diagnostic Performance of Universal-Learning Ultrasound AI Across Multiple Organs and Tasks: the UUSIC25 Challenge</title><link>https://arxiv.org/abs/2512.17279</link><description>arXiv:2512.17279v2 Announce Type: replace 
Abstract: IMPORTANCE: Modern ultrasound systems are universal diagnostic tools capable of imaging the entire body. However, current AI solutions remain fragmented into single-task tools. This critical gap between hardware versatility and software specificity limits workflow integration and clinical utility.
  OBJECTIVE: To evaluate the diagnostic accuracy, versatility, and efficiency of single general-purpose deep learning models for multi-organ classification and segmentation.
  DESIGN: The Universal UltraSound Image Challenge 2025 (UUSIC25) involved developing algorithms on 11,644 images aggregated from 12 sources (9 public, 3 private). Evaluation used an independent, multi-center private test set of 2,479 images, including data from a center completely unseen during training to assess generalization.
  OUTCOMES: Diagnostic performance (Dice Similarity Coefficient [DSC]; Area Under the Receiver Operating Characteristic Curve [AUC]) and computational efficiency (inference time, GPU memory).
  RESULTS: Of 15 valid algorithms, the top model (SMART) achieved a macro-averaged DSC of 0.854 across 5 segmentation tasks and AUC of 0.766 for binary classification. Models demonstrated high capability in anatomical segmentation (e.g., fetal head DSC: 0.942) but variability in complex diagnostic tasks subject to domain shift. Specifically, in breast cancer molecular subtyping, the top model's performance dropped from an AUC of 0.571 (internal) to 0.508 (unseen external center), highlighting the challenge of generalization.
  CONCLUSIONS: General-purpose AI models can achieve high accuracy and efficiency across multiple tasks using a single architecture. However, significant performance degradation on unseen data suggests domain generalization is critical for future clinical deployment.</description><author>cs.CV updates on arXiv.org</author><pubDate>Tue, 13 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2512.17279v2</guid></item><item><title>[arXiv-ML] A Foundation Model Approach for Fetal Stress Prediction During Labor From cardiotocography (CTG) recordings</title><link>https://arxiv.org/abs/2601.06149</link><description>arXiv:2601.06149v1 Announce Type: new 
Abstract: Intrapartum cardiotocography (CTG) is widely used for fetal monitoring during labor, yet its interpretation suffers from high inter-observer variability and limited predictive accuracy. Deep learning approaches have been constrained by the scarcity of CTG recordings with clinical outcome labels. We present the first application of self-supervised pre-training to intrapartum CTG analysis, leveraging 2,444 hours of unlabeled recordings for masked pre-training followed by fine-tuning on the 552-recording CTU-UHB benchmark. Using a PatchTST transformer architecture with a channel-asymmetric masking scheme designed for fetal heart rate reconstruction, we achieve an area under the receiver operating characteristic curve of 0.83 on the full test set and 0.853 on uncomplicated vaginal deliveries, exceeding previously reported results on this benchmark (0.68-0.75). Error analysis reveals that false-positive alerts typically correspond to CTG patterns judged concerning on retrospective clinical review, suggesting clinically meaningful predictions even when umbilical pH is normal. We release standardized dataset splits and model weights to enable reproducible benchmarking. Our results demonstrate that self-supervised pre-training can address data scarcity in fetal monitoring, offering a path toward reliable decision support in the labor room.</description><author>cs.LG updates on arXiv.org</author><pubDate>Tue, 13 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2601.06149v1</guid></item><item><title>[arXiv-ML] Cyber Threat Detection and Vulnerability Assessment System using Generative AI and Large Language Model</title><link>https://arxiv.org/abs/2601.06213</link><description>arXiv:2601.06213v1 Announce Type: cross 
Abstract: Background: Cyber-attacks have evolved rapidly in recent years, many individuals and business owners have been affected by cyber-attacks in various ways. Cyber-attacks include various threats such as ransomware, malware, phishing, and Denial of Service (DoS)-related attacks. Challenges: Traditional models such as Generative Artificial Intelligence (AI) and Security Bidirectional Encoder Representations from Transformers (BERT) were implemented to detect cyber threats. However, the existing Security BERT model has a limited contextual understanding of text data, which has less impact on detecting cyber-attacks. Proposed Methodology: To overcome the above-mentioned challenges, Robustly Optimized Bidirectional Encoder Representations from Transformers Pretraining Approach (RoBERTa) model is proposed which consists of diverse words of vocabulary understanding. Initially, data are extracted from a Packet Capture (PCAP) file and encrypted using Fully Harmonic Encryption (FHE). Subsequently, a Byte-level and Byte Pair Encoding (BBPE) tokenizer was used to generate tokens and help maintain the vocabulary for the encrypted values. Then, these values are applied to the RoBERTa model of the transformer with extensive training. Finally, Softmax is used for the detection and classification of attacks. The proposed RoBERTa model achieved better results than the existing BERT model in terms of accuracy (0.99), recall (0.91), and precision (0.89) respectively.</description><author>cs.LG updates on arXiv.org</author><pubDate>Tue, 13 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2601.06213v1</guid></item><item><title>[arXiv-ML] A Backpropagation-Free Feedback-Hebbian Network for Continual Learning Dynamics</title><link>https://arxiv.org/abs/2601.06758</link><description>arXiv:2601.06758v1 Announce Type: cross 
Abstract: Feedback-rich neural architectures can regenerate earlier representations and inject temporal context, making them a natural setting for strictly local synaptic plasticity. We ask whether a minimal, backpropagation-free feedback--Hebbian system can already express interpretable continual-learning--relevant behaviors under controlled training schedules. We introduce a compact prediction--reconstruction architecture with two feedforward layers for supervised association learning and two dedicated feedback layers trained to reconstruct earlier activity and re-inject it as additive temporal context. All synapses are updated by a unified local rule combining centered Hebbian covariance, Oja-style stabilization, and a local supervised drive where targets are available, requiring no weight transport or global error backpropagation. On a small two-pair association task, we characterize learning through layer-wise activity snapshots, connectivity trajectories (row/column means of learned weights), and a normalized retention index across phases. Under sequential A-&gt;B training, forward output connectivity exhibits a long-term depression (LTD)-like suppression of the earlier association while feedback connectivity preserves an A-related trace during acquisition of B. Under deterministic interleaving A,B,A,B,..., both associations are concurrently maintained rather than sequentially suppressed. Architectural controls and rule-term ablations isolate the role of dedicated feedback in regeneration and co-maintenance, and the role of the local supervised term in output selectivity and unlearning. Together, the results show that a compact feedback pathway trained with local plasticity can support regeneration and continual-learning--relevant dynamics in a minimal, mechanistically transparent setting.</description><author>cs.LG updates on arXiv.org</author><pubDate>Tue, 13 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2601.06758v1</guid></item><item><title>[arXiv-ML] Unity Forests: Improving Interaction Modelling and Interpretability in Random Forests</title><link>https://arxiv.org/abs/2601.07003</link><description>arXiv:2601.07003v1 Announce Type: cross 
Abstract: Random forests (RFs) are widely used for prediction and variable importance analysis and are often believed to capture any types of interactions via recursive splitting. However, since the splits are chosen locally, interactions are only reliably captured when at least one involved covariate has a marginal effect. We introduce unity forests (UFOs), an RF variant designed to better exploit interactions involving covariates without marginal effects. In UFOs, the first few splits of each tree are optimized jointly across a random covariate subset to form a "tree root" capturing such interactions; the remainder is grown conventionally. We further propose the unity variable importance measure (VIM), which is based on out-of-bag split criterion values from the tree roots. Here, only a small fraction of tree root splits with the highest in-bag criterion values are considered per covariate, reflecting that covariates with purely interaction-based effects are discriminative only if a split in an interacting covariate occurred earlier in the tree. Finally, we introduce covariate-representative tree roots (CRTRs), which select representative tree roots per covariate and provide interpretable insight into the conditions - marginal or interactive - under which each covariate has its strongest effects. In a simulation study, the unity VIM reliably identified interacting covariates without marginal effects, unlike conventional RF-based VIMs. In a large-scale real-data comparison, UFOs achieved higher discrimination and predictive accuracy than standard RFs, with comparable calibration. The CRTRs reproduced the covariates' true effect types reliably in simulated data and provided interesting insights in a real data analysis.</description><author>cs.LG updates on arXiv.org</author><pubDate>Tue, 13 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2601.07003v1</guid></item><item><title>[arXiv-ML] Covariance-Driven Regression Trees: Reducing Overfitting in CART</title><link>https://arxiv.org/abs/2601.07281</link><description>arXiv:2601.07281v1 Announce Type: cross 
Abstract: Decision trees are powerful machine learning algorithms, widely used in fields such as economics and medicine for their simplicity and interpretability. However, decision trees such as CART are prone to overfitting, especially when grown deep or the sample size is small. Conventional methods to reduce overfitting include pre-pruning and post-pruning, which constrain the growth of uninformative branches. In this paper, we propose a complementary approach by introducing a covariance-driven splitting criterion for regression trees (CovRT). This method is more robust to overfitting than the empirical risk minimization criterion used in CART, as it produces more balanced and stable splits and more effectively identifies covariates with true signals. We establish an oracle inequality of CovRT and prove that its predictive accuracy is comparable to that of CART in high-dimensional settings. We find that CovRT achieves superior prediction accuracy compared to CART in both simulations and real-world tasks.</description><author>cs.LG updates on arXiv.org</author><pubDate>Tue, 13 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2601.07281v1</guid></item><item><title>[arXiv-ML] Learning About Learning: A Physics Path from Spin Glasses to Artificial Intelligence</title><link>https://arxiv.org/abs/2601.07635</link><description>arXiv:2601.07635v1 Announce Type: cross 
Abstract: The Hopfield model, originally inspired by spin-glass physics, occupies a central place at the intersection of statistical mechanics, neural networks, and modern artificial intelligence. Despite its conceptual simplicity and broad applicability -- from associative memory to near-optimal solutions of combinatorial optimization problems -- it is rarely integrated into standard undergraduate physics curricula. In this paper, we present the Hopfield model as a pedagogically rich framework that naturally unifies core topics from undergraduate statistical physics, dynamical systems, linear algebra, and computational methods. We provide a concise and illustrated theoretical introduction grounded in familiar physics concepts, analyze the model's energy function, dynamics, and pattern stability, and discuss practical aspects of simulation, including a freely available simulation code. To support instruction, we conclude with classroom-ready example problems designed to mirror research practice. By explicitly connecting fundamental physics to contemporary AI applications, this work aims to help prepare physics students to understand, apply, and critically engage with the computational tools increasingly central to research, industry, and society.</description><author>cs.LG updates on arXiv.org</author><pubDate>Tue, 13 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2601.07635v1</guid></item><item><title>[arXiv-ML] Learning to accelerate Krasnosel'skii-Mann fixed-point iterations with guarantees</title><link>https://arxiv.org/abs/2601.07665</link><description>arXiv:2601.07665v1 Announce Type: cross 
Abstract: We introduce a principled learning to optimize (L2O) framework for solving fixed-point problems involving general nonexpansive mappings. Our idea is to deliberately inject summable perturbations into a standard Krasnosel'skii-Mann iteration to improve its average-case performance over a specific distribution of problems while retaining its convergence guarantees. Under a metric sub-regularity assumption, we prove that the proposed parametrization includes only iterations that locally achieve linear convergence-up to a vanishing bias term-and that it encompasses all iterations that do so at a sufficiently fast rate. We then demonstrate how our framework can be used to augment several widely-used operator splitting methods to accelerate the solution of structured monotone inclusion problems, and validate our approach on a best approximation problem using an L2O-augmented Douglas-Rachford splitting algorithm.</description><author>cs.LG updates on arXiv.org</author><pubDate>Tue, 13 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2601.07665v1</guid></item><item><title>[arXiv-ML] Failure-Aware RL: Reliable Offline-to-Online Reinforcement Learning with Self-Recovery for Real-World Manipulation</title><link>https://arxiv.org/abs/2601.07821</link><description>arXiv:2601.07821v1 Announce Type: cross 
Abstract: Post-training algorithms based on deep reinforcement learning can push the limits of robotic models for specific objectives, such as generalizability, accuracy, and robustness. However, Intervention-requiring Failures (IR Failures) (e.g., a robot spilling water or breaking fragile glass) during real-world exploration happen inevitably, hindering the practical deployment of such a paradigm. To tackle this, we introduce Failure-Aware Offline-to-Online Reinforcement Learning (FARL), a new paradigm minimizing failures during real-world reinforcement learning. We create FailureBench, a benchmark that incorporates common failure scenarios requiring human intervention, and propose an algorithm that integrates a world-model-based safety critic and a recovery policy trained offline to prevent failures during online exploration. Extensive simulation and real-world experiments demonstrate the effectiveness of FARL in significantly reducing IR Failures while improving performance and generalization during online reinforcement learning post-training. FARL reduces IR Failures by 73.1% while elevating performance by 11.3% on average during real-world RL post-training. Videos and code are available at https://failure-aware-rl.github.io.</description><author>cs.LG updates on arXiv.org</author><pubDate>Tue, 13 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2601.07821v1</guid></item><item><title>[arXiv-ML] Prophet as a Reproducible Forecasting Framework: A Methodological Guide for Business and Financial Analytics</title><link>https://arxiv.org/abs/2601.05929</link><description>arXiv:2601.05929v2 Announce Type: replace 
Abstract: Reproducibility remains a persistent challenge in forecasting research and practice, particularly in business and financial analytics, where forecasts inform high-stakes decisions. Traditional forecasting methods, while theoretically interpretable, often require extensive manual tuning and are difficult to replicate in proprietary environments. Machine learning approaches offer predictive flexibility but introduce challenges related to interpretability, stochastic training procedures, and cross-environment reproducibility. This paper examines Prophet, an open-source forecasting framework developed by Meta, as a reproducibility-enabling solution that balances interpretability, standardized workflows, and accessibility. Rather than proposing a new algorithm, this study evaluates how Prophet's additive structure, open-source implementation, and standardized workflow contribute to transparent and replicable forecasting practice. Using publicly available financial and retail datasets, we compare the performance and interpretability of Prophet with multiple ARIMA specifications (auto-selected, manually specified, and seasonal variants) and Random Forest, under a controlled and fully documented experimental design. This multi-model comparison provides a robust assessment of Prophet's relative performance and reproducibility advantages. Through concrete Python examples, we demonstrate how Prophet facilitates efficient forecasting workflows and integration with analytical pipelines. The study positions Prophet within the broader context of reproducible research. It highlights Prophet's role as a methodological building block that supports verification, auditability, and methodological rigor. This work provides researchers and practitioners with a practical reference framework for reproducible forecasting in Python-based research workflows.</description><author>cs.LG updates on arXiv.org</author><pubDate>Tue, 13 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2601.05929v2</guid></item><item><title>[BSPC] Depth-conditioned adversarial learning with dual discriminators for polyp segmentation</title><link>https://www.sciencedirect.com/science/article/pii/S1746809426001412?dgcid=rss_sd_all</link><description>&lt;p&gt;Publication date: 1 May 2026&lt;/p&gt;&lt;p&gt;&lt;b&gt;Source:&lt;/b&gt; Biomedical Signal Processing and Control, Volume 116&lt;/p&gt;&lt;p&gt;Author(s): Haiyu Wang, Jianan Zhang, Xueyu Liu, Yongfei Wu, Di Huang&lt;/p&gt;</description><author>ScienceDirect Publication: Biomedical Signal Processing and Control</author><pubDate>Mon, 12 Jan 2026 18:39:30 GMT</pubDate><guid isPermaLink="true">https://www.sciencedirect.com/science/article/pii/S1746809426001412</guid></item><item><title>[arXiv-CV] Prompt-Free SAM-Based Multi-Task Framework for Breast Ultrasound Lesion Segmentation and Classification</title><link>https://arxiv.org/abs/2601.05498</link><description>arXiv:2601.05498v1 Announce Type: new 
Abstract: Accurate tumor segmentation and classification in breast ultrasound (BUS) imaging remain challenging due to low contrast, speckle noise, and diverse lesion morphology. This study presents a multi-task deep learning framework that jointly performs lesion segmentation and diagnostic classification using embeddings from the Segment Anything Model (SAM) vision encoder. Unlike prompt-based SAM variants, our approach employs a prompt-free, fully supervised adaptation where high-dimensional SAM features are decoded through either a lightweight convolutional head or a UNet-inspired decoder for pixel-wise segmentation. The classification branch is enhanced via mask-guided attention, allowing the model to focus on lesion-relevant features while suppressing background artifacts. Experiments on the PRECISE 2025 breast ultrasound dataset, split per class into 80 percent training and 20 percent testing, show that the proposed method achieves a Dice Similarity Coefficient (DSC) of 0.887 and an accuracy of 92.3 percent, ranking among the top entries on the PRECISE challenge leaderboard. These results demonstrate that SAM-based representations, when coupled with segmentation-guided learning, significantly improve both lesion delineation and diagnostic prediction in breast ultrasound imaging.</description><author>cs.CV updates on arXiv.org</author><pubDate>Mon, 12 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2601.05498v1</guid></item><item><title>[arXiv-CV] Bidirectional Channel-selective Semantic Interaction for Semi-Supervised Medical Segmentation</title><link>https://arxiv.org/abs/2601.05855</link><description>arXiv:2601.05855v1 Announce Type: new 
Abstract: Semi-supervised medical image segmentation is an effective method for addressing scenarios with limited labeled data. Existing methods mainly rely on frameworks such as mean teacher and dual-stream consistency learning. These approaches often face issues like error accumulation and model structural complexity, while also neglecting the interaction between labeled and unlabeled data streams. To overcome these challenges, we propose a Bidirectional Channel-selective Semantic Interaction~(BCSI) framework for semi-supervised medical image segmentation. First, we propose a Semantic-Spatial Perturbation~(SSP) mechanism, which disturbs the data using two strong augmentation operations and leverages unsupervised learning with pseudo-labels from weak augmentations. Additionally, we employ consistency on the predictions from the two strong augmentations to further improve model stability and robustness. Second, to reduce noise during the interaction between labeled and unlabeled data, we propose a Channel-selective Router~(CR) component, which dynamically selects the most relevant channels for information exchange. This mechanism ensures that only highly relevant features are activated, minimizing unnecessary interference. Finally, the Bidirectional Channel-wise Interaction~(BCI) strategy is employed to supplement additional semantic information and enhance the representation of important channels. Experimental results on multiple benchmarking 3D medical datasets demonstrate that the proposed method outperforms existing semi-supervised approaches.</description><author>cs.CV updates on arXiv.org</author><pubDate>Mon, 12 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2601.05855v1</guid></item><item><title>[arXiv-ML] Tensor-DTI: Enhancing Biomolecular Interaction Prediction with Contrastive Embedding Learning</title><link>https://arxiv.org/abs/2601.05792</link><description>arXiv:2601.05792v1 Announce Type: new 
Abstract: Accurate drug-target interaction (DTI) prediction is essential for computational drug discovery, yet existing models often rely on single-modality predefined molecular descriptors or sequence-based embeddings with limited representativeness. We propose Tensor-DTI, a contrastive learning framework that integrates multimodal embeddings from molecular graphs, protein language models, and binding-site predictions to improve interaction modeling. Tensor-DTI employs a siamese dual-encoder architecture, enabling it to capture both chemical and structural interaction features while distinguishing interacting from non-interacting pairs. Evaluations on multiple DTI benchmarks demonstrate that Tensor-DTI outperforms existing sequence-based and graph-based models. We also conduct large-scale inference experiments on CDK2 across billion-scale chemical libraries, where Tensor-DTI produces chemically plausible hit distributions even when CDK2 is withheld from training. In enrichment studies against Glide docking and Boltz-2 co-folder, Tensor-DTI remains competitive on CDK2 and improves the screening budget required to recover moderate fractions of high-affinity ligands on out-of-family targets under strict family-holdout splits. Additionally, we explore its applicability to protein-RNA and peptide-protein interactions. Our findings highlight the benefits of integrating multimodal information with contrastive objectives to enhance interaction-prediction accuracy and to provide more interpretable and reliability-aware models for virtual screening.</description><author>cs.LG updates on arXiv.org</author><pubDate>Mon, 12 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2601.05792v1</guid></item><item><title>[arXiv-ML] Fusion Matters: Length-Aware Analysis of Positional-Encoding Fusion in Transformers</title><link>https://arxiv.org/abs/2601.05807</link><description>arXiv:2601.05807v1 Announce Type: new 
Abstract: Transformers require positional encodings to represent sequence order, yet most prior work focuses on designing new positional encodings rather than examining how positional information is fused with token embeddings. In this paper, we study whether the fusion mechanism itself affects performance, particularly in long-sequence settings. We conduct a controlled empirical study comparing three canonical fusion strategies--element-wise addition, concatenation with projection, and scalar gated fusion--under identical Transformer architectures, data splits, and random seeds. Experiments on three text classification datasets spanning short (AG News), medium (IMDB), and long (ArXiv) sequences show that fusion choice has negligible impact on short texts but produces consistent gains on long documents. To verify that these gains are structural rather than stochastic, we perform paired-seed analysis and cross-dataset comparison across sequence-length regimes. Additional experiments on the ArXiv dataset indicate that the benefit of learnable fusion generalizes across multiple positional encoding families. Finally, we explore a lightweight convolutional gating mechanism that introduces local inductive bias at the fusion level, evaluated on long documents only. Our results indicate that positional-encoding fusion is a non-trivial design choice for long-sequence Transformers and should be treated as an explicit modeling decision rather than a fixed default.</description><author>cs.LG updates on arXiv.org</author><pubDate>Mon, 12 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2601.05807v1</guid></item><item><title>[arXiv-ML] Prophet as a Repro ducible Forecasting Framework: A Methodological Guide for Business and Financial Analytics</title><link>https://arxiv.org/abs/2601.05929</link><description>arXiv:2601.05929v1 Announce Type: new 
Abstract: Reproducibility remains a persistent challenge in forecasting research and practice, particularly in business and financial analytics where forecasts inform high-stakes decisions. Traditional forecasting methods, while theoretically interpretable, often require extensive manual tuning and are difficult to replicate in proprietary environments. Machine learning approaches offer predictive flexibility but introduce challenges related to interpretability, stochastic training procedures, and cross-environment reproducibility. This paper examines Prophet, an open-source forecasting framework developed by Meta, as a reproducibility-enabling solution that balances interpretability, standardized workflows, and accessibility. Rather than proposing a new algorithm, this study evaluates how Prophet's additive structure, open-source implementation, and standardized workflow contribute to transparent and replicable forecasting practice. Using publicly available financial and retail datasets, we compare Prophet's performance and interpretability with multiple ARIMA specifications (auto-selected, manually specified, and seasonal variants) and Random Forest under a controlled and fully documented experimental design. This multi-model comparison provides a robust assessment of Prophet's relative performance and reproducibility advantages. Through concrete Python examples, we demonstrate how Prophet facilitates efficient forecasting workflows and integration with analytical pipelines. The study positions Prophet within the broader context of reproducible research. It highlights Prophet's role as a methodological building block that supports verification, auditability, and methodological rigor. This work provides researchers and practitioners with a practical reference framework for reproducible forecasting in Python-based research workflows.</description><author>cs.LG updates on arXiv.org</author><pubDate>Mon, 12 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2601.05929v1</guid></item><item><title>[arXiv-ML] SCOPE: Sequential Causal Optimization of Process Interventions</title><link>https://arxiv.org/abs/2512.17629</link><description>arXiv:2512.17629v3 Announce Type: replace 
Abstract: Prescriptive Process Monitoring (PresPM) recommends interventions during business processes to optimize key performance indicators (KPIs). In realistic settings, interventions are rarely isolated: organizations need to align sequences of interventions to jointly steer the outcome of a case. Existing PresPM approaches fall short in this respect. Many focus on a single intervention decision, while others treat multiple interventions independently, ignoring how they interact over time. Methods that do address these dependencies depend either on simulation or data augmentation to approximate the process to train a Reinforcement Learning (RL) agent, which can create a reality gap and introduce bias. We introduce SCOPE, a PresPM approach that learns aligned sequential intervention recommendations. SCOPE employs backward induction to estimate the effect of each candidate intervention action, propagating its impact from the final decision point back to the first. By leveraging causal learners, our method can utilize observational data directly, unlike methods that require constructing process approximations for reinforcement learning. Experiments on both an existing synthetic dataset and a new semi-synthetic dataset show that SCOPE consistently outperforms state-of-the-art PresPM techniques in optimizing the KPI. The novel semi-synthetic setup, based on a real-life event log, is provided as a reusable benchmark for future work on sequential PresPM.</description><author>cs.LG updates on arXiv.org</author><pubDate>Mon, 12 Jan 2026 05:00:00 GMT</pubDate><guid isPermaLink="true">oai:arXiv.org:2512.17629v3</guid></item><item><title>[BSPC] MACNet:Multiscale attention cross-sharing networks for colorectal polyp segmentation</title><link>https://www.sciencedirect.com/science/article/pii/S1746809426000042?dgcid=rss_sd_all</link><description>&lt;p&gt;Publication date: 1 May 2026&lt;/p&gt;&lt;p&gt;&lt;b&gt;Source:&lt;/b&gt; Biomedical Signal Processing and Control, Volume 116&lt;/p&gt;&lt;p&gt;Author(s): Xinhui Jiang, Chunmiao Wei, Xiaolin Li, Zhicheng Dai&lt;/p&gt;</description><author>ScienceDirect Publication: Biomedical Signal Processing and Control</author><pubDate>Sun, 11 Jan 2026 18:35:03 GMT</pubDate><guid isPermaLink="true">https://www.sciencedirect.com/science/article/pii/S1746809426000042</guid></item><item><title>[BSPC] CSA-Net: Class self-attention based feature extraction network for polyp segmentation</title><link>https://www.sciencedirect.com/science/article/pii/S174680942600042X?dgcid=rss_sd_all</link><description>&lt;p&gt;Publication date: 1 May 2026&lt;/p&gt;&lt;p&gt;&lt;b&gt;Source:&lt;/b&gt; Biomedical Signal Processing and Control, Volume 116&lt;/p&gt;&lt;p&gt;Author(s): Xuehu Wang, Xiangqian Liu, Jin Lu, Yuhao Wang, Chao Xue, Han Yu, Yongchang Zheng, Chen Geng, Chengwei Guo, Xiaoping Yin&lt;/p&gt;</description><author>ScienceDirect Publication: Biomedical Signal Processing and Control</author><pubDate>Sun, 11 Jan 2026 18:35:03 GMT</pubDate><guid isPermaLink="true">https://www.sciencedirect.com/science/article/pii/S174680942600042X</guid></item><item><title>[BSPC] EGNet: A boundary-region closed-loop network for medical image segmentation with fuzzy lesions</title><link>https://www.sciencedirect.com/science/article/pii/S1746809426001564?dgcid=rss_sd_all</link><description>&lt;p&gt;Publication date: 1 May 2026&lt;/p&gt;&lt;p&gt;&lt;b&gt;Source:&lt;/b&gt; Biomedical Signal Processing and Control, Volume 116&lt;/p&gt;&lt;p&gt;Author(s): Shuwen Wu, Qilong Li, Ruixue Xia, Chongsheng Zhang&lt;/p&gt;</description><author>ScienceDirect Publication: Biomedical Signal Processing and Control</author><pubDate>Sun, 11 Jan 2026 18:35:03 GMT</pubDate><guid isPermaLink="true">https://www.sciencedirect.com/science/article/pii/S1746809426001564</guid></item><item><title>[BSPC] Hawk-Net: Medical image segmentation and classification using multi-scale convolutional self-attention-based image processor with DK-CNN-Mamba-xAttention Fusion Network</title><link>https://www.sciencedirect.com/science/article/pii/S1746809425019123?dgcid=rss_sd_all</link><description>&lt;p&gt;Publication date: 15 April 2026&lt;/p&gt;&lt;p&gt;&lt;b&gt;Source:&lt;/b&gt; Biomedical Signal Processing and Control, Volume 115&lt;/p&gt;&lt;p&gt;Author(s): Mahmudul Hasan, Md. Kamrul Hasan&lt;/p&gt;</description><author>ScienceDirect Publication: Biomedical Signal Processing and Control</author><pubDate>Sun, 11 Jan 2026 13:09:00 GMT</pubDate><guid isPermaLink="true">https://www.sciencedirect.com/science/article/pii/S1746809425019123</guid></item><item><title>[BSPC] EFFM: Rethinking feature fusion mechanisms for medical image segmentation</title><link>https://www.sciencedirect.com/science/article/pii/S1746809425018452?dgcid=rss_sd_all</link><description>&lt;p&gt;Publication date: 15 April 2026&lt;/p&gt;&lt;p&gt;&lt;b&gt;Source:&lt;/b&gt; Biomedical Signal Processing and Control, Volume 115&lt;/p&gt;&lt;p&gt;Author(s): Xiaoyan Zhang, Zheng Zhao, Yongqin Zhang, Chunlin Yu, Xiangfu Meng, Shuai Li&lt;/p&gt;</description><author>ScienceDirect Publication: Biomedical Signal Processing and Control</author><pubDate>Sun, 11 Jan 2026 13:09:00 GMT</pubDate><guid isPermaLink="true">https://www.sciencedirect.com/science/article/pii/S1746809425018452</guid></item><item><title>[BSPC] Applications of a novel deep neural network to the classification of liver steatosis and breast lesions in ultrasound images</title><link>https://www.sciencedirect.com/science/article/pii/S1746809425019068?dgcid=rss_sd_all</link><description>&lt;p&gt;Publication date: 15 April 2026&lt;/p&gt;&lt;p&gt;&lt;b&gt;Source:&lt;/b&gt; Biomedical Signal Processing and Control, Volume 115&lt;/p&gt;&lt;p&gt;Author(s): zlem Polat, Zmray Dokur, Tamer lmez&lt;/p&gt;</description><author>ScienceDirect Publication: Biomedical Signal Processing and Control</author><pubDate>Sun, 11 Jan 2026 13:09:00 GMT</pubDate><guid isPermaLink="true">https://www.sciencedirect.com/science/article/pii/S1746809425019068</guid></item><item><title>[BSPC] Frequency matters: Dual Domain Consistency for Semi-supervised Skin Lesion Segmentation</title><link>https://www.sciencedirect.com/science/article/pii/S1746809425019421?dgcid=rss_sd_all</link><description>&lt;p&gt;Publication date: 15 April 2026&lt;/p&gt;&lt;p&gt;&lt;b&gt;Source:&lt;/b&gt; Biomedical Signal Processing and Control, Volume 115&lt;/p&gt;&lt;p&gt;Author(s): Haoran Xi, Shan Ling, Min He, Xiaolin Li&lt;/p&gt;</description><author>ScienceDirect Publication: Biomedical Signal Processing and Control</author><pubDate>Sun, 11 Jan 2026 13:09:00 GMT</pubDate><guid isPermaLink="true">https://www.sciencedirect.com/science/article/pii/S1746809425019421</guid></item><item><title>[BSPC] Flexible dilated convolution and bidirectional pyramid approach to polyp segmentation</title><link>https://www.sciencedirect.com/science/article/pii/S1746809425019202?dgcid=rss_sd_all</link><description>&lt;p&gt;Publication date: 15 April 2026&lt;/p&gt;&lt;p&gt;&lt;b&gt;Source:&lt;/b&gt; Biomedical Signal Processing and Control, Volume 115&lt;/p&gt;&lt;p&gt;Author(s): Yunfei Yin, Zumnan Timbi, Zheng Yuan, Sijing Xiong, MD Tanvir Islam, Argho Dey, Swachha Ray&lt;/p&gt;</description><author>ScienceDirect Publication: Biomedical Signal Processing and Control</author><pubDate>Sun, 11 Jan 2026 13:09:00 GMT</pubDate><guid isPermaLink="true">https://www.sciencedirect.com/science/article/pii/S1746809425019202</guid></item><item><title>[BSPC] Enhanced diagnosis of skin lesions through torsional wave propagation and probabilistic inverse problem algorithms: An experimental study</title><link>https://www.sciencedirect.com/science/article/pii/S174680942501907X?dgcid=rss_sd_all</link><description>&lt;p&gt;Publication date: 15 April 2026&lt;/p&gt;&lt;p&gt;&lt;b&gt;Source:&lt;/b&gt; Biomedical Signal Processing and Control, Volume 115&lt;/p&gt;&lt;p&gt;Author(s): Yousef Almashakbeh, Hirad Shamimi, Inas H Faris, J.L. Martn-Rodrguez, Antonio Callejas, Guillermo Rus&lt;/p&gt;</description><author>ScienceDirect Publication: Biomedical Signal Processing and Control</author><pubDate>Sun, 11 Jan 2026 13:09:00 GMT</pubDate><guid isPermaLink="true">https://www.sciencedirect.com/science/article/pii/S174680942501907X</guid></item><item><title>[BSPC] DiffMoE-UNet: A differential transformer with Mixture-of-Experts for accurate medical image segmentation</title><link>https://www.sciencedirect.com/science/article/pii/S174680942501924X?dgcid=rss_sd_all</link><description>&lt;p&gt;Publication date: 15 April 2026&lt;/p&gt;&lt;p&gt;&lt;b&gt;Source:&lt;/b&gt; Biomedical Signal Processing and Control, Volume 115&lt;/p&gt;&lt;p&gt;Author(s): Jaouad Tagnamas, Hiba Ramadan, Ali Yahyaouy, Hamid Tairi&lt;/p&gt;</description><author>ScienceDirect Publication: Biomedical Signal Processing and Control</author><pubDate>Sun, 11 Jan 2026 13:09:00 GMT</pubDate><guid isPermaLink="true">https://www.sciencedirect.com/science/article/pii/S174680942501924X</guid></item><item><title>[BSPC] Uncertainty-weighted feature alignment and information interaction network for multi-scale medical image segmentation</title><link>https://www.sciencedirect.com/science/article/pii/S174680942501910X?dgcid=rss_sd_all</link><description>&lt;p&gt;Publication date: 15 April 2026&lt;/p&gt;&lt;p&gt;&lt;b&gt;Source:&lt;/b&gt; Biomedical Signal Processing and Control, Volume 115&lt;/p&gt;&lt;p&gt;Author(s): Min Zhang, Junxia Wang, Junkai Wang, Yuanjie Zheng&lt;/p&gt;</description><author>ScienceDirect Publication: Biomedical Signal Processing and Control</author><pubDate>Sun, 11 Jan 2026 13:09:00 GMT</pubDate><guid isPermaLink="true">https://www.sciencedirect.com/science/article/pii/S174680942501910X</guid></item><item><title>[BSPC] DisFreAda: Distribution-driven multi-frequency adaptive network for generalizable medical image segmentation</title><link>https://www.sciencedirect.com/science/article/pii/S1746809426000236?dgcid=rss_sd_all</link><description>&lt;p&gt;Publication date: 1 May 2026&lt;/p&gt;&lt;p&gt;&lt;b&gt;Source:&lt;/b&gt; Biomedical Signal Processing and Control, Volume 116&lt;/p&gt;&lt;p&gt;Author(s): Minjun Wang, Houjin Chen, Yanfeng Li, Jia Sun, Luyifu Chen, Peng Liang&lt;/p&gt;</description><author>ScienceDirect Publication: Biomedical Signal Processing and Control</author><pubDate>Sun, 11 Jan 2026 13:09:00 GMT</pubDate><guid isPermaLink="true">https://www.sciencedirect.com/science/article/pii/S1746809426000236</guid></item><item><title>[BSPC] Morphology-enhanced CAM-guided SAM for weakly supervised breast lesion segmentation</title><link>https://www.sciencedirect.com/science/article/pii/S1746809426000637?dgcid=rss_sd_all</link><description>&lt;p&gt;Publication date: 1 May 2026&lt;/p&gt;&lt;p&gt;&lt;b&gt;Source:&lt;/b&gt; Biomedical Signal Processing and Control, Volume 116&lt;/p&gt;&lt;p&gt;Author(s): Xin Yue, Qing Zhao, Xiaoling Liu, Jianqiang Li, Jing Bai, Changwei Song, Suqin Liu, Rodrigo Moreno, Zhikai Yang, Stefano E. Romero, Gabriel Jimenez, Guanghui Fu&lt;/p&gt;</description><author>ScienceDirect Publication: Biomedical Signal Processing and Control</author><pubDate>Sun, 11 Jan 2026 13:09:00 GMT</pubDate><guid isPermaLink="true">https://www.sciencedirect.com/science/article/pii/S1746809426000637</guid></item><item><title>[BSPC] Medical image segmentation based on 3D PDC with Swin Transformer</title><link>https://www.sciencedirect.com/science/article/pii/S1746809426000704?dgcid=rss_sd_all</link><description>&lt;p&gt;Publication date: 1 May 2026&lt;/p&gt;&lt;p&gt;&lt;b&gt;Source:&lt;/b&gt; Biomedical Signal Processing and Control, Volume 116&lt;/p&gt;&lt;p&gt;Author(s): Lin Fan, Xiaojia Ding, Zhongmin Wang, Hai Wang, Rong Zhang&lt;/p&gt;</description><author>ScienceDirect Publication: Biomedical Signal Processing and Control</author><pubDate>Sun, 11 Jan 2026 13:09:00 GMT</pubDate><guid isPermaLink="true">https://www.sciencedirect.com/science/article/pii/S1746809426000704</guid></item><item><title>[EAAI] Cross-Granularity Fusion Vision Mamba UNet for medical image segmentation</title><link>https://www.sciencedirect.com/science/article/pii/S0952197626000679?dgcid=rss_sd_all</link><description>&lt;p&gt;Publication date: 1 March 2026&lt;/p&gt;&lt;p&gt;&lt;b&gt;Source:&lt;/b&gt; Engineering Applications of Artificial Intelligence, Volume 167, Part 1&lt;/p&gt;&lt;p&gt;Author(s): Tuersunjiang Baidi, Zitong Ren, Kurban Ubul, Alimjan Aysa, Boyuan Li, Shihao Wang&lt;/p&gt;</description><author>ScienceDirect Publication: Engineering Applications of Artificial Intelligence</author><pubDate>Sun, 11 Jan 2026 13:09:00 GMT</pubDate><guid isPermaLink="true">https://www.sciencedirect.com/science/article/pii/S0952197626000679</guid></item><item><title>[AIM] Siamese evolutionary masking: Enhancing the generalization of self-supervised medical image segmentation model</title><link>https://www.sciencedirect.com/science/article/pii/S0933365726000011?dgcid=rss_sd_all</link><description>&lt;p&gt;Publication date: Available online 7 January 2026&lt;/p&gt;&lt;p&gt;&lt;b&gt;Source:&lt;/b&gt; Artificial Intelligence in Medicine&lt;/p&gt;&lt;p&gt;Author(s): Yichen Zhi, Hongxia Bie, Jiali Wang, Zhao Jing&lt;/p&gt;</description><author>ScienceDirect Publication: Artificial Intelligence in Medicine</author><pubDate>Sun, 11 Jan 2026 13:09:00 GMT</pubDate><guid isPermaLink="true">https://www.sciencedirect.com/science/article/pii/S0933365726000011</guid></item><item><title>[NC] UP2D: Uncertainty-aware progressive pseudo-label denoising for source-free domain adaptive medical image segmentation</title><link>https://www.sciencedirect.com/science/article/pii/S0925231226000561?dgcid=rss_sd_all</link><description>&lt;p&gt;Publication date: Available online 9 January 2026&lt;/p&gt;&lt;p&gt;&lt;b&gt;Source:&lt;/b&gt; Neurocomputing&lt;/p&gt;&lt;p&gt;Author(s): Thanh-Huy Nguyen, Quang-Khai Bui-Tran, Manh D. Ho, Thinh B. Lam, Vi Vu, Hoang-Thien Nguyen, Phat Huynh, Ulas Bagci&lt;/p&gt;</description><author>ScienceDirect Publication: Neurocomputing</author><pubDate>Sun, 11 Jan 2026 13:08:59 GMT</pubDate><guid isPermaLink="true">https://www.sciencedirect.com/science/article/pii/S0925231226000561</guid></item><item><title>[KBS] BACFormer: A robust boundary-aware transformer for medical image segmentation</title><link>https://www.sciencedirect.com/science/article/pii/S0950705125022439?dgcid=rss_sd_all</link><description>&lt;p&gt;Publication date: 28 February 2026&lt;/p&gt;&lt;p&gt;&lt;b&gt;Source:&lt;/b&gt; Knowledge-Based Systems, Volume 335&lt;/p&gt;&lt;p&gt;Author(s): Zhiyong Huang, Mingyu Wang, Mingyang Hou, Zhi Yu, Shiwei Wang, Xiaoyu Li, Jiahong Wang, Yan Yan, Yushi Liu&lt;/p&gt;</description><author>ScienceDirect Publication: Knowledge-Based Systems</author><pubDate>Sun, 11 Jan 2026 13:08:59 GMT</pubDate><guid isPermaLink="true">https://www.sciencedirect.com/science/article/pii/S0950705125022439</guid></item><item><title>[KBS] Multimodality-based framework for enhanced skin lesion recognition over federated learning</title><link>https://www.sciencedirect.com/science/article/pii/S0950705125021859?dgcid=rss_sd_all</link><description>&lt;p&gt;Publication date: 28 February 2026&lt;/p&gt;&lt;p&gt;&lt;b&gt;Source:&lt;/b&gt; Knowledge-Based Systems, Volume 335&lt;/p&gt;&lt;p&gt;Author(s): Abdul Hai Karimi, Taimoor Khan, Chang Choi&lt;/p&gt;</description><author>ScienceDirect Publication: Knowledge-Based Systems</author><pubDate>Sun, 11 Jan 2026 13:08:59 GMT</pubDate><guid isPermaLink="true">https://www.sciencedirect.com/science/article/pii/S0950705125021859</guid></item><item><title>[ESWA] WBDM-ECRF: A bridge diffusion model with efficient conditional random field for skin lesion segmentation</title><link>https://www.sciencedirect.com/science/article/pii/S0957417425046159?dgcid=rss_sd_all</link><description>&lt;p&gt;Publication date: 25 April 2026&lt;/p&gt;&lt;p&gt;&lt;b&gt;Source:&lt;/b&gt; Expert Systems with Applications, Volume 307&lt;/p&gt;&lt;p&gt;Author(s): Hefeng Ji, Jing Xiao, Jimin Liu, Haoyong Yu, Chao Chen&lt;/p&gt;</description><author>ScienceDirect Publication: Expert Systems with Applications</author><pubDate>Sun, 11 Jan 2026 13:08:59 GMT</pubDate><guid isPermaLink="true">https://www.sciencedirect.com/science/article/pii/S0957417425046159</guid></item><item><title>[ESWA] MED-Net: Leveraging multi-visual encoding and Manhattan-distance pooling for robust medical image segmentation</title><link>https://www.sciencedirect.com/science/article/pii/S0957417426000175?dgcid=rss_sd_all</link><description>&lt;p&gt;Publication date: 1 May 2026&lt;/p&gt;&lt;p&gt;&lt;b&gt;Source:&lt;/b&gt; Expert Systems with Applications, Volume 308&lt;/p&gt;&lt;p&gt;Author(s): Zhaozhao Su, Yuxuan Luo, Shiji Song, Zhiqiang Zhu, Jiajie Wang, Zhihua Fang, Bowen Wang, Liejun Wang&lt;/p&gt;</description><author>ScienceDirect Publication: Expert Systems with Applications</author><pubDate>Sun, 11 Jan 2026 13:08:59 GMT</pubDate><guid isPermaLink="true">https://www.sciencedirect.com/science/article/pii/S0957417426000175</guid></item><item><title>[ESWA] A colon polyp segmentation network via collaborative decision-making of mixture of experts</title><link>https://www.sciencedirect.com/science/article/pii/S095741742600031X?dgcid=rss_sd_all</link><description>&lt;p&gt;Publication date: 1 May 2026&lt;/p&gt;&lt;p&gt;&lt;b&gt;Source:&lt;/b&gt; Expert Systems with Applications, Volume 308&lt;/p&gt;&lt;p&gt;Author(s): Wenchao Zhang, Wenhui Ye, Zhenhua Yu, Jianguo Ju&lt;/p&gt;</description><author>ScienceDirect Publication: Expert Systems with Applications</author><pubDate>Sun, 11 Jan 2026 13:08:59 GMT</pubDate><guid isPermaLink="true">https://www.sciencedirect.com/science/article/pii/S095741742600031X</guid></item><item><title>[PR] MFSF&lt;sup&gt;2&lt;/sup&gt;-NET: towards improved skin lesion diagnosis via multi-scale and multi-frequency-domain fusion</title><link>https://www.sciencedirect.com/science/article/pii/S003132032501581X?dgcid=rss_sd_all</link><description>&lt;p&gt;Publication date: June 2026&lt;/p&gt;&lt;p&gt;&lt;b&gt;Source:&lt;/b&gt; Pattern Recognition, Volume 174&lt;/p&gt;&lt;p&gt;Author(s): Hui Liu, Yibo Dou, Meng Cao, Kai Wang, Yunmin Zou, Guoxiang Ma, Chaohui Li&lt;/p&gt;</description><author>ScienceDirect Publication: Pattern Recognition</author><pubDate>Sun, 11 Jan 2026 13:08:58 GMT</pubDate><guid isPermaLink="true">https://www.sciencedirect.com/science/article/pii/S003132032501581X</guid></item><item><title>[PR] Efficient breast cancer segmentation via Brownian Bridge diffusion with semantic fusion strategy</title><link>https://www.sciencedirect.com/science/article/pii/S0031320325016565?dgcid=rss_sd_all</link><description>&lt;p&gt;Publication date: June 2026&lt;/p&gt;&lt;p&gt;&lt;b&gt;Source:&lt;/b&gt; Pattern Recognition, Volume 174&lt;/p&gt;&lt;p&gt;Author(s): Feiyan Feng, Tianyu Liu, Fulin Zheng, Yanshen Sun, Hong Wang&lt;/p&gt;</description><author>ScienceDirect Publication: Pattern Recognition</author><pubDate>Sun, 11 Jan 2026 13:08:58 GMT</pubDate><guid isPermaLink="true">https://www.sciencedirect.com/science/article/pii/S0031320325016565</guid></item><item><title>[PR] Empowering 2D neural network for 3D medical image segmentation via neighborhood information fusion</title><link>https://www.sciencedirect.com/science/article/pii/S003132032501698X?dgcid=rss_sd_all</link><description>&lt;p&gt;Publication date: July 2026&lt;/p&gt;&lt;p&gt;&lt;b&gt;Source:&lt;/b&gt; Pattern Recognition, Volume 175&lt;/p&gt;&lt;p&gt;Author(s): Qiankun Li, Xiaolong Huang, Yani Zhang, Bo Fang, Duo Hong, Junxin Chen&lt;/p&gt;</description><author>ScienceDirect Publication: Pattern Recognition</author><pubDate>Sun, 11 Jan 2026 13:08:58 GMT</pubDate><guid isPermaLink="true">https://www.sciencedirect.com/science/article/pii/S003132032501698X</guid></item><item><title>[MedIA] SicTTA: Single image continual test time adaptation for medical image segmentation</title><link>https://www.sciencedirect.com/science/article/pii/S1361841525004050?dgcid=rss_sd_all</link><description>&lt;p&gt;Publication date: February 2026&lt;/p&gt;&lt;p&gt;&lt;b&gt;Source:&lt;/b&gt; Medical Image Analysis, Volume 108&lt;/p&gt;&lt;p&gt;Author(s): Jianghao Wu, Xinya Liu, Guotai Wang, Shaoting Zhang&lt;/p&gt;</description><author>ScienceDirect Publication: Medical Image Analysis</author><pubDate>Sun, 11 Jan 2026 13:08:58 GMT</pubDate><guid isPermaLink="true">https://www.sciencedirect.com/science/article/pii/S1361841525004050</guid></item><item><title>[MedIA] Adaptive mix for semi-supervised medical image segmentation</title><link>https://www.sciencedirect.com/science/article/pii/S1361841525004037?dgcid=rss_sd_all</link><description>&lt;p&gt;Publication date: February 2026&lt;/p&gt;&lt;p&gt;&lt;b&gt;Source:&lt;/b&gt; Medical Image Analysis, Volume 108&lt;/p&gt;&lt;p&gt;Author(s): Zhiqiang Shen, Peng Cao, Junming Su, Jinzhu Yang, Osmar R. Zaiane&lt;/p&gt;</description><author>ScienceDirect Publication: Medical Image Analysis</author><pubDate>Sun, 11 Jan 2026 13:08:58 GMT</pubDate><guid isPermaLink="true">https://www.sciencedirect.com/science/article/pii/S1361841525004037</guid></item><item><title>[MedIA] Test-time generative augmentation for medical image segmentation</title><link>https://www.sciencedirect.com/science/article/pii/S1361841525004487?dgcid=rss_sd_all</link><description>&lt;p&gt;Publication date: March 2026&lt;/p&gt;&lt;p&gt;&lt;b&gt;Source:&lt;/b&gt; Medical Image Analysis, Volume 109&lt;/p&gt;&lt;p&gt;Author(s): Xiao Ma, Yuhui Tao, Zetian Zhang, Yuhan Zhang, Xi Wang, Sheng Zhang, Zexuan Ji, Yizhe Zhang, Qiang Chen, Guang Yang&lt;/p&gt;</description><author>ScienceDirect Publication: Medical Image Analysis</author><pubDate>Sun, 11 Jan 2026 13:08:58 GMT</pubDate><guid isPermaLink="true">https://www.sciencedirect.com/science/article/pii/S1361841525004487</guid></item><item><title>[MedIA] CHAP: Channel-spatial hierarchical adversarial perturbation for semi-supervised medical image segmentation</title><link>https://www.sciencedirect.com/science/article/pii/S1361841525004645?dgcid=rss_sd_all</link><description>&lt;p&gt;Publication date: March 2026&lt;/p&gt;&lt;p&gt;&lt;b&gt;Source:&lt;/b&gt; Medical Image Analysis, Volume 109&lt;/p&gt;&lt;p&gt;Author(s): Si-Ping Zhou, Zhi-Fang Gong, Kai-Ni Wang, Ping Zhou, Yang Chen, Guang-Quan Zhou&lt;/p&gt;</description><author>ScienceDirect Publication: Medical Image Analysis</author><pubDate>Sun, 11 Jan 2026 13:08:58 GMT</pubDate><guid isPermaLink="true">https://www.sciencedirect.com/science/article/pii/S1361841525004645</guid></item><item><title>[TIP] SGNet: Style-Guided Network With Temporal Compensation for Unpaired Low-Light Colonoscopy Video Enhancement</title><link>http://ieeexplore.ieee.org/document/11306257</link><description>A low-light colonoscopy video enhancement method is needed as poor illumination in colonoscopy can hinder accurate disease diagnosis and adversely affect surgical procedures. Existing low-light video enhancement methods usually apply a frame-by-frame enhancement strategy without considering the temporal correlation between them, which often causes a flickering problem. In addition, most methods are designed for endoscopic devices with fixed imaging styles and cannot be easily adapted to different devices. In this paper, we propose a Style-Guided Network (SGNet) for unpaired Low-Light Colonoscopy Video Enhancement (LLCVE). Given that collecting content-consistent paired videos is difficult, SGNet adopts a CycleGAN-based framework to convert low-light videos to normal-light videos, in which a Temporal Compensation (TC) module and a Style Guidance (SG) module are proposed to alleviate the flickering problem and achieve flexible style transfer, respectively. The TC module compensates for a low-light frame by learning the correlated feature of its adjacent frames, thereby improving the temporal smoothness of the enhanced video. The SG module encodes the text of the imaging style and adaptively explores its intrinsic relationships with video features to obtain style representations, which are then used to guide the subsequent enhancement process. Extensive experiments on a curated database show that SGNet achieves promising performance on the LLCVE task, outperforming state-of-the-art methods in both quantitative metrics and visual quality.</description><author>IEEE Transactions on Image Processing - new TOC</author><pubDate>Mon, 22 Dec 2025 13:19:02 GMT</pubDate><guid isPermaLink="true">http://ieeexplore.ieee.org/document/11306257</guid></item><item><title>[MP] Uncertaintyguided testtime optimization for personalizing segmentation models in longitudinal medical imaging</title><link>https://aapm.onlinelibrary.wiley.com/doi/10.1002/mp.70206?af=R</link><description>Medical Physics, Volume 53, Issue 1, January 2026.</description><author>Wiley: Medical Physics: Table of Contents</author><pubDate>Mon, 22 Dec 2025 04:06:14 GMT</pubDate><guid isPermaLink="true">10.1002/mp.70206</guid></item><item><title>[MP] Accelerating vision foundation model for efficient medical image segmentation</title><link>https://aapm.onlinelibrary.wiley.com/doi/10.1002/mp.70193?af=R</link><description>Medical Physics, Volume 53, Issue 1, January 2026.</description><author>Wiley: Medical Physics: Table of Contents</author><pubDate>Sat, 20 Dec 2025 03:50:16 GMT</pubDate><guid isPermaLink="true">10.1002/mp.70193</guid></item><item><title>[TIP] Uncertainty-Guided Adaptive Correction for Semi-Supervised Medical Image Segmentation</title><link>http://ieeexplore.ieee.org/document/11273087</link><description>Consistent perturbation strategies have emerged as a dominant paradigm in semi-supervised medical image segmentation. Nevertheless, prevailing approaches inadequately address two critical challenges: 1) prediction errors induced by data uncertainty from distribution shifts, and 2) loss instability caused by model uncertainty in parameter generalization. To overcome these limitations, we propose an Uncertainty-Guided Adaptive Correction (UGAC) framework with three key innovations. First, we develop a dual-path uncertainty rectification mechanism that employs normalized entropy measures to detect error-prone regions in unlabeled predictions, followed by bilateral correction through confidence-weighted fusion. Second, we introduce adversarial consistency constraints that leverage labeled data to discriminate authentic segmentation patterns, effectively regularizing uncertainty propagation in unlabeled predictions through spectral normalization. Third, we architect a frequency-aware segmentation backbone through our novel Freqfusion module, which performs adaptive spectral decomposition during feature decoding to explicitly disentangle high-frequency (boundary-aware) and low-frequency (structural) components, thereby enhancing anatomical boundary sensitivity. Comprehensive evaluations on MM-WHS, BUSI, M&amp;amp;Ms and PROMISE12 datasets demonstrate UGACs superior performance. The proposed framework exhibits robust generalizability across CT, MRI, and ultrasound modalities, while achieving significantly lower computational complexity than baseline UNet implementations. The code will be available at https://github.com/SIGMACX/UGAC.</description><author>IEEE Transactions on Image Processing - new TOC</author><pubDate>Wed, 03 Dec 2025 13:18:43 GMT</pubDate><guid isPermaLink="true">http://ieeexplore.ieee.org/document/11273087</guid></item><item><title>[TPAMI] H2OT: Hierarchical Hourglass Tokenizer for Efficient Video Pose Transformers</title><link>http://ieeexplore.ieee.org/document/11155208</link><description>Transformers have been successfully applied in the field of video-based 3D human pose estimation. However, the high computational costs of these video pose transformers (VPTs) make them impractical on resource-constrained devices. In this paper, we present a hierarchical plug-and-play pruning-and-recovering framework, called Hierarchical Hourglass Tokenizer (H2OT), for efficient transformer-based 3D human pose estimation from videos. H2OT begins with progressively pruning pose tokens of redundant frames and ends with recovering full-length sequences, resulting in a few pose tokens in the intermediate transformer blocks and thus improving the model efficiency. It works with two key modules, namely, a Token Pruning Module (TPM) and a Token Recovering Module (TRM). TPM dynamically selects a few representative tokens to eliminate the redundancy of video frames, while TRM restores the detailed spatio-temporal information based on the selected tokens, thereby expanding the network output to the original full-length temporal resolution for fast inference. Our method is general-purpose: it can be easily incorporated into common VPT models on both seq2seq and seq2frame pipelines while effectively accommodating different token pruning and recovery strategies. In addition, our H2OT reveals that maintaining the full pose sequence is unnecessary, and a few pose tokens of representative frames can achieve both high efficiency and estimation accuracy. Extensive experiments on multiple benchmark datasets demonstrate both the effectiveness and efficiency of the proposed method.</description><author>IEEE Transactions on Pattern Analysis and Machine Intelligence - new TOC</author><pubDate>Wed, 10 Sep 2025 13:15:34 GMT</pubDate><guid isPermaLink="true">http://ieeexplore.ieee.org/document/11155208</guid></item><item><title>[JBHI] SegTom: A 3D Volumetric Medical Image Segmentation Framework for Thoracoabdominal Multi-Organ Anatomical Structures</title><link>http://ieeexplore.ieee.org/document/11151753</link><description>Accurate segmentation of thoracoabdominal anatomical structures in three-dimensional medical imaging modalities is fundamental for informed clinical decision-making across a wide array of medical disciplines. Current approaches often struggle to efficiently and comprehensively process this regions intricate and heterogeneous anatomical information, leading to suboptimal outcomes in diagnosis, treatment planning, and disease management. To address this challenge, we introduce SegTom, a novel volumetric segmentation framework equipped with a cutting-edge SegTom Block specifically engineered to effectively capture the complex anatomical representations inherent to the thoracoabdominal region. This SegTom Block incorporates a hierarchical anatomical-representation decomposition to facilitate efficient information exchange by decomposing the computationally intensive self-attention mechanism and cost-effectively aggregating the extracted representations. Rigorous validation of SegTom across nine diverse datasets, encompassing both computed tomography (CT) and magnetic resonance imaging (MRI) modalities, consistently demonstrates high performance across a broad spectrum of anatomical structures. Specifically, SegTom achieves a mean Dice similarity coefficient (DSC) of 87.29% for cardiac segmentation on the MM-WHS MRI dataset, 83.48% for multi-organ segmentation on the BTCV abdominal CT dataset, and 92.01% for airway segmentation on a dedicated CT dataset.</description><author>IEEE Journal of Biomedical and Health Informatics - new TOC</author><pubDate>Fri, 05 Sep 2025 13:16:46 GMT</pubDate><guid isPermaLink="true">http://ieeexplore.ieee.org/document/11151753</guid></item><item><title>[TMI] EPDiff: Erasure Perception Diffusion Model for Unsupervised Anomaly Detection in Preoperative Multimodal Images</title><link>http://ieeexplore.ieee.org/document/11121881</link><description>Unsupervised anomaly detection (UAD) methods typically detect anomalies by learning and reconstructing the normative distribution. However, since anomalies constantly invade and affect their surroundings, sub-healthy areas in the junction present structural deformations that could be easily misidentified as anomalies, posing difficulties for UAD methods that solely learn the normative distribution. The use of multimodal images can facilitate to address the above challenges, as they can provide complementary information of anomalies. Therefore, this paper propose a novel method for UAD in preoperative multimodal images, called Erasure Perception Diffusion model (EPDiff). First, the Local Erasure Progressive Training (LEPT) framework is designed to better rebuild sub-healthy structures around anomalies through the diffusion model with a two-phase process. Initially, healthy images are used to capture deviation features labeled as potential anomalies. Then, these anomalies are locally erased in multimodal images to progressively learn sub-healthy structures, obtaining a more detailed reconstruction around anomalies. Second, the Global Structural Perception (GSP) module is developed in the diffusion model to realize global structural representation and correlation within images and between modalities through interactions of high-level semantic information. In addition, a training-free module, named Multimodal Attention Fusion (MAF) module, is presented for weighted fusion of anomaly maps between different modalities and obtaining binary anomaly outputs. Experimental results show that EPDiff improves the AUPRC and mDice scores by 2% and 3.9% on BraTS2021, and by 5.2% and 4.5% on Shifts over the state-of-the-art methods, which proves the applicability of EPDiff in diverse anomaly diagnosis. The code is available at https://github.com/wjiazheng/EPDiff</description><author>IEEE Transactions on Medical Imaging - new TOC</author><pubDate>Mon, 11 Aug 2025 13:16:40 GMT</pubDate><guid isPermaLink="true">http://ieeexplore.ieee.org/document/11121881</guid></item><item><title>[TMI] GM-ABS: Promptable Generalist Model Drives Active Barely Supervised Training in Specialist Model for 3D Medical Image Segmentation</title><link>http://ieeexplore.ieee.org/document/11119675</link><description>Semi-supervised learning (SSL) has greatly advanced 3D medical image segmentation by alleviating the need for intensive labeling by radiologists. While previous efforts focused on model-centric advancements, the emergence of foundational generalist models like the Segment Anything Model (SAM) is expected to reshape the SSL landscape. Although these generalists usually show performance gaps relative to previous specialists in medical imaging, they possess impressive zero-shot segmentation abilities with manual prompts. Thus, this capability could serve as free lunch for training specialists, offering future SSL a promising data-centric perspective, especially revolutionizing both pseudo and expert labeling strategies to enhance the data pool. In this regard, we propose the Generalist Model-driven Active Barely Supervised (GM-ABS) learning paradigm, for developing specialized 3D segmentation models under extremely limited (barely) annotation budgets, e.g., merely cross-labeling three slices per selected scan. In specific, building upon a basic mean-teacher SSL framework, GM-ABS modernizes the SSL paradigm with two key data-centric designs: (i) Specialist-generalist collaboration, where the in-training specialist leverages class-specific positional prompts derived from class prototypes to interact with the frozen class-agnostic generalist across multiple views to achieve noisy-yet-effective label augmentation. Then, the specialist robustly assimilates the augmented knowledge via noise-tolerant collaborative learning. (ii) Expert-model collaboration that promotes active cross-labeling with notably low labeling efforts. This design progressively furnishes the specialist with informative and efficient supervision via a human-in-the-loop manner, which in turn benefits the quality of class-specific prompts. Extensive experiments on three benchmark datasets highlight the promising performance of GM-ABS over recent SSL approaches under extremely constrained labeling resources.</description><author>IEEE Transactions on Medical Imaging - new TOC</author><pubDate>Thu, 07 Aug 2025 13:17:44 GMT</pubDate><guid isPermaLink="true">http://ieeexplore.ieee.org/document/11119675</guid></item><item><title>[TMI] Collaborative Learning of Augmentation and Disentanglement for Semi-Supervised Domain Generalized Medical Image Segmentation</title><link>http://ieeexplore.ieee.org/document/11115113</link><description>This paper explores a challenging yet realistic scenario: semi-supervised domain generalization (SSDG) that includes label scarcity and domain shift problems. We pinpoint that the limitations of previous SSDG methods lie in 1) neglecting the difference between domain shifts existing within a training dataset (intra-domain shift, IDS) and those occurring between training and testing datasets (cross-domain shift, CDS) and 2) overlooking the interplay between label scarcity and domain shifts, resulting in these methods merely stitching together semi-supervised learning (SSL) and domain generalization (DG) techniques. Considering these limitations, we propose a novel perspective to decompose SSDG into the combination of unsupervised domain adaptation (UDA) and DG problems. To this end, we design a causal augmentation and disentanglement framework (CausalAD) for semi-supervised domain generalized medical image segmentation. Concretely, CausalAD involves two collaborative processes: an augmentation process, which utilizes disentangled style factors to perform style augmentation for UDA, and a disentanglement process, which decouples domain-invariant (content) and domain-variant (noise and style) features for DG. Furthermore, we propose a proxy-based self-paced training strategy (ProSPT) to guide the training of CausalAD by gradually selecting unlabeled image pixels with high-quality pseudo labels in a self-paced training manner. Finally, we introduce a hierarchical structural causal model (HSCM) to explain the intuition and concept behind our method. Extensive experiments in the cross-sequence, cross-site, and cross-modality semi-supervised domain generalized medical image segmentation settings show the effectiveness of CausalAD and its superiority over the state-of-the-art. The code is available at https://github.com/Senyh/CausalAD</description><author>IEEE Transactions on Medical Imaging - new TOC</author><pubDate>Wed, 06 Aug 2025 13:17:46 GMT</pubDate><guid isPermaLink="true">http://ieeexplore.ieee.org/document/11115113</guid></item><item><title>[TBME] Training-Free Breast Ultrasound Image Segmentation With Retrieval-Based SAM2</title><link>http://ieeexplore.ieee.org/document/11113315</link><description>Objective: Early detection and treatment are crucial for improving the prognosis of breast cancer. While ultrasound imaging is widely utilized for screening, its interpretation is subjective and heavily dependent on clinician expertise. Although supervised models have shown great promise in medical image segmentation, they encounter challenges such as the need for large labeled datasets, extended training times, and limited generalization to external datasets. Methods: We propose TFSeg, a Training-Free automatic image segmentation framework based on Segment Anything Model 2 (SAM2). TFSeg eliminates the need for re-training new models and tuning hyper-parameters by generating image sequences through the retrieval of the top-k most similar images from the training set, using their mask prompts to guide SAM2 in achieving accurate and efficient segmentation. Results: Our approach, validated on a breast ultrasound image dataset, achieved a Dice score of 82.66%, surpassing the performance of most supervised and training-free segmentation models. Notably, it achieved the highest precision (87.93%) among all the compared methods. Conclusion: By combining image retrieval with sequence generation, TFSeg leverages SAM2 to provide an efficient and effective solution for breast cancer ultrasound image segmentation and demonstrates potential for broader applications in medical image analysis. Significance: The Training-Free automatic image Segmentation framework addresses challenges associated with supervised models, making it a promising method for improving breast cancer diagnosis.</description><author>IEEE Transactions on Biomedical Engineering - new TOC</author><pubDate>Tue, 05 Aug 2025 13:18:20 GMT</pubDate><guid isPermaLink="true">http://ieeexplore.ieee.org/document/11113315</guid></item><item><title>[TMI] A Trust-Guided Approach to MR Image Reconstruction With Side Information</title><link>http://ieeexplore.ieee.org/document/11105520</link><description>Reducing MRI scan times can improve patient care and lower healthcare costs. Many acceleration methods are designed to reconstruct diagnostic-quality images from sparse k-space data, via an ill-posed or ill-conditioned linear inverse problem (LIP). To address the resulting ambiguities, it is crucial to incorporate prior knowledge into the optimization problem, e.g., in the form of regularization. Another form of prior knowledge less commonly used in medical imaging is the readily available auxiliary data (a.k.a. side information) obtained from sources other than the current acquisition. In this paper, we present the Trust-Guided Variational Network (TGVN), an end-to-end deep learning framework that effectively and reliably integrates side information into LIPs. We demonstrate its effectiveness in multi-coil, multi-contrast MRI reconstruction, where incomplete or low-SNR measurements from one contrast are used as side information to reconstruct high-quality images of another contrast from heavily under-sampled data. TGVN is robust across different contrasts, anatomies, and field strengths. Compared to baselines utilizing side information, TGVN achieves superior image quality while preserving subtle pathological features even at challenging acceleration levels, drastically speeding up acquisition while minimizing hallucinations. Source code and dataset splits are available on github.com/sodicksonlab/TGVN</description><author>IEEE Transactions on Medical Imaging - new TOC</author><pubDate>Thu, 31 Jul 2025 13:17:16 GMT</pubDate><guid isPermaLink="true">http://ieeexplore.ieee.org/document/11105520</guid></item><item><title>[TMI] Dual Cross-Image Semantic Consistency With Self-Aware Pseudo Labeling for Semi-Supervised Medical Image Segmentation</title><link>http://ieeexplore.ieee.org/document/11104231</link><description>Semi-supervised learning has proven highly effective in tackling the challenge of limited labeled training data in medical image segmentation. In general, current approaches, which rely on intra-image pixel-wise consistency training via pseudo-labeling, overlook the consistency at more comprehensive semantic levels (e.g., object region) and suffer from severe discrepancy of extracted features resulting from an imbalanced number of labeled and unlabeled data. To overcome these limitations, we present a new Dual Cross-image Semantic Consistency (DuCiSC) learning framework, for semi-supervised medical image segmentation. Concretely, beyond enforcing pixel-wise semantic consistency, DuCiSC proposes dual paradigms to encourage region-level semantic consistency across: 1) labeled and unlabeled images; and 2) labeled and fused images, by explicitly aligning their prototypes. Relying on the dual paradigms, DuCiSC can effectively establish consistent cross-image semantics via prototype representations, thereby addressing the feature discrepancy issue. Moreover, we devise a novel self-aware confidence estimation strategy to accurately select reliable pseudo labels, allowing for exploiting the training dynamics of unlabeled data. Our DuCiSC method is extensively validated on four datasets, including two popular binary benchmarks in segmenting the left atrium and pancreas, a multi-class Automatic Cardiac Diagnosis Challenge dataset, and a challenging scenario of segmenting the inferior alveolar nerve that features complicated anatomical structures, showing superior segmentation results over previous state-of-the-art approaches. Our code is publicly available at https://github.com/ShanghaiTech-IMPACT/DuCiSC</description><author>IEEE Transactions on Medical Imaging - new TOC</author><pubDate>Wed, 30 Jul 2025 13:17:43 GMT</pubDate><guid isPermaLink="true">http://ieeexplore.ieee.org/document/11104231</guid></item><item><title>[TMI] 3D Deep-Learning-Based Segmentation of Human Skin Sweat Glands and Their 3D Morphological Response to Temperature Variations</title><link>http://ieeexplore.ieee.org/document/11098795</link><description>Skin, the primary regulator of heat exchange, relies on sweat glands for thermoregulation. Alterations in sweat gland morphology play a crucial role in various pathological conditions and clinical diagnoses. Current methods for observing sweat gland morphology are limited by their two-dimensional, in vitro, and destructive nature, underscoring the urgent need for real-time, non-invasive, quantifiable technologies. We proposed a novel three-dimensional (3D) transformer-based segmentation framework, enabling quite precise 3D sweat gland segmentation from skin volume data captured by optical coherence tomography (OCT). We quantitatively reveal, for the first time, 3D sweat gland morphological changes with temperature: for instance, volume, surface area, and length increase by 42.0%, 26.4%, and 12.8% at 43C vs. 10C (all p &lt;0.001), while S/V ratio decreases (p =0.01). By establishing a benchmark for normal sweat gland morphology and offering a real-time, non-invasive tool for quantifying 3D structural parameters, our approach facilitates the study of individual variability and pathological changes in sweat gland morphology, contributing to advancements in dermatological research and clinical applications.</description><author>IEEE Transactions on Medical Imaging - new TOC</author><pubDate>Tue, 29 Jul 2025 13:17:02 GMT</pubDate><guid isPermaLink="true">http://ieeexplore.ieee.org/document/11098795</guid></item><item><title>[TMI] Visualization of Breast Cancer Using Contrast-Enhanced Optical Coherence Elastography Based on Tissue Heterogeneity</title><link>http://ieeexplore.ieee.org/document/11098988</link><description>By mapping the mechanical properties of tissue, elastography can improve identification of breast cancer. On the macro-scale, ultrasound elastography and magnetic resonance elastography have emerged as effective clinical methods for the diagnosis of tumors. On the micro-scale, optical coherence elastography (OCE) shows promise for intraoperative tumor margin assessment during breast-conserving surgery. Whilst several OCE studies have demonstrated strong potential, the mechanical models used require the assumption of uniaxial stress throughout the sample. However, breast tissue is heterogeneous and contains compressible features (e.g., ducts and blood vessels) and collagen-rich fibrotic features (e.g., stroma). This heterogeneity can invalidate the assumption of uniaxial stress and reduce the accuracy of OCE, often making it challenging to interpret images. Here, we demonstrate a new variant of OCE based on mapping the Euler angle, i.e., the angle between the principal compression and the loading axis induced by tissue heterogeneity, which removes the assumption of uniaxial deformation. This is enabled by a hybrid three-dimensional (3-D) displacement estimation method that combines phase-sensitive detection and complex cross-correlation, providing access to the 3-D displacement and 3-D strain tensor on the micro-scale. Through experiments on phantoms, we demonstrate that an accuracy of 98.6%, a sensitivity of 0.95 (i.e., 16.58 mrad), and a spatial resolution as high as  $36~\mu $ m can be achieved in Euler angle imaging. We demonstrate the potential of Euler angle imaging for visualization of breast cancer. Through close correspondence with histology, our results show that mapping the Euler angle provides additional contrast to both optical coherence tomography and a compression OCE technique in identifying cancer. Mapping the Euler angle in breast tissue may provide a new biomarker for intraoperative tumor margin assessment.</description><author>IEEE Transactions on Medical Imaging - new TOC</author><pubDate>Tue, 29 Jul 2025 13:17:02 GMT</pubDate><guid isPermaLink="true">http://ieeexplore.ieee.org/document/11098988</guid></item><item><title>[TMI] A Deep Learning Multimodal Fusion-Based Method for Cell and Nucleus Segmentation</title><link>http://ieeexplore.ieee.org/document/11096725</link><description>In recent years, deep learning has been widely utilized in the fields of biomedical image segmentation and cellular image analysis. Supervised deep neural networks trained on annotated data have demonstrated good performance in tasks related to cell and nucleus segmentation. However, the use of supervised models necessitates carefully constructed training data and a substantial amount of ground truth information. Unfortunately, high-quality annotated data for cellular images are scarce. To address the issue of limited datasets, we propose a cell and nucleus segmentation method based on deep learning multimodal fusion. The proposed method includes three modules: a segmentation fundamental module, a multimodal prompter module, and an object output module. This comprehensive approach enables cell and nucleus segmentation tasks to be performed without the need for retraining on new data. The segmentation fundamental module is the core of the framework, as it provides essential segmentation capabilities. By leveraging preexisting models trained on natural imagery, this module effectively performs cell segmentation by incorporating prior knowledge. The multimodal prompter module, a pretrained model, aids in combining image and textual information. It employs a data fusion technique for multiple modalities to deliver prompts that steer the networks output, thereby avoiding the constraints inherent to single-modality approaches. The object output module combines the inputs from the preceding modules to generate the final segmentation output. The experimental validation confirms the superiority of the proposed method, which outperforms comparative methods in cell and nucleus segmentation tasks and has promise for future applications in cell tracking.</description><author>IEEE Transactions on Medical Imaging - new TOC</author><pubDate>Fri, 25 Jul 2025 13:18:29 GMT</pubDate><guid isPermaLink="true">http://ieeexplore.ieee.org/document/11096725</guid></item><item><title>[TMI] SarAdapter: Prioritizing Attention on Semantic-Aware Representative Tokens for Enhanced Medical Image Segmentation</title><link>http://ieeexplore.ieee.org/document/11089976</link><description>Transformer-based segmentation methods exhibit considerable potential in medical image analysis. However, their improved performance often comes with increased computational complexity, limiting their application in resource-constrained medical settings. Prior methods follow two independent tracks: (i) accelerating existing networks via semantic-aware routing, and (ii) optimizing token adapter design to enhance network performance. Despite directness, they encounter unavoidable defects (e.g., inflexible acceleration techniques or non-discriminative processing) limiting further improvements of quality-complexity trade-off. To address these shortcomings, we integrate these schemes by proposing the semantic-aware adapter (SarAdapter), which employs a semantic-based routing strategy, leveraging neural operators (ViT and CNN) of varying complexities. Specifically, it merges semantically similar tokens volume into low-resolution regions while preserving semantically distinct tokens as high-resolution regions. Additionally, we introduce a Mixed-adapter unit, which adaptively selects convolutional operators of varying complexities to better model regions at different scales. We evaluate our method on four medical datasets from three modalities and show that it achieves a superior balance between accuracy, model size, and efficiency. Notably, our proposed method achieves state-of-the-art segmentation quality on the Synapse dataset while reducing the number of tokens by 65.6%, signifying a substantial improvement in the efficiency of ViTs for the segmentation task.</description><author>IEEE Transactions on Medical Imaging - new TOC</author><pubDate>Tue, 22 Jul 2025 13:16:21 GMT</pubDate><guid isPermaLink="true">http://ieeexplore.ieee.org/document/11089976</guid></item><item><title>[TMI] EICSeg: Universal Medical Image Segmentation via Explicit In-Context Learning</title><link>http://ieeexplore.ieee.org/document/11090002</link><description>Deep learning models for medical image segmentation often struggle with task-specific characteristics, limiting their generalization to unseen tasks with new anatomies, labels, or modalities. Retraining or fine-tuning these models requires substantial human effort and computational resources. To address this, in-context learning (ICL) has emerged as a promising paradigm, enabling query image segmentation by conditioning on example image-mask pairs provided as prompts. Unlike previous approaches that rely on implicit modeling or non-end-to-end pipelines, we redefine the core interaction mechanism in ICL as an explicit retrieval process, termed E-ICL, benefiting from the emergence of vision foundation models (VFMs). E-ICL captures dense correspondences between queries and prompts at minimal learning cost and leverages them to dynamically weight multi-class prompt masks. Built upon E-ICL, we propose EICSeg, the first end-to-end ICL framework that integrates complementary VFMs for universal medical image segmentation. Specifically, we introduce a lightweight SD-Adapter to bridge the distinct functionalities of the VFMs, enabling more accurate segmentation predictions. To fully exploit the potential of EICSeg, we further design a scalable self-prompt training strategy and an adaptive token-to-image prompt selection mechanism, facilitating both efficient training and inference. EICSeg is trained on 47 datasets covering diverse modalities and segmentation targets. Experiments on nine unseen datasets demonstrate its strong few-shot generalization ability, achieving an average Dice score of 74.0%, outperforming existing in-context and few-shot methods by 4.5%, and reducing the gap to task-specific models to 10.8%. Even with a single prompt, EICSeg achieves a competitive average Dice score of 60.1%. Notably, it performs automatic segmentation without manual prompt engineering, delivering results comparable to interactive models while requiring minimal labeled data. Source code will be available at https://github.com/zerone-fg/EICSeg</description><author>IEEE Transactions on Medical Imaging - new TOC</author><pubDate>Tue, 22 Jul 2025 13:16:21 GMT</pubDate><guid isPermaLink="true">http://ieeexplore.ieee.org/document/11090002</guid></item><item><title>[TMI] REDNet: Reliable Evidential Discounting Network for Multi-Modality Medical Image Segmentation</title><link>http://ieeexplore.ieee.org/document/11087642</link><description>In the field of computer-aided diagnosis, particularly for tumor diseases, segmentation is a prerequisite and primary step. Multi-modality images become essential for achieving accurate segmentation, which offer critical insights beyond the limitations of single-modality data. However, different modalities and images may suffer from different types of data imperfection, such as intensity non-uniformity, motion artifact, and low quality due to hardware limitations, which challenge image segmentation algorithms. To address this challenge, we propose a Reliable Evidential Discounting Network (REDNet), which is composed of three main modules: 1) the Intra-modality Consistency Evaluation Module (ICEM) measuring the data cohesion within the same modality; 2) the Cross-modality Difference Aggregation Module (CDAM) identifing data discrepancy across modalities; 3) the Discounting Fusion Module (DFM) processing the multi-modality evidence by applying discounting strategies to fuse the data. This approach maintains segmentation accuracy by effectively integrating multi-modality evidence, while discounting the influence of lower-quality data, ensuring reliable results despite the presence of image imperfections. We evaluated REDNet on two distinct datasets, BRATS2021 and an in-house pancreas dataset from Changhai Hospital. REDNet outperforms other methods, particularly in scenarios with imperfect image sources, and achieves reliable results in multi-modality tumor segmentation.</description><author>IEEE Transactions on Medical Imaging - new TOC</author><pubDate>Mon, 21 Jul 2025 13:19:10 GMT</pubDate><guid isPermaLink="true">http://ieeexplore.ieee.org/document/11087642</guid></item><item><title>[TMI] SegMamba-V2: Long-Range Sequential Modeling Mamba for General 3-D Medical Image Segmentation</title><link>http://ieeexplore.ieee.org/document/11084842</link><description>The Transformer architecture has demonstrated remarkable results in 3D medical image segmentation due to its capability of modeling global relationships. However, it poses a significant computational burden when processing high-dimensional medical images. Mamba, as a State Space Model (SSM), has recently emerged as a notable approach for modeling long-range dependencies in sequential data. Although a substantial amount of Mamba-based research has focused on natural language and 2D image processing, few studies explore the capability of Mamba on 3D medical images. In this paper, we propose SegMamba-V2, a novel 3D medical image segmentation model, to effectively capture long-range dependencies within whole-volume features at each scale. To achieve this goal, we first devise a hierarchical scale downsampling strategy to enhance the receptive field and mitigate information loss during downsampling. Furthermore, we design a novel tri-orientated spatial Mamba block that extends the global dependency modeling process from one plane to three orthogonal planes to improve feature representation capability. Moreover, we collect and annotate a large-scale dataset (named CRC-2000) with fine-grained categories to facilitate benchmarking evaluation in 3D colorectal cancer (CRC) segmentation. We evaluate the effectiveness of our SegMamba-V2 on CRC-2000 and three other large-scale 3D medical image segmentation datasets, covering various modalities, organs, and segmentation targets. Experimental results demonstrate that our Segmamba-V2 outperforms state-of-the-art methods by a significant margin, which indicates the universality and effectiveness of the proposed model on 3D medical image segmentation tasks. The code for SegMamba-V2 is publicly available at: https://github.com/ge-xing/SegMamba-V2</description><author>IEEE Transactions on Medical Imaging - new TOC</author><pubDate>Fri, 18 Jul 2025 13:16:20 GMT</pubDate><guid isPermaLink="true">http://ieeexplore.ieee.org/document/11084842</guid></item><item><title>[TBME] A Multimodal Ultrasound-Driven Approach for Automated Tumor Assessment With B-Mode and Multi-Frequency Harmonic Motion Images</title><link>http://ieeexplore.ieee.org/document/11072028</link><description>Objective: Harmonic Motion Imaging (HMI) is an ultrasound elasticity imaging method that measures the mechanical properties of tissue using amplitude-modulated acoustic radiation force (AM-ARF). Multi-frequency HMI (MF-HMI) excites tissue at various AM frequencies simultaneously, allowing for image optimization without prior knowledge of inclusion size and stiffness. However, challenges remain in size estimation as inconsistent boundary effects result in different perceived sizes across AM frequencies. Herein, we developed an automated assessment method for tumor and focused ultrasound surgery (FUS) induced lesions using a transformer-based multi-modality neural network, HMINet, and further automated neoadjuvant chemotherapy (NACT) response prediction. HMINet was trained on 380 pairs of MF-HMI and B-mode images of phantoms and in vivo orthotopic breast cancer mice (4T1). Test datasets included phantoms (n = 32), in vivo 4T1 mice (n = 24), breast cancer patients (n = 20), FUS-induced lesions in ex vivo animal tissue and in vivo clinical settings with real-time inference, with average segmentation accuracy (Dice) of 0.91, 0.83, 0.80, and 0.81, respectively. HMINet outperformed state-of-the-art models; we also demonstrated the enhanced robustness of the multi-modality strategy over B-mode-only, both quantitatively through Dice scores and in terms of interpretation using saliency analysis. The contribution of AM frequency based on the number of salient pixels showed that the most significant AM frequencies are 800 and 200 Hz across clinical cases. Significance: We developed an automated, multimodality ultrasound-based tumor and FUS lesion assessment method, which facilitates the clinical translation of stiffness-based breast cancer treatment response prediction and real-time image-guided FUS therapy.</description><author>IEEE Transactions on Biomedical Engineering - new TOC</author><pubDate>Fri, 04 Jul 2025 13:19:16 GMT</pubDate><guid isPermaLink="true">http://ieeexplore.ieee.org/document/11072028</guid></item><item><title>[TBME] Assessment of Breast Composition With a Transmission-Based Microwave Imaging System</title><link>http://ieeexplore.ieee.org/document/11059861</link><description>Breast density is a key risk factor for breast cancer, but it is typically unknown before a first mammogram. Microwave imaging, proposed for cancer detection and monitoring, offers potential for measuring the composition of the breast. Objective: Assess the potential of microwave imaging as a method for estimating breast composition via correlation with mammogram metrics. Methods: Transmission based microwave imaging was applied to a cohort of 110 participants with prior mammograms. Several techniques were developed to estimate breast composition from microwave images, including average permittivity calculation, image thresholding and segmentation, and estimation of the fraction of glandular tissue in each pixel. These measures were compared to breast density category and percent density available from mammograms. Results: Average permittivity from microwave images correlated strongly with mammogram-based metrics. For the average permittivity, statistical analysis using one-way ANOVA revealed significant group differences across the various breast density categories. Thresholding and segmentation involved more detailed analysis of the images, and showed potential as alternative approaches to differentiating between breast composition categories. Conclusions: This study represents the largest cohort of healthy participants in which microwave breast images were compared with breast composition data available from clinical imaging. The cohort is well balanced across all categories. It highlights microwave imaging as a safe, portable, and affordable tool for non-invasive breast composition assessment and early cancer risk detection. Significance: The correlation between microwave imaging and mammogram-based breast density metrics highlights the potential for microwave imaging as a novel method for assessment of breast composition.</description><author>IEEE Transactions on Biomedical Engineering - new TOC</author><pubDate>Mon, 30 Jun 2025 13:19:40 GMT</pubDate><guid isPermaLink="true">http://ieeexplore.ieee.org/document/11059861</guid></item><item><title>[TBME] Prompt Learning With Bounding Box Constraints for Medical Image Segmentation</title><link>http://ieeexplore.ieee.org/document/11049003</link><description>Pixel-wise annotations are notoriously labourious and costly to obtain in the medical domain. To mitigate this burden, weakly supervised approaches based on bounding box annotationsmuch easier to acquireoffer a practical alternative. Vision foundation models have recently shown noteworthy segmentation performance when provided with prompts such as points or bounding boxes. Prompt learning exploits these models by adapting them to downstream tasks and automating segmentation, thereby reducing user intervention. However, existing prompt learning approaches depend on fully annotated segmentation masks. This paper proposes a novel framework that combines the representational power of foundation models with the annotation efficiency of weakly supervised segmentation. More specifically, our approach automates prompt generation for foundation models using only bounding box annotations. Our proposed optimization scheme integrates multiple constraints derived from box annotations with pseudo-labels generated by the prompted foundation model. Extensive experiments across multi-modal datasets reveal that, using the Segment Anything Model (SAM) as backbone, our weakly supervised method achieves an average Dice score of 84.90% in a limited data setting, outperforming existing fully-supervised and weakly-supervised approaches.</description><author>IEEE Transactions on Biomedical Engineering - new TOC</author><pubDate>Tue, 24 Jun 2025 13:17:26 GMT</pubDate><guid isPermaLink="true">http://ieeexplore.ieee.org/document/11049003</guid></item></channel></rss>